{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aac6b9e3-63cc-47d8-993b-7bfb24f97f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-import necessary libraries due to execution reset\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Define a function to categorize U.S. states based on agricultural land percentage\n",
    "def categorize_us_states_by_agriculture(df):\n",
    "    \"\"\"\n",
    "    Categorizes U.S. states into High, Medium, or Low Agriculture based on external data.\n",
    "    It then creates one-hot encoded dummy variables for each category.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing a 'state' column.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with new agriculture-based dummy variables.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the mapping of states to Agriculture categories based on external USDA data\n",
    "    agriculture_mapping = {\n",
    "        # High Agriculture States (Above 53% agricultural land)\n",
    "        \"Iowa\": \"High Ag\", \"Nebraska\": \"High Ag\", \"South Dakota\": \"High Ag\",\n",
    "        \"North Dakota\": \"High Ag\", \"Kansas\": \"High Ag\", \"Montana\": \"High Ag\",\n",
    "        \"Minnesota\": \"High Ag\", \"Missouri\": \"High Ag\", \"Idaho\": \"High Ag\",\n",
    "        \"Oklahoma\": \"High Ag\", \"Illinois\": \"High Ag\", \"Indiana\": \"High Ag\",\n",
    "        \"Wisconsin\": \"High Ag\", \"Arkansas\": \"High Ag\", \"Kentucky\": \"High Ag\",\n",
    "\n",
    "        # Medium Agriculture States (Between 40% - 53% agricultural land)\n",
    "        \"Texas\": \"Medium Ag\", \"Mississippi\": \"Medium Ag\", \"Alabama\": \"Medium Ag\",\n",
    "        \"Tennessee\": \"Medium Ag\", \"Georgia\": \"Medium Ag\", \"North Carolina\": \"Medium Ag\",\n",
    "        \"South Carolina\": \"Medium Ag\", \"Louisiana\": \"Medium Ag\", \"Colorado\": \"Medium Ag\",\n",
    "        \"Ohio\": \"Medium Ag\", \"Michigan\": \"Medium Ag\", \"Virginia\": \"Medium Ag\",\n",
    "\n",
    "        # Low Agriculture States (Below 40% agricultural land)\n",
    "        \"California\": \"Low Ag\", \"New York\": \"Low Ag\", \"New Jersey\": \"Low Ag\",\n",
    "        \"Nevada\": \"Low Ag\", \"Florida\": \"Low Ag\", \"Washington\": \"Low Ag\",\n",
    "        \"Oregon\": \"Low Ag\", \"Arizona\": \"Low Ag\", \"Pennsylvania\": \"Low Ag\",\n",
    "        \"Maine\": \"Low Ag\", \"Massachusetts\": \"Low Ag\", \"New Hampshire\": \"Low Ag\",\n",
    "        \"Vermont\": \"Low Ag\", \"Rhode Island\": \"Low Ag\", \"Connecticut\": \"Low Ag\",\n",
    "        \"Delaware\": \"Low Ag\", \"Maryland\": \"Low Ag\", \"West Virginia\": \"Low Ag\",\n",
    "        \"Hawaii\": \"Low Ag\", \"Alaska\": \"Low Ag\", \"Washington DC\": \"Low Ag\",\n",
    "        \"Puerto Rico\": \"Low Ag\"\n",
    "    }\n",
    "\n",
    "    # Map states in the dataset to Agriculture categories\n",
    "    df[\"state_agriculture_category\"] = df[\"state\"].map(agriculture_mapping).fillna(\"Other\")\n",
    "\n",
    "    # One-hot encode the agriculture categories\n",
    "    df = pd.get_dummies(df, columns=[\"state_agriculture_category\"], prefix=\"agriculture\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define a function to create machine age categories and one-hot encode them\n",
    "def create_machine_age_category(df):\n",
    "    \"\"\"\n",
    "    Adds machine age categories and one-hot encodes them in the given DataFrame.\n",
    "    \n",
    "    A machine's age is calculated as (sale_year - YearMade), and categorized into:\n",
    "    - \"New (0-5)\"\n",
    "    - \"Lightly Used (6-10)\"\n",
    "    - \"Moderately Used (11-20)\"\n",
    "    - \"Old (21-50)\"\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing 'YearMade' and 'saledate'.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with new categorical machine age columns.\n",
    "    \"\"\"\n",
    "    #df = df.copy()  # Avoid modifying the original DataFrame\n",
    "\n",
    "    # Convert 'saledate' to datetime if not already\n",
    "   # if not pd.api.types.is_datetime64_any_dtype(df[\"saledate\"]):\n",
    "    #    df[\"saledate\"] = pd.to_datetime(df[\"saledate\"], errors=\"coerce\")\n",
    "\n",
    "    # Remove invalid YearMade values\n",
    "    #df = df.dropna(subset=[\"YearMade\"])\n",
    "    #df = df[df[\"YearMade\"] != 1000]\n",
    "\n",
    "    # Extract sale year and compute machine age\n",
    "    #df[\"sale_year\"] = df[\"saledate\"].dt.year\n",
    "    df[\"machine_age\"] = df[\"sale_year\"] - df[\"YearMade\"]\n",
    "\n",
    "    # Remove unrealistic ages (negative or older than 50 years)\n",
    "    #df = df[(df[\"machine_age\"] >= 0) & (df[\"machine_age\"] <= 150)]\n",
    "\n",
    "    # Define bins and labels for machine age categories\n",
    "    #bins = [0, 5, 10, 20, 50,100]\n",
    "    #labels = [\"New (0-5)\", \"Lightly Used (6-10)\", \"Moderately Used (11-20)\", \"Old (21-50)\", \"vintage (51-150)\"]\n",
    "\n",
    "    # Create categorical column\n",
    "    #df[\"machine_age_category\"] = pd.cut(df[\"machine_age\"], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "    # One-hot encode machine age categories\n",
    "    #df = pd.get_dummies(df, columns=[\"machine_age_category\"], prefix=\"age\")\n",
    "    \n",
    "    # Create 'is_new_machine' dummy variable (1 for new, 0 for used)\n",
    "    df[\"is_new_machine\"] = (df[\"machine_age\"] <= 1).astype(int)\n",
    "\n",
    "    #df.drop(columns=[\"machine_age\"],inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def winter_month(df):\n",
    "        # Define winter months\n",
    "    winter_months = [12, 1, 2]\n",
    "    \n",
    "    # Assign 1 to high_price_season for winter months, 0 otherwise\n",
    "    df[\"high_price_season\"] = 0  # Default to 0\n",
    "    df.loc[df[\"sale_month\"].isin(winter_months), \"high_price_season\"] = 1\n",
    "    return df\n",
    "\n",
    "# Define final categorization for Steering_Controls\n",
    "def categorize_steering_controls_final(value):\n",
    "    if value in [\"Four Wheel Standard\", \"Conventional\"]:\n",
    "        return \"Mid Price Steering\"\n",
    "    elif value in [\"Command Control\"]:\n",
    "        return \"High Price Steering\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "\n",
    "\n",
    "# Define price-based categories for Track_Type\n",
    "def categorize_track_type(value):\n",
    "    if value in [\"Rubber\"]:\n",
    "        return \"Low Price Track\"\n",
    "    elif value in [\"Steel\"]:\n",
    "        return \"High Price Track\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "\n",
    "# Define price-based categories for Transmission\n",
    "def categorize_transmission(value):\n",
    "    if value in [\"Powershuttle\", \"Standard\", \"Direct Drive\"]:\n",
    "        return \"Low Price Transmission\"\n",
    "    elif value in [\"Autoshift\", \"Hydrostatic\"]:\n",
    "        return \"Mid Price Transmission\"\n",
    "    elif value in [\"Powershift\", \"None or Unspecified\", \"AutoShift\"]:\n",
    "        return \"High Price Transmission\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "\n",
    "# Define price-based categories for Steering_Controls\n",
    "def categorize_steering_controls(value):\n",
    "    if value in [\"Four Wheel Standard\"]:\n",
    "        return \"Low Price Steering\"\n",
    "    elif value in [\"Conventional\"]:\n",
    "        return \"Mid Price Steering\"\n",
    "    elif value in [\"Command Control\"]:\n",
    "        return \"High Price Steering\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "\n",
    "\n",
    "# Define price-based categories for ProductGroup\n",
    "def categorize_product_group(price):\n",
    "    if price < 20000:\n",
    "        return \"Low Price Group\"\n",
    "    elif 20000 <= price < 40000:\n",
    "        return \"Mid Price Group\"\n",
    "    else:\n",
    "        return \"High Price Group\"\n",
    "\n",
    "\n",
    "def create_model_category_mapping(df,model_avg_price):\n",
    "    \"\"\"\n",
    "    Creates a mapping of ModelID to Model_Category based on fiModelDesc and SalePrice in training data.\n",
    "    \n",
    "    Parameters:\n",
    "    training_df (pd.DataFrame): Training dataset containing fiModelDesc and SalePrice.\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary mapping ModelID to Model_Category.\n",
    "    \"\"\"\n",
    "    # Compute average SalePrice per fiModelDesc in training data\n",
    "    \n",
    "    \n",
    "    # Define price categories\n",
    "    def categorize_price(price):\n",
    "        if price < 20000:\n",
    "            return \"Low Price Models\"\n",
    "        elif 20000 <= price < 60000:\n",
    "            return \"Mid Price Models\"\n",
    "        else:\n",
    "            return \"High Price Models\"\n",
    "    \n",
    "    # Map fiModelDesc to categories\n",
    "    model_category_mapping = model_avg_price.apply(categorize_price).to_dict()\n",
    "    \n",
    "    # Create ModelID to category mapping using fiModelDesc\n",
    "    df[\"Model_Category\"] = df[\"fiModelDesc\"].map(model_category_mapping)\n",
    "    modelid_to_category = df.set_index(\"ModelID\")[\"Model_Category\"].to_dict()\n",
    "    \n",
    "    return modelid_to_category, model_category_mapping\n",
    "\n",
    "def categorize_model_id(df, modelid_to_category, model_category_mapping):\n",
    "    \"\"\"\n",
    "    Categorizes ModelID based on precomputed price categories.\n",
    "    If ModelID is not found, falls back to fiModelDesc categorization.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing ModelID and fiModelDesc columns.\n",
    "    modelid_to_category (dict): Mapping of ModelID to Model_Category.\n",
    "    model_category_mapping (dict): Mapping of fiModelDesc to Model_Category for fallback.\n",
    "    \n",
    "    Returns:\n",
    "    pd.Series: A Series with categorized model price labels.\n",
    "    \"\"\"\n",
    "    df[\"Predicted_Model_Category\"] = df[\"ModelID\"].map(modelid_to_category)\n",
    "    \n",
    "    # Handle missing ModelID by checking fiModelDesc mapping\n",
    "    missing_mask = df[\"Predicted_Model_Category\"].isna()\n",
    "    df.loc[missing_mask, \"Predicted_Model_Category\"] = df.loc[missing_mask, \"fiModelDesc\"].map(model_category_mapping)\n",
    "    \n",
    "    # Handle any remaining missing values by assigning 'Unknown'\n",
    "    df[\"Predicted_Model_Category\"].fillna(\"Unknown\", inplace=True)\n",
    "    \n",
    "    return df[\"Predicted_Model_Category\"]\n",
    "\n",
    "\n",
    "\n",
    "def extract_horsepower(df):\n",
    "    \"\"\"\n",
    "    Extracts and imputes missing Horsepower values from fiProductClassDesc.\n",
    "    Optimized for performance using vectorized operations.\n",
    "    \"\"\"\n",
    "    def extract_numeric_range(value):\n",
    "        \"\"\"Extracts the average horsepower value from a range like '100 to 120 Horsepower'.\"\"\"\n",
    "        if pd.isna(value) or not isinstance(value, str):\n",
    "            return np.nan\n",
    "        numbers = re.findall(r'[\\d\\.]+', value)\n",
    "        if len(numbers) == 2:\n",
    "            return (float(numbers[0]) + float(numbers[1])) / 2\n",
    "        elif len(numbers) == 1:\n",
    "            return float(numbers[0])\n",
    "        return np.nan\n",
    "    \n",
    "    # Identify rows with Horsepower in fiProductClassDesc\n",
    "    mask_horsepower = df['fiProductClassDesc'].str.contains(\"Horsepower\", na=False)\n",
    "    \n",
    "    # Extract horsepower values\n",
    "    df.loc[mask_horsepower, 'Horsepower_Unit_Type'] = 'Horsepower'\n",
    "    df.loc[mask_horsepower, 'Extracted_Horsepower'] = df.loc[mask_horsepower, 'fiProductClassDesc'].apply(extract_numeric_range)\n",
    "    \n",
    "    # Handle 'Variable' and 'No' values in Engine_Horsepower\n",
    "    df.loc[df['Engine_Horsepower'].isin(['Variable', 'No']), 'Engine_Horsepower'] = np.nan\n",
    "    \n",
    "    # Impute missing Engine_Horsepower values where Extracted_Horsepower is available\n",
    "    df['Engine_Horsepower_Imputed'] = df['Engine_Horsepower'].combine_first(df['Extracted_Horsepower'])\n",
    "    \n",
    "    # Fill remaining NaN values with median horsepower\n",
    "    #df['Engine_Horsepower_Imputed'].fillna(df['Engine_Horsepower_Imputed'].median(), inplace=True)\n",
    "    df.drop(columns='Engine_Horsepower',inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def preprocess_product_size(df):\n",
    "    \"\"\"\n",
    "    Function to preprocess and impute missing ProductSize values based on fiProductClassDesc.\n",
    "    \n",
    "    Steps:\n",
    "    1. Extract Product Type and Metric Tons / Horsepower from fiProductClassDesc.\n",
    "    2. Identify Unit Type (Metric Tons, Horsepower, or Lb Operating Capacity).\n",
    "    3. Convert Metric Tons / Horsepower to numerical values.\n",
    "    4. Use Metric Tons to impute missing ProductSize values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Extract Product Type and Size Descriptor\n",
    "    def split_product_desc(desc):\n",
    "        if pd.isna(desc):\n",
    "            return pd.NA, pd.NA\n",
    "        match = re.search(r'(.+?)-\\s*([\\d\\.]+.*)', desc)\n",
    "        if match:\n",
    "            return match.group(1).strip(), match.group(2).strip()\n",
    "        return desc.strip(), pd.NA  # If no match, return full description as product\n",
    "    \n",
    "    df[['Product_Type', 'Metric_Tons_HP']] = df['fiProductClassDesc'].apply(lambda x: pd.Series(split_product_desc(x)))\n",
    "    \n",
    "    # Step 2: Identify Unit Type\n",
    "    def extract_unit_type(desc):\n",
    "        if pd.isna(desc):\n",
    "            return pd.NA\n",
    "        if \"Metric Tons\" in desc:\n",
    "            return \"Metric Tons\"\n",
    "        elif \"Horsepower\" in desc:\n",
    "            return \"Horsepower\"\n",
    "        elif \"Lb Operating Capacity\" in desc:\n",
    "            return \"Lb Operating Capacity\"\n",
    "        return pd.NA\n",
    "    \n",
    "    df[\"Unit_Type\"] = df[\"Metric_Tons_HP\"].apply(extract_unit_type)\n",
    "    \n",
    "    # Step 3: Convert Metric Tons to numerical values\n",
    "    def extract_numeric_range(value):\n",
    "        \"\"\"Extracts the average value from a range like '12.0 to 14.0 Metric Tons'.\"\"\"\n",
    "        if pd.isna(value) or not isinstance(value, str):\n",
    "            return np.nan\n",
    "        numbers = re.findall(r'[\\d\\.]+', value)\n",
    "        if len(numbers) == 2:  # If range exists, take the average\n",
    "            return (float(numbers[0]) + float(numbers[1])) / 2\n",
    "        elif len(numbers) == 1:  # If only one number exists, use it\n",
    "            return float(numbers[0])\n",
    "        return np.nan\n",
    "    \n",
    "    df['Metric_Tons_Value'] = df['Metric_Tons_HP'].apply(lambda x: extract_numeric_range(x) if isinstance(x, str) else np.nan)\n",
    "    \n",
    "    # Step 4: Impute missing ProductSize values using Metric Tons\n",
    "    def impute_product_size(row):\n",
    "        if pd.isna(row['ProductSize']) and not pd.isna(row['Metric_Tons_Value']) and (row['Metric_Tons_Value']=='Metric Tons'):\n",
    "            if row['Metric_Tons_Value']  <= 5:\n",
    "                return 'Mini'\n",
    "            elif 5 < row['Metric_Tons_Value'] <= 75:\n",
    "                return 'Compact'\n",
    "            elif 20 < row['Metric_Tons_Value'] <= 50:\n",
    "                return 'Large / Medium'\n",
    "            elif 75 < row['Metric_Tons_Value'] <= 200:\n",
    "                return 'Medium'\n",
    "            elif row['Metric_Tons_Value'] > 200:\n",
    "                return 'Large'\n",
    "        return row['ProductSize']\n",
    "    \n",
    "    df['ProductSize_Imputed'] = df.apply(impute_product_size, axis=1)\n",
    "    df.drop(columns=['ProductSize','Metric_Tons_Value','Metric_Tons_HP','Unit_Type'],inplace=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    return df\n",
    "def Enclosure_fun(df):\n",
    "    list1=['EROPS','OROPS','EROPS AC']\n",
    "    list2=['NO ROPS','None or Unspecified']\n",
    "    df['Enclosure_cat']=0\n",
    "    df.loc[df['Enclosure'].isin(list1), 'Enclosure_cat'] = 1\n",
    "    df.loc[df['Enclosure'].isin(list2), 'Enclosure_cat'] = 0\n",
    "    df.loc[df['Enclosure']=='EROPS w AC','Enclosure_cat']=2\n",
    "    df.drop(columns='Enclosure',inplace=True)\n",
    "    return df\n",
    "# Define categories based on observed price trends\n",
    "def categorize_hydraulics(value):\n",
    "    if value in [\"Missing\", \"Auxiliary\", \"Standard\"]:\n",
    "        return \"Basic\"\n",
    "    elif value in [\"2 Valve\", \"None or Unspecified\", \"3 Valve\"]:\n",
    "        return \"Mid-Level\"\n",
    "    else:\n",
    "        return \"Advanced\"\n",
    "\n",
    "\n",
    "\n",
    "# Reload the dataset\n",
    "    \n",
    "\n",
    "#df=df[['SalesID','saledate',\"Enclosure\",\"YearMade\",\"ModelID\",\"MachineID\",\"Ripper\",\"ProductGroup\",\"ProductSize\",\"ProductGroupDesc\",\"Tire_Size\",\"MachineHoursCurrentMeter\",\"Blade_Type\",\"ProductGroupDesc\",\"Travel_Controls\",\"auctioneerID\",\"datasource\",\"Drive_System\",\"Pushblock\",\"Tire_Size\",\"Ride_Control\",\"UsageBand\",\"Coupler\",\"Engine_Horsepower_Imputed\"]].copy()\n",
    "def saledate(df):\n",
    "    # Convert 'saledate' to datetime and extract year, month, and day\n",
    "    df['saledate'] = pd.to_datetime(df['saledate'], errors='coerce')\n",
    "    df['sale_year'] = df['saledate'].dt.year\n",
    "    df['sale_month'] = df['saledate'].dt.month\n",
    "    df['sale_day'] = df['saledate'].dt.day\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_data(df,modelid_to_category, model_category_mapping,product_group_avg_price):\n",
    "    # Apply categorization\n",
    "    df[\"Hydraulics_Category\"] = df[\"Hydraulics\"].apply(categorize_hydraulics)\n",
    "    # Apply categorization\n",
    "    #df[\"Steering_Controls_Category\"] = df[\"Steering_Controls\"].apply(categorize_steering_controls)\n",
    "    # Apply categorization\n",
    "    df[\"Transmission_Category\"] = df[\"Transmission\"].apply(categorize_transmission)\n",
    "    \n",
    "    # Apply categorization\n",
    "    df[\"Steering_Controls_Category\"] = df[\"Steering_Controls\"].apply(categorize_steering_controls_final)\n",
    "    # Apply categorization to test data\n",
    "    # Apply categorization to test dat\n",
    "    df[\"Predicted_Model_Category\"] = categorize_model_id(df, modelid_to_category, model_category_mapping) # need to upload the modelid_to_category, model_category_mapping in the test\n",
    "    df[\"Track_Type_Category\"] = df[\"Track_Type\"].apply(categorize_track_type)\n",
    "    # Map each ProductGroup to a price category\n",
    "    df[\"ProductGroup_Category\"] = df[\"ProductGroup\"].map(product_group_avg_price).apply(categorize_product_group) # need to upload the categorize_product_group in the test\n",
    "    # Apply categorization\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "# Define function to update 'YearMade' and create 'YearMade_Bucket'\n",
    "def update_YearMade(df):\n",
    "    print(\"update_YearMade START\")\n",
    "    \n",
    "    # Compute median YearMade for each ModelID\n",
    "    model_medians = df.loc[df['YearMade'] > 1000].groupby('ModelID')['YearMade'].median()\n",
    "    \n",
    "    # Update YearMade where it is 1000 using ModelID median\n",
    "    df.loc[df['YearMade'] == 1000, 'YearMade'] = df['ModelID'].map(model_medians)\n",
    "    \n",
    "    # Compute overall median YearMade excluding 1000 values\n",
    "    overall_median = df.loc[df['YearMade'] > 1000, 'YearMade'].median()\n",
    "    \n",
    "    # Replace any remaining 1000 values with overall median\n",
    "    df['YearMade'].fillna(overall_median, inplace=True)\n",
    "        \n",
    "    # Bucketize YearMade\n",
    "    bins = [0, 1980, 1995, 2005, 2010, 2025]\n",
    "    labels = [\"Before 1980\", \"1980-1995\", \"1996-2005\", \"2006-2010\", \"2011-2025\"]\n",
    "    df['YearMade_Bucket'] = pd.cut(df['YearMade'], bins=bins, labels=labels, right=True)\n",
    "\n",
    "    df[\"YearMade_Bucket\"] = df[\"YearMade_Bucket\"].astype(str).fillna(\"Unknown\")\n",
    "    \n",
    "    # Define category order and apply Ordinal Encoding\n",
    "    year_bucket_encoder = OrdinalEncoder(\n",
    "        categories=[[\"Before 1980\", \"1980-1995\", \"1996-2005\", \"2006-2010\", \"2011-2025\", \"Unknown\"]],\n",
    "        handle_unknown=\"use_encoded_value\",\n",
    "        unknown_value=-1\n",
    "    )\n",
    "    \n",
    "    df[\"YearMade_Bucket\"] = year_bucket_encoder.fit_transform(df[[\"YearMade_Bucket\"]])\n",
    "    \n",
    "    print(\"update_YearMade END\")\n",
    "    return df\n",
    "def pre_train(df,file_path):\n",
    "    model_avg_price = df.groupby(\"fiModelDesc\")[\"SalePrice\"].mean()\n",
    "    product_group_avg_price = df.groupby(\"ProductGroup\")[\"SalePrice\"].mean()\n",
    "\n",
    "    model_avg_price.to_csv(file_path+\"\\model_avg_price.csv\", header=True)\n",
    "    product_group_avg_price.to_csv(file_path+\"\\product_group_avg_price.csv\", header=True)\n",
    "\n",
    "    \n",
    "\n",
    "    return model_avg_price,product_group_avg_price\n",
    "\n",
    "def Pre_train_test_model(df,train:str=True):\n",
    "    # Filter dataset to only include the last 5 years\n",
    "    recent_years = df['sale_year'].dropna().unique()\n",
    "    recent_years.sort()\n",
    "    #selected_years = recent_years[-5:]\n",
    "    #df_filtered = df[df['sale_year'].isin(selected_years)]\n",
    "    df_filtered=df.copy()\n",
    "    \n",
    "    # Drop irrelevant columns: 'SalesID', 'saledate'\n",
    "    high_cardinality_cols = [col for col in df_filtered.select_dtypes(include=['object']).columns if df_filtered[col].nunique() > 50]\n",
    "    df_filtered = df_filtered.drop(columns=['SalesID', 'saledate'] + high_cardinality_cols, errors='ignore')\n",
    "    \n",
    "    # Identify categorical columns\n",
    "    categorical_cols = df_filtered.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Identify high, moderate, and low cardinality categorical columns\n",
    "    high_cardinality_cols = [col for col in categorical_cols if df_filtered[col].nunique() >= 50]\n",
    "    moderate_cardinality_cols = [col for col in categorical_cols if 10 <= df_filtered[col].nunique() < 50]\n",
    "    low_cardinality_cols = [col for col in categorical_cols if df_filtered[col].nunique() < 10]\n",
    "    \n",
    "    # Fill missing categorical values with \"Unknown\"\n",
    "    for col in categorical_cols:\n",
    "        df_filtered[col] = df_filtered[col].fillna(\"Unknown\")\n",
    "    \n",
    "    # Encoding strategy\n",
    "    \n",
    "    # Apply One-Hot Encoding (OHE) to low-cardinality categories\n",
    "    df_encoded = pd.get_dummies(df_filtered, columns=low_cardinality_cols, drop_first=True)\n",
    "    \n",
    "    # Apply Ordinal Encoding to moderate-cardinality categories\n",
    "    if moderate_cardinality_cols:\n",
    "        ord_encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "        df_encoded[moderate_cardinality_cols] = ord_encoder.fit_transform(df_encoded[moderate_cardinality_cols])\n",
    "    \n",
    "    # Apply Frequency Encoding to high-cardinality categories\n",
    "    for col in high_cardinality_cols:\n",
    "        freq_encoding = df_encoded[col].value_counts().to_dict()\n",
    "        df_encoded[col] = df_encoded[col].map(freq_encoding)\n",
    "    \n",
    "    # Drop rows with missing SalePrice (target variable)\n",
    "    if train==True:\n",
    "        df_encoded = df_encoded.dropna(subset=['SalePrice'])\n",
    "        \n",
    "    \n",
    "\n",
    "    return df_encoded\n",
    "def train_run(df_encoded,median_price_map):\n",
    "    #!wandb login --relogin\n",
    "\n",
    "    # Initialize a new W&B run\n",
    "    #wandb.init(\n",
    "     #   project=\"Predict-heavy-machinery-price\"\n",
    "    #)\n",
    "\n",
    "\n",
    "    # Split into features and target\n",
    "    X = df_encoded.drop(columns=['SalePrice'])\n",
    "    y = df_encoded['SalePrice']\n",
    "    \n",
    "    # Split data into train and test sets\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "   # Merge `median_price_by_year` back into X_test for conversion\n",
    "    X_test = X_test.merge(median_price_map, on='sale_year', how='left')\n",
    "\n",
    "    # Train a Random Forest model\n",
    "    best_parameter = {'n_estimators': 100, \n",
    "                      'min_samples_split': 10,  \n",
    "                      'min_samples_leaf': 3,   \n",
    "                      'max_features': 0.5,\n",
    "                      'max_depth': 100}\n",
    "\n",
    "    rf_model = RandomForestRegressor(**best_parameter, random_state=42, n_jobs=-1)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict adjusted sale prices\n",
    "    y_pred_adjusted = rf_model.predict(X_test.drop(columns=['median_price_by_year']))\n",
    "\n",
    "    # Convert adjusted predictions and actual values back to real SalePrice\n",
    "    y_pred_real = y_pred_adjusted * X_test['median_price_by_year']\n",
    "    y_test_real = y_test * X_test['median_price_by_year']\n",
    "\n",
    "    # Calculate RMSE in real SalePrice terms\n",
    "    mae = mean_absolute_error(y_test_real, y_pred_real)\n",
    "    mse = mean_squared_error(y_test_real, y_pred_real)\n",
    "    rmse = mse ** 0.5\n",
    "\n",
    "    # Display model performance\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"MSE: {mse:.2f}\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    return rf_model\n",
    "\n",
    "def function_preperation(df,file_path:str,train:str=True): # add new function to this part\n",
    "# Apply YearMade update\n",
    "    df = update_YearMade(df)\n",
    "    df= saledate(df)\n",
    "    df=winter_month(df) # winter sales\n",
    "    # Apply the function to the dataset\n",
    "    df = create_machine_age_category(df)\n",
    "    if train==True:\n",
    "        model_avg_price,product_group_avg_price=pre_train(df,file_path)\n",
    "    else:\n",
    "        model_avg_price=pd.read_csv(file_path+\"/model_avg_price.csv\", index_col=0)\n",
    "        product_group_avg_price=pd.read_csv(file_path+\"/product_group_avg_price.csv\", index_col=0)\n",
    "        # Convert to Series explicitly (if needed)\n",
    "        model_avg_price = model_avg_price.iloc[:, 0]  # Extract first column as Series\n",
    "        product_group_avg_price = product_group_avg_price.iloc[:, 0]\n",
    "    # Create mappings from training data only in saleprice \n",
    "    modelid_to_category, model_category_mapping = create_model_category_mapping(df,model_avg_price) # need to upload the modelid_to_category, model_category_mapping in the test\n",
    "    \n",
    "    \n",
    "    df=preprocess_data(df,modelid_to_category, model_category_mapping,product_group_avg_price)\n",
    "    df=Enclosure_fun(df)\n",
    "    df=extract_horsepower(df)\n",
    "    \n",
    "    # Map each ProductGroup to a price category\n",
    "    df=preprocess_product_size(df)\n",
    "    \n",
    "    # Apply the function to the dataset\n",
    "    df = categorize_us_states_by_agriculture(df)\n",
    "    \n",
    "    df.drop(columns='state',inplace=True)\n",
    "\n",
    "    return df\n",
    "#-------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10846c87-6396-4c93-a1cf-6aff3667fad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update_YearMade START\n",
      "update_YearMade END\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'RandomForestRegressor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 43\u001b[0m\n\u001b[0;32m     39\u001b[0m df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYearMade_Bucket\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHydraulics\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDifferential_Type_Locking\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     40\u001b[0m                  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSteering_Controls_No\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSteering_Controls_Wheel\u001b[39m\u001b[38;5;124m'\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m rf_model \u001b[38;5;241m=\u001b[39m train_run(df,median_price_map)\n",
      "Cell \u001b[1;32mIn[14], line 523\u001b[0m, in \u001b[0;36mtrain_run\u001b[1;34m(df_encoded, median_price_map)\u001b[0m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;66;03m# Train a Random Forest model\u001b[39;00m\n\u001b[0;32m    517\u001b[0m best_parameter \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m100\u001b[39m, \n\u001b[0;32m    518\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_samples_split\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m10\u001b[39m,  \n\u001b[0;32m    519\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_samples_leaf\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m3\u001b[39m,   \n\u001b[0;32m    520\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_features\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.5\u001b[39m,\n\u001b[0;32m    521\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m100\u001b[39m}\n\u001b[1;32m--> 523\u001b[0m rf_model \u001b[38;5;241m=\u001b[39m RandomForestRegressor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbest_parameter, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    524\u001b[0m rf_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m    526\u001b[0m \u001b[38;5;66;03m# Predict adjusted sale prices\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RandomForestRegressor' is not defined"
     ]
    }
   ],
   "source": [
    "#def main():\n",
    "file_path=r\"C:\\Users\\eitanb\\Documents\\DS\\ML\\ML_project\\DATA\"\n",
    "\n",
    "\n",
    "df = pd.read_csv(file_path+\"/Train.csv\")[['SalesID', 'SalePrice',  'ModelID',\n",
    "        'YearMade', 'MachineHoursCurrentMeter', 'UsageBand',\n",
    "       'saledate', 'ProductSize',\n",
    "       'fiProductClassDesc', 'state', 'ProductGroup', \n",
    "       'Drive_System', 'Enclosure', \n",
    "        'Transmission', 'Turbocharged',  'Engine_Horsepower', 'Hydraulics',\n",
    "         'Tire_Size',\n",
    "       'Track_Type',\n",
    "       'Travel_Controls', 'Differential_Type', 'Steering_Controls','fiModelDesc']]\n",
    "\n",
    "df=function_preperation(df,file_path,True)\n",
    "\n",
    "\n",
    "# Compute the average price for each ProductGroup\n",
    "# Apply categorization\n",
    "\n",
    "\n",
    "df=Pre_train_test_model(df,True)\n",
    "\n",
    "\n",
    "### 📌 1️⃣ Inflation Adjustment Before Training ###\n",
    "# Compute yearly median price\n",
    "df['median_price_by_year'] = df.groupby('sale_year')['SalePrice'].transform('median')\n",
    "\n",
    "# Save median prices separately for post-processing\n",
    "median_price_map = df[['sale_year', 'median_price_by_year']].drop_duplicates()\n",
    "\n",
    "# Normalize SalePrice\n",
    "df['SalePrice'] = df['SalePrice'] / df['median_price_by_year']\n",
    "\n",
    "# Drop 'median_price_by_year' BEFORE training (avoiding data leakage)\n",
    "df.drop(columns=['median_price_by_year'], inplace=True)\n",
    "\n",
    "# Drop irrelevant features\n",
    "df.drop(columns=['YearMade_Bucket', 'Hydraulics', 'Differential_Type_Locking', \n",
    "                 'Steering_Controls_No', 'Steering_Controls_Wheel'], inplace=True)\n",
    "\n",
    "# Train the model\n",
    "rf_model = train_run(df,median_price_map)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9dfe8b-5973-477c-8489-76534c54985e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Log feature importances\n",
    "importances = rf_model.feature_importances_\n",
    "\n",
    "# Create a zip of feature names and feature importances\n",
    "feature_zip = zip(rf_model.feature_names_in_, rf_model.feature_importances_)\n",
    "\n",
    "# Sort the zip by feature importance in descending order\n",
    "sorted_feature_zip = sorted(feature_zip, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature names, importances, and index\n",
    "for idx, (feat, importance) in enumerate(sorted_feature_zip, 1):\n",
    "  print({f\"feature_{idx}_{feat}_importance\" : importance})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532f9c12-1577-4c6b-aa7b-b784cdc857e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "file_path=r\"C:\\Users\\eitanb\\Documents\\DS\\ML\\ML_project\\DATA\"\n",
    "# Load validation data\n",
    "Valid_df = pd.read_csv(file_path+\"/Valid.csv\")[[\n",
    "    'SalesID', 'ModelID', 'YearMade', 'MachineHoursCurrentMeter', 'UsageBand',\n",
    "    'saledate', 'ProductSize', 'fiProductClassDesc', 'state', 'ProductGroup',\n",
    "    'Drive_System', 'Enclosure', 'Transmission', 'Turbocharged',\n",
    "    'Engine_Horsepower', 'Hydraulics', 'Tire_Size', 'Track_Type',\n",
    "    'Travel_Controls', 'Differential_Type', 'Steering_Controls', 'fiModelDesc'\n",
    "]]\n",
    "\n",
    "# Preserve SalesID before preprocessing\n",
    "sales_id_col = Valid_df[['SalesID']].copy()\n",
    "\n",
    "\n",
    "df=function_preperation(Valid_df,file_path,False)\n",
    "\n",
    "# Model pre-training process\n",
    "#df = Pre_train_test_model(df)\n",
    "# Model pre-training process\n",
    "df = Pre_train_test_model(df,False)\n",
    "\n",
    "# Ensure index alignment\n",
    "df.index = sales_id_col.index  # Keep index same as SalesID\n",
    "\n",
    "# Load training feature names\n",
    "train_feature_names = rf_model.feature_names_in_  # Retrieves features seen during model training\n",
    "\n",
    "# Ensure test data has the same columns as training data\n",
    "for col in train_feature_names:\n",
    "    if col not in df.columns:\n",
    "        df[col] = 0  # Add missing features with default value (zero)\n",
    "\n",
    "# Drop extra columns in test data\n",
    "df = df[train_feature_names]\n",
    "\n",
    "# Make predictions\n",
    "predictions = rf_model.predict(df)\n",
    "\n",
    "# Ensure matching row count\n",
    "if len(predictions) != len(df):\n",
    "    raise ValueError(\"Mismatch in row count between processed data and predictions!\")\n",
    "\n",
    "# Construct results DataFrame with correct alignment\n",
    "results = pd.DataFrame({\n",
    "    \"SalesID\": sales_id_col[\"SalesID\"].values,  # Retrieve original SalesID\n",
    "    \"SalePrice\": predictions  # Reverse log transformation if needed\n",
    "})\n",
    "\n",
    "# Save to Excel\n",
    "results.to_excel(\"predictions.xlsx\", index=False)\n",
    "\n",
    "print(\"Predictions saved successfully.\")\n",
    "# Save to Excel\n",
    "file_path=r\"C:\\Users\\eitanb\\Documents\\DS\\ML\\ML_project\\DATA\"\n",
    "results.to_csv(file_path+\"\\predictions_12_2_1250.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d94237-658a-4aed-8b20-76d710611819",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
