{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58ec66a5-6762-4bf1-8974-3044a438234d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ec4e951-da90-4635-9814-d910e9e5c78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eitanb\\AppData\\Local\\Temp\\ipykernel_12692\\3239662035.py:4: DtypeWarning: Columns (13,39,40,41) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path_train)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 51\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestRegressor\n\u001b[0;32m     50\u001b[0m rf_model \u001b[38;5;241m=\u001b[39m RandomForestRegressor(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 51\u001b[0m rf_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_absolute_error, mean_squared_error\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:487\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    476\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    479\u001b[0m ]\n\u001b[0;32m    481\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 487\u001b[0m trees \u001b[38;5;241m=\u001b[39m Parallel(\n\u001b[0;32m    488\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[0;32m    489\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    490\u001b[0m     prefer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreads\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    491\u001b[0m )(\n\u001b[0;32m    492\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    493\u001b[0m         t,\n\u001b[0;32m    494\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbootstrap,\n\u001b[0;32m    495\u001b[0m         X,\n\u001b[0;32m    496\u001b[0m         y,\n\u001b[0;32m    497\u001b[0m         sample_weight,\n\u001b[0;32m    498\u001b[0m         i,\n\u001b[0;32m    499\u001b[0m         \u001b[38;5;28mlen\u001b[39m(trees),\n\u001b[0;32m    500\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    501\u001b[0m         class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight,\n\u001b[0;32m    502\u001b[0m         n_samples_bootstrap\u001b[38;5;241m=\u001b[39mn_samples_bootstrap,\n\u001b[0;32m    503\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[0;32m    504\u001b[0m     )\n\u001b[0;32m    505\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trees)\n\u001b[0;32m    506\u001b[0m )\n\u001b[0;32m    508\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path_train = r\"C:\\Users\\eitanb\\Documents\\DS\\ML\\ML_project\\DATA/Train.csv\"\n",
    "df = pd.read_csv(file_path_train)\n",
    "# Convert 'saledate' to datetime and extract year, month, and day\n",
    "df['saledate'] = pd.to_datetime(df['saledate'], errors='coerce')\n",
    "df['sale_year'] = df['saledate'].dt.year\n",
    "df['sale_month'] = df['saledate'].dt.month\n",
    "df['sale_day'] = df['saledate'].dt.day\n",
    "\n",
    "# Filter the dataset to only include the last 5 years of data\n",
    "recent_years = df['sale_year'].dropna().unique()\n",
    "recent_years.sort()\n",
    "selected_years = recent_years[-5:]\n",
    "df_filtered = df[df['sale_year'].isin(selected_years)]\n",
    "\n",
    "# Drop irrelevant columns: 'SalesID', 'saledate', and high-cardinality object columns\n",
    "high_cardinality_cols = [col for col in df_filtered.select_dtypes(include=['object']).columns if df_filtered[col].nunique() > 50]\n",
    "df_filtered = df_filtered.drop(columns=['SalesID', 'saledate'] + high_cardinality_cols, errors='ignore')\n",
    "\n",
    "# Drop rows with missing SalePrice (target variable)\n",
    "df_filtered = df_filtered.dropna(subset=['SalePrice'])\n",
    "\n",
    "# Identify categorical columns and convert them to category type\n",
    "categorical_cols = df_filtered.select_dtypes(include=['object']).columns\n",
    "df_filtered[categorical_cols] = df_filtered[categorical_cols].astype('category')\n",
    "\n",
    "# Fill missing values: numerical columns with median, categorical with mode\n",
    "for col in df_filtered.select_dtypes(include=['number']).columns:\n",
    "    df_filtered[col] = df_filtered[col].fillna(df_filtered[col].median())\n",
    "\n",
    "for col in categorical_cols:\n",
    "    df_filtered[col] = df_filtered[col].fillna(df_filtered[col].mode()[0])\n",
    "\n",
    "# Encode categorical variables\n",
    "df_encoded = pd.get_dummies(df_filtered, drop_first=True)\n",
    "\n",
    "# Split into features and target\n",
    "X = df_encoded.drop(columns=['SalePrice'])\n",
    "y = df_encoded['SalePrice']\n",
    "\n",
    "# Split data into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "y_pred = rf_model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mse ** 0.5\n",
    "\n",
    "# Display model performance\n",
    "mae, rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "06e4ef5e-0d84-44a1-b983-a3dd131c6358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(y_, y_pred_):\n",
    "    '''\n",
    "    RSME\n",
    "    '''\n",
    "    return ((y_ - y_pred_) ** 2).mean() ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d2b2ea22-dd36-4285-a523-5c6cad593936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feature_1_Enclosure_EROPS w AC_importance': 0.2892118595873668}\n",
      "{'feature_2_YearMade_importance': 0.12487402704702251}\n",
      "{'feature_3_ModelID_importance': 0.11829126119004313}\n",
      "{'feature_4_MachineID_importance': 0.04573237305124742}\n",
      "{'feature_5_Ripper_None or Unspecified_importance': 0.03802813823731736}\n",
      "{'feature_6_sale_year_importance': 0.03607100932771089}\n",
      "{'feature_7_ProductGroup_SSL_importance': 0.03221605291181244}\n",
      "{'feature_8_ProductSize_Mini_importance': 0.031867151945836086}\n",
      "{'feature_9_ProductGroupDesc_Skid Steer Loaders_importance': 0.029094141942740615}\n",
      "{'feature_10_ProductSize_Large / Medium_importance': 0.023168544653347077}\n",
      "{'feature_11_sale_month_importance': 0.01967121459907647}\n",
      "{'feature_12_sale_day_importance': 0.018800011754424743}\n",
      "{'feature_13_Tire_Size_None or Unspecified_importance': 0.015933707555163393}\n",
      "{'feature_14_Stick_Standard_importance': 0.015125249963802044}\n",
      "{'feature_15_MachineHoursCurrentMeter_importance': 0.011092164781110333}\n",
      "{'feature_16_ProductGroup_MG_importance': 0.009691991796865813}\n",
      "{'feature_17_ProductSize_Medium_importance': 0.008495047623967139}\n",
      "{'feature_18_Blade_Type_PAT_importance': 0.008314916561298229}\n",
      "{'feature_19_ProductGroupDesc_Motor Graders_importance': 0.008255383986772155}\n",
      "{'feature_20_Travel_Controls_Differential Steer_importance': 0.006818082285854086}\n",
      "{'feature_21_auctioneerID_importance': 0.006704725401803756}\n",
      "{'feature_22_ProductSize_Small_importance': 0.006260025979826085}\n",
      "{'feature_23_Blade_Type_Semi U_importance': 0.005597439975888464}\n",
      "{'feature_24_ProductGroup_WL_importance': 0.005529503887372531}\n",
      "{'feature_25_ProductGroupDesc_Track Type Tractors_importance': 0.005296222577071724}\n",
      "{'feature_26_ProductGroup_TTT_importance': 0.0049779693563850504}\n",
      "{'feature_27_ProductGroupDesc_Wheel Loader_importance': 0.00494350553369605}\n",
      "{'feature_28_ProductSize_Large_importance': 0.004857756228968381}\n",
      "{'feature_29_datasource_importance': 0.004217900390873422}\n",
      "{'feature_30_Drive_System_Four Wheel Drive_importance': 0.0029107158143156313}\n",
      "{'feature_31_Enclosure_OROPS_importance': 0.0028180626598277563}\n",
      "{'feature_32_Travel_Controls_None or Unspecified_importance': 0.0025466373567641126}\n",
      "{'feature_33_Pushblock_Yes_importance': 0.002491231351714629}\n",
      "{'feature_34_Tire_Size_20.5\"_importance': 0.0023947793024358824}\n",
      "{'feature_35_Ride_Control_None or Unspecified_importance': 0.002362688941015217}\n",
      "{'feature_36_Tire_Size_23.5_importance': 0.0023511176119616695}\n",
      "{'feature_37_Tire_Size_26.5_importance': 0.0023163364348338595}\n",
      "{'feature_38_ProductGroup_TEX_importance': 0.001987792426347376}\n",
      "{'feature_39_UsageBand_Low_importance': 0.0018579257383101021}\n",
      "{'feature_40_Tire_Size_20.5_importance': 0.0018004546274400233}\n",
      "{'feature_41_ProductGroupDesc_Track Excavators_importance': 0.0016877034320989664}\n",
      "{'feature_42_UsageBand_Medium_importance': 0.0014376564896573945}\n",
      "{'feature_43_Drive_System_Two Wheel Drive_importance': 0.0014135258529107803}\n",
      "{'feature_44_Ride_Control_Yes_importance': 0.001393904648008393}\n",
      "{'feature_45_Blade_Type_Straight_importance': 0.0013503522481587002}\n",
      "{'feature_46_Blade_Type_None or Unspecified_importance': 0.0012540465438347858}\n",
      "{'feature_47_Coupler_None or Unspecified_importance': 0.001175881605927573}\n",
      "{'feature_48_Tire_Size_17.5_importance': 0.0010768352963114113}\n",
      "{'feature_49_Hydraulics_Standard_importance': 0.0010145294504713424}\n",
      "{'feature_50_Thumb_None or Unspecified_importance': 0.0009011999611103724}\n",
      "{'feature_51_Coupler_Manual_importance': 0.0008468385761236542}\n",
      "{'feature_52_Hydraulics_Auxiliary_importance': 0.0008316333263829486}\n",
      "{'feature_53_Transmission_Standard_importance': 0.0008073426625249516}\n",
      "{'feature_54_Drive_System_No_importance': 0.0007974984512999771}\n",
      "{'feature_55_Blade_Type_VPAT_importance': 0.000758369791796468}\n",
      "{'feature_56_Grouser_Type_Triple_importance': 0.000744343158035293}\n",
      "{'feature_57_Transmission_None or Unspecified_importance': 0.0007023997680244225}\n",
      "{'feature_58_Tire_Size_29.5_importance': 0.0006997493519226446}\n",
      "{'feature_59_Forks_Yes_importance': 0.0006892506063183606}\n",
      "{'feature_60_Stick_Length_None or Unspecified_importance': 0.000663560969967143}\n",
      "{'feature_61_Scarifier_Yes_importance': 0.000648639083769754}\n",
      "{'feature_62_Travel_Controls_Finger Tip_importance': 0.00055538684460154}\n",
      "{'feature_63_Enclosure_Type_None or Unspecified_importance': 0.0005551184291411913}\n",
      "{'feature_64_Pattern_Changer_Yes_importance': 0.0005489240628878618}\n",
      "{'feature_65_Stick_Length_10\\' 6\"_importance': 0.0005122381739444443}\n",
      "{'feature_66_Hydraulics_3 Valve_importance': 0.0005055658888205715}\n",
      "{'feature_67_Transmission_Powershift_importance': 0.00047706749805326466}\n",
      "{'feature_68_Pattern_Changer_None or Unspecified_importance': 0.0004582361646736564}\n",
      "{'feature_69_Hydraulics_Base + 1 Function_importance': 0.00045418748175372897}\n",
      "{'feature_70_Hydraulics_4 Valve_importance': 0.0004422398548272187}\n",
      "{'feature_71_Thumb_Manual_importance': 0.00044131830338764574}\n",
      "{'feature_72_Track_Type_Steel_importance': 0.0004324334512671785}\n",
      "{'feature_73_Stick_Length_9\\' 7\"_importance': 0.00042210930989579104}\n",
      "{'feature_74_Ripper_Single Shank_importance': 0.0004173300798888233}\n",
      "{'feature_75_Undercarriage_Pad_Width_None or Unspecified_importance': 0.000413108124485997}\n",
      "{'feature_76_Stick_Length_9\\' 6\"_importance': 0.0004120606914984929}\n",
      "{'feature_77_Blade_Type_U_importance': 0.00039182875104541633}\n",
      "{'feature_78_Steering_Controls_Conventional_importance': 0.0003779721651868558}\n",
      "{'feature_79_Stick_Length_11\\' 0\"_importance': 0.00033391790560794143}\n",
      "{\"feature_80_Blade_Width_14'_importance\": 0.0003334794080660577}\n",
      "{'feature_81_Transmission_Hydrostatic_importance': 0.00031910232914103104}\n",
      "{'feature_82_Ripper_Yes_importance': 0.00030655921702263123}\n",
      "{'feature_83_Tip_Control_Sideshift & Tip_importance': 0.00027805420104487505}\n",
      "{'feature_84_Tip_Control_Tip_importance': 0.0002503071718517854}\n",
      "{'feature_85_Blade_Width_None or Unspecified_importance': 0.00023006163955574543}\n",
      "{'feature_86_Enclosure_Type_Low Profile_importance': 0.00022710196907311416}\n",
      "{'feature_87_Undercarriage_Pad_Width_32 inch_importance': 0.00022596322221109951}\n",
      "{'feature_88_Tire_Size_14\"_importance': 0.0002205724535192327}\n",
      "{'feature_89_Stick_Length_12\\' 10\"_importance': 0.0002072158093844847}\n",
      "{'feature_90_Undercarriage_Pad_Width_36 inch_importance': 0.00019456248474447564}\n",
      "{'feature_91_Engine_Horsepower_Variable_importance': 0.00018822917452012246}\n",
      "{'feature_92_Undercarriage_Pad_Width_24 inch_importance': 0.00018645009082318313}\n",
      "{\"feature_93_Blade_Width_16'_importance\": 0.00017478966441621395}\n",
      "{'feature_94_Stick_Length_10\\' 2\"_importance': 0.00015838736594029343}\n",
      "{'feature_95_Undercarriage_Pad_Width_28 inch_importance': 0.00015284352771302025}\n",
      "{'feature_96_Stick_Length_9\\' 8\"_importance': 0.0001519932348007204}\n",
      "{'feature_97_Tire_Size_17.5\"_importance': 0.00014211362574095246}\n",
      "{'feature_98_Stick_Length_9\\' 10\"_importance': 0.0001372981515233808}\n",
      "{'feature_99_Blade_Extension_Yes_importance': 0.00012281690599138989}\n",
      "{'feature_100_Travel_Controls_2 Pedal_importance': 0.00011932263341788458}\n",
      "{'feature_101_Undercarriage_Pad_Width_30 inch_importance': 0.00011703830656766191}\n",
      "{'feature_102_Travel_Controls_Lever_importance': 0.00010575930128854881}\n",
      "{'feature_103_Differential_Type_Standard_importance': 0.00010483795462683819}\n",
      "{'feature_104_Undercarriage_Pad_Width_34 inch_importance': 8.874519730587473e-05}\n",
      "{'feature_105_Tire_Size_15.5_importance': 8.858157061732308e-05}\n",
      "{'feature_106_Stick_Length_8\\' 6\"_importance': 7.607519099688617e-05}\n",
      "{'feature_107_Tire_Size_23.5\"_importance': 6.927197650431379e-05}\n",
      "{'feature_108_Transmission_Powershuttle_importance': 6.89046405913859e-05}\n",
      "{'feature_109_Tire_Size_15.5\"_importance': 6.32051611786152e-05}\n",
      "{'feature_110_Stick_Length_12\\' 8\"_importance': 6.0693372168643305e-05}\n",
      "{'feature_111_Pad_Type_None or Unspecified_importance': 5.5991326484681514e-05}\n",
      "{'feature_112_Undercarriage_Pad_Width_18 inch_importance': 5.4740566726224876e-05}\n",
      "{'feature_113_Stick_Length_8\\' 2\"_importance': 5.392211436140411e-05}\n",
      "{'feature_114_Stick_Length_12\\' 4\"_importance': 5.2005940627215476e-05}\n",
      "{'feature_115_Undercarriage_Pad_Width_20 inch_importance': 5.0335078125528675e-05}\n",
      "{'feature_116_Blade_Type_No_importance': 3.932682145287478e-05}\n",
      "{'feature_117_Undercarriage_Pad_Width_16 inch_importance': 3.234573435607984e-05}\n",
      "{'feature_118_Pad_Type_Reversible_importance': 3.16937990678635e-05}\n",
      "{'feature_119_Turbocharged_Yes_importance': 3.080084232286651e-05}\n",
      "{'feature_120_Tire_Size_13\"_importance': 2.9944697633242363e-05}\n",
      "{'feature_121_Grouser_Tracks_Yes_importance': 2.968055854167772e-05}\n",
      "{'feature_122_Pad_Type_Street_importance': 2.9546071446204572e-05}\n",
      "{\"feature_123_Blade_Width_13'_importance\": 2.9172038855922262e-05}\n",
      "{'feature_124_Stick_Length_11\\' 10\"_importance': 2.7163461619281673e-05}\n",
      "{'feature_125_Stick_Length_8\\' 10\"_importance': 2.6893009755217157e-05}\n",
      "{'feature_126_Hydraulics_Base + 3 Function_importance': 2.5393014738375107e-05}\n",
      "{'feature_127_Stick_Length_15\\' 9\"_importance': 2.2356239556191555e-05}\n",
      "{'feature_128_Hydraulics_Base + 2 Function_importance': 1.795965269021081e-05}\n",
      "{'feature_129_Stick_Length_8\\' 4\"_importance': 1.3519427451573302e-05}\n",
      "{'feature_130_Differential_Type_No Spin_importance': 1.2724565854324301e-05}\n",
      "{'feature_131_Blade_Type_Landfill_importance': 1.0411534474009822e-05}\n",
      "{'feature_132_Hydraulics_Flow_Standard_importance': 9.567404227766148e-06}\n",
      "{'feature_133_Hydraulics_Base + 4 Function_importance': 9.366679289497705e-06}\n",
      "{'feature_134_Undercarriage_Pad_Width_22 inch_importance': 9.019514080735478e-06}\n",
      "{'feature_135_Undercarriage_Pad_Width_26 inch_importance': 8.970933906814347e-06}\n",
      "{'feature_136_Tire_Size_23.1\"_importance': 8.86337605379699e-06}\n",
      "{'feature_137_Stick_Length_9\\' 5\"_importance': 8.315835590795145e-06}\n",
      "{'feature_138_Coupler_System_Yes_importance': 7.971979187397576e-06}\n",
      "{'feature_139_Undercarriage_Pad_Width_31 inch_importance': 7.841293813527104e-06}\n",
      "{'feature_140_Travel_Controls_Pedal_importance': 7.31515909960091e-06}\n",
      "{'feature_141_Transmission_Direct Drive_importance': 7.024332218700946e-06}\n",
      "{'feature_142_Hydraulics_Base + 5 Function_importance': 6.2918575079690616e-06}\n",
      "{'feature_143_Tire_Size_7.0\"_importance': 5.761480480825475e-06}\n",
      "{'feature_144_Stick_Length_13\\' 7\"_importance': 5.084555427731748e-06}\n",
      "{'feature_145_Undercarriage_Pad_Width_33 inch_importance': 4.095005863753441e-06}\n",
      "{'feature_146_Steering_Controls_Four Wheel Standard_importance': 4.02992426767865e-06}\n",
      "{'feature_147_Hydraulics_Base + 6 Function_importance': 3.661759777558964e-06}\n",
      "{'feature_148_Undercarriage_Pad_Width_27 inch_importance': 3.3760058897366608e-06}\n",
      "{'feature_149_Stick_Length_15\\' 4\"_importance': 3.265385824501046e-06}\n",
      "{\"feature_150_Blade_Width_<12'_importance\": 2.4495604959957e-06}\n",
      "{'feature_151_Enclosure_EROPS AC_importance': 2.1527578311201885e-06}\n",
      "{'feature_152_Hydraulics_None or Unspecified_importance': 2.017895598048393e-06}\n",
      "{'feature_153_Stick_Length_6\\' 3\"_importance': 1.2431663948820138e-06}\n",
      "{'feature_154_Transmission_Autoshift_importance': 9.082932084227495e-07}\n",
      "{'feature_155_Stick_Length_14\\' 1\"_importance': 7.249486863963477e-07}\n",
      "{'feature_156_Blade_Type_Coal_importance': 7.069292206544309e-07}\n",
      "{'feature_157_Stick_Length_19\\' 8\"_importance': 6.193069816836594e-07}\n",
      "{'feature_158_Undercarriage_Pad_Width_15 inch_importance': 5.154019269347487e-07}\n",
      "{'feature_159_Enclosure_NO ROPS_importance': 3.1890863504081583e-07}\n",
      "{'feature_160_Differential_Type_Locking_importance': 2.2627469854924588e-07}\n",
      "{'feature_161_Hydraulics_Flow_None or Unspecified_importance': 2.2379880423516433e-07}\n",
      "{'feature_162_Undercarriage_Pad_Width_25 inch_importance': 2.1160716360816834e-07}\n",
      "{'feature_163_Stick_Length_13\\' 9\"_importance': 8.773361119321497e-08}\n",
      "{'feature_164_Steering_Controls_Wheel_importance': 5.672691515450271e-08}\n",
      "{'feature_165_Backhoe_Mounting_Yes_importance': 2.3903577634808744e-08}\n",
      "{'feature_166_Enclosure_None or Unspecified_importance': 0.0}\n",
      "{'feature_167_Undercarriage_Pad_Width_31.5 inch_importance': 0.0}\n",
      "{'feature_168_Stick_Length_13\\' 10\"_importance': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# Optional: Log feature importances\n",
    "importances = rf_model.feature_importances_\n",
    "\n",
    "# Create a zip of feature names and feature importances\n",
    "feature_zip = zip(rf_model.feature_names_in_, rf_model.feature_importances_)\n",
    "\n",
    "# Sort the zip by feature importance in descending order\n",
    "sorted_feature_zip = sorted(feature_zip, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature names, importances, and index\n",
    "for idx, (feat, importance) in enumerate(sorted_feature_zip, 1):\n",
    "  print({f\"feature_{idx}_{feat}_importance\" : importance})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0aa5ae-d9a9-4ad7-a6e7-ccbce62f3c29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1333dc6-0bd3-4d80-8624-742cdb7489ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eitanb\\AppData\\Local\\Temp\\ipykernel_31536\\255710431.py:8: DtypeWarning: Columns (13,39,40,41) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path_train)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5036.5769087707195, 8135.890307052758)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-import necessary libraries due to execution reset\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "\n",
    "ProductSize_path=r'C:\\Users\\eitanb\\Documents\\DS\\ML\\ML_project\\DATA/ProductSize_Null_fix_model.pkl'\n",
    "file_path_train = r\"C:\\Users\\eitanb\\Documents\\DS\\ML\\ML_project\\DATA/Train.csv\"\n",
    "df = pd.read_csv(file_path_train)\n",
    "\n",
    "# Convert 'saledate' to datetime and extract year, month, and day\n",
    "df['saledate'] = pd.to_datetime(df['saledate'], errors='coerce')\n",
    "df['sale_year'] = df['saledate'].dt.year\n",
    "df['sale_month'] = df['saledate'].dt.month\n",
    "df['sale_day'] = df['saledate'].dt.day\n",
    "\n",
    "# Filter dataset to only include the last 5 years\n",
    "recent_years = df['sale_year'].dropna().unique()\n",
    "recent_years.sort()\n",
    "selected_years = recent_years[-5:]\n",
    "df_filtered = df[df['sale_year'].isin(selected_years)]\n",
    "\n",
    "# Drop irrelevant columns: 'SalesID', 'saledate'\n",
    "# Drop irrelevant columns: 'SalesID', 'saledate', and high-cardinality object columns\n",
    "high_cardinality_cols = [col for col in df_filtered.select_dtypes(include=['object']).columns if df_filtered[col].nunique() > 50]\n",
    "df_filtered = df_filtered.drop(columns=['SalesID', 'saledate'] + high_cardinality_cols, errors='ignore')\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = df_filtered.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Identify high cardinality categorical columns (≥ 50 unique values)\n",
    "high_cardinality_cols = [col for col in categorical_cols if df_filtered[col].nunique() >= 50]\n",
    "moderate_cardinality_cols = [col for col in categorical_cols if 10 <= df_filtered[col].nunique() < 50]\n",
    "low_cardinality_cols = [col for col in categorical_cols if df_filtered[col].nunique() < 10]\n",
    "\n",
    "# Fill missing categorical values with \"Unknown\"\n",
    "for col in categorical_cols:\n",
    "    df_filtered[col] = df_filtered[col].fillna(\"Unknown\")\n",
    "\n",
    "# Encoding strategy\n",
    "\n",
    "# Apply One-Hot Encoding (OHE) to low-cardinality categories\n",
    "df_encoded = pd.get_dummies(df_filtered, columns=low_cardinality_cols, drop_first=True)\n",
    "\n",
    "# Apply Ordinal Encoding to moderate-cardinality categories\n",
    "if moderate_cardinality_cols:\n",
    "    ord_encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "    df_encoded[moderate_cardinality_cols] = ord_encoder.fit_transform(df_encoded[moderate_cardinality_cols])\n",
    "\n",
    "# Apply Frequency Encoding to high-cardinality categories\n",
    "for col in high_cardinality_cols:\n",
    "    freq_encoding = df_encoded[col].value_counts().to_dict()\n",
    "    df_encoded[col] = df_encoded[col].map(freq_encoding)\n",
    "\n",
    "# Drop rows with missing SalePrice (target variable)\n",
    "df_encoded = df_encoded.dropna(subset=['SalePrice'])\n",
    "\n",
    "# Split into features and target\n",
    "X = df_encoded.drop(columns=['SalePrice'])\n",
    "y = df_encoded['SalePrice']\n",
    "\n",
    "# Split data into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "y_pred = rf_model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mse ** 0.5\n",
    "\n",
    "# Display model performance\n",
    "mae, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "30c3eaa9-7c2b-4845-8414-c9de21be350a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feature_1_Enclosure_EROPS w AC_importance': 0.2892118595873668}\n",
      "{'feature_2_YearMade_importance': 0.12487402704702251}\n",
      "{'feature_3_ModelID_importance': 0.11829126119004313}\n",
      "{'feature_4_MachineID_importance': 0.04573237305124742}\n",
      "{'feature_5_Ripper_None or Unspecified_importance': 0.03802813823731736}\n",
      "{'feature_6_sale_year_importance': 0.03607100932771089}\n",
      "{'feature_7_ProductGroup_SSL_importance': 0.03221605291181244}\n",
      "{'feature_8_ProductSize_Mini_importance': 0.031867151945836086}\n",
      "{'feature_9_ProductGroupDesc_Skid Steer Loaders_importance': 0.029094141942740615}\n",
      "{'feature_10_ProductSize_Large / Medium_importance': 0.023168544653347077}\n",
      "{'feature_11_sale_month_importance': 0.01967121459907647}\n",
      "{'feature_12_sale_day_importance': 0.018800011754424743}\n",
      "{'feature_13_Tire_Size_None or Unspecified_importance': 0.015933707555163393}\n",
      "{'feature_14_Stick_Standard_importance': 0.015125249963802044}\n",
      "{'feature_15_MachineHoursCurrentMeter_importance': 0.011092164781110333}\n",
      "{'feature_16_ProductGroup_MG_importance': 0.009691991796865813}\n",
      "{'feature_17_ProductSize_Medium_importance': 0.008495047623967139}\n",
      "{'feature_18_Blade_Type_PAT_importance': 0.008314916561298229}\n",
      "{'feature_19_ProductGroupDesc_Motor Graders_importance': 0.008255383986772155}\n",
      "{'feature_20_Travel_Controls_Differential Steer_importance': 0.006818082285854086}\n",
      "{'feature_21_auctioneerID_importance': 0.006704725401803756}\n",
      "{'feature_22_ProductSize_Small_importance': 0.006260025979826085}\n",
      "{'feature_23_Blade_Type_Semi U_importance': 0.005597439975888464}\n",
      "{'feature_24_ProductGroup_WL_importance': 0.005529503887372531}\n",
      "{'feature_25_ProductGroupDesc_Track Type Tractors_importance': 0.005296222577071724}\n",
      "{'feature_26_ProductGroup_TTT_importance': 0.0049779693563850504}\n",
      "{'feature_27_ProductGroupDesc_Wheel Loader_importance': 0.00494350553369605}\n",
      "{'feature_28_ProductSize_Large_importance': 0.004857756228968381}\n",
      "{'feature_29_datasource_importance': 0.004217900390873422}\n",
      "{'feature_30_Drive_System_Four Wheel Drive_importance': 0.0029107158143156313}\n",
      "{'feature_31_Enclosure_OROPS_importance': 0.0028180626598277563}\n",
      "{'feature_32_Travel_Controls_None or Unspecified_importance': 0.0025466373567641126}\n",
      "{'feature_33_Pushblock_Yes_importance': 0.002491231351714629}\n",
      "{'feature_34_Tire_Size_20.5\"_importance': 0.0023947793024358824}\n",
      "{'feature_35_Ride_Control_None or Unspecified_importance': 0.002362688941015217}\n",
      "{'feature_36_Tire_Size_23.5_importance': 0.0023511176119616695}\n",
      "{'feature_37_Tire_Size_26.5_importance': 0.0023163364348338595}\n",
      "{'feature_38_ProductGroup_TEX_importance': 0.001987792426347376}\n",
      "{'feature_39_UsageBand_Low_importance': 0.0018579257383101021}\n",
      "{'feature_40_Tire_Size_20.5_importance': 0.0018004546274400233}\n",
      "{'feature_41_ProductGroupDesc_Track Excavators_importance': 0.0016877034320989664}\n",
      "{'feature_42_UsageBand_Medium_importance': 0.0014376564896573945}\n",
      "{'feature_43_Drive_System_Two Wheel Drive_importance': 0.0014135258529107803}\n",
      "{'feature_44_Ride_Control_Yes_importance': 0.001393904648008393}\n",
      "{'feature_45_Blade_Type_Straight_importance': 0.0013503522481587002}\n",
      "{'feature_46_Blade_Type_None or Unspecified_importance': 0.0012540465438347858}\n",
      "{'feature_47_Coupler_None or Unspecified_importance': 0.001175881605927573}\n",
      "{'feature_48_Tire_Size_17.5_importance': 0.0010768352963114113}\n",
      "{'feature_49_Hydraulics_Standard_importance': 0.0010145294504713424}\n",
      "{'feature_50_Thumb_None or Unspecified_importance': 0.0009011999611103724}\n",
      "{'feature_51_Coupler_Manual_importance': 0.0008468385761236542}\n",
      "{'feature_52_Hydraulics_Auxiliary_importance': 0.0008316333263829486}\n",
      "{'feature_53_Transmission_Standard_importance': 0.0008073426625249516}\n",
      "{'feature_54_Drive_System_No_importance': 0.0007974984512999771}\n",
      "{'feature_55_Blade_Type_VPAT_importance': 0.000758369791796468}\n",
      "{'feature_56_Grouser_Type_Triple_importance': 0.000744343158035293}\n",
      "{'feature_57_Transmission_None or Unspecified_importance': 0.0007023997680244225}\n",
      "{'feature_58_Tire_Size_29.5_importance': 0.0006997493519226446}\n",
      "{'feature_59_Forks_Yes_importance': 0.0006892506063183606}\n",
      "{'feature_60_Stick_Length_None or Unspecified_importance': 0.000663560969967143}\n",
      "{'feature_61_Scarifier_Yes_importance': 0.000648639083769754}\n",
      "{'feature_62_Travel_Controls_Finger Tip_importance': 0.00055538684460154}\n",
      "{'feature_63_Enclosure_Type_None or Unspecified_importance': 0.0005551184291411913}\n",
      "{'feature_64_Pattern_Changer_Yes_importance': 0.0005489240628878618}\n",
      "{'feature_65_Stick_Length_10\\' 6\"_importance': 0.0005122381739444443}\n",
      "{'feature_66_Hydraulics_3 Valve_importance': 0.0005055658888205715}\n",
      "{'feature_67_Transmission_Powershift_importance': 0.00047706749805326466}\n",
      "{'feature_68_Pattern_Changer_None or Unspecified_importance': 0.0004582361646736564}\n",
      "{'feature_69_Hydraulics_Base + 1 Function_importance': 0.00045418748175372897}\n",
      "{'feature_70_Hydraulics_4 Valve_importance': 0.0004422398548272187}\n",
      "{'feature_71_Thumb_Manual_importance': 0.00044131830338764574}\n",
      "{'feature_72_Track_Type_Steel_importance': 0.0004324334512671785}\n",
      "{'feature_73_Stick_Length_9\\' 7\"_importance': 0.00042210930989579104}\n",
      "{'feature_74_Ripper_Single Shank_importance': 0.0004173300798888233}\n",
      "{'feature_75_Undercarriage_Pad_Width_None or Unspecified_importance': 0.000413108124485997}\n",
      "{'feature_76_Stick_Length_9\\' 6\"_importance': 0.0004120606914984929}\n",
      "{'feature_77_Blade_Type_U_importance': 0.00039182875104541633}\n",
      "{'feature_78_Steering_Controls_Conventional_importance': 0.0003779721651868558}\n",
      "{'feature_79_Stick_Length_11\\' 0\"_importance': 0.00033391790560794143}\n",
      "{\"feature_80_Blade_Width_14'_importance\": 0.0003334794080660577}\n",
      "{'feature_81_Transmission_Hydrostatic_importance': 0.00031910232914103104}\n",
      "{'feature_82_Ripper_Yes_importance': 0.00030655921702263123}\n",
      "{'feature_83_Tip_Control_Sideshift & Tip_importance': 0.00027805420104487505}\n",
      "{'feature_84_Tip_Control_Tip_importance': 0.0002503071718517854}\n",
      "{'feature_85_Blade_Width_None or Unspecified_importance': 0.00023006163955574543}\n",
      "{'feature_86_Enclosure_Type_Low Profile_importance': 0.00022710196907311416}\n",
      "{'feature_87_Undercarriage_Pad_Width_32 inch_importance': 0.00022596322221109951}\n",
      "{'feature_88_Tire_Size_14\"_importance': 0.0002205724535192327}\n",
      "{'feature_89_Stick_Length_12\\' 10\"_importance': 0.0002072158093844847}\n",
      "{'feature_90_Undercarriage_Pad_Width_36 inch_importance': 0.00019456248474447564}\n",
      "{'feature_91_Engine_Horsepower_Variable_importance': 0.00018822917452012246}\n",
      "{'feature_92_Undercarriage_Pad_Width_24 inch_importance': 0.00018645009082318313}\n",
      "{\"feature_93_Blade_Width_16'_importance\": 0.00017478966441621395}\n",
      "{'feature_94_Stick_Length_10\\' 2\"_importance': 0.00015838736594029343}\n",
      "{'feature_95_Undercarriage_Pad_Width_28 inch_importance': 0.00015284352771302025}\n",
      "{'feature_96_Stick_Length_9\\' 8\"_importance': 0.0001519932348007204}\n",
      "{'feature_97_Tire_Size_17.5\"_importance': 0.00014211362574095246}\n",
      "{'feature_98_Stick_Length_9\\' 10\"_importance': 0.0001372981515233808}\n",
      "{'feature_99_Blade_Extension_Yes_importance': 0.00012281690599138989}\n",
      "{'feature_100_Travel_Controls_2 Pedal_importance': 0.00011932263341788458}\n",
      "{'feature_101_Undercarriage_Pad_Width_30 inch_importance': 0.00011703830656766191}\n",
      "{'feature_102_Travel_Controls_Lever_importance': 0.00010575930128854881}\n",
      "{'feature_103_Differential_Type_Standard_importance': 0.00010483795462683819}\n",
      "{'feature_104_Undercarriage_Pad_Width_34 inch_importance': 8.874519730587473e-05}\n",
      "{'feature_105_Tire_Size_15.5_importance': 8.858157061732308e-05}\n",
      "{'feature_106_Stick_Length_8\\' 6\"_importance': 7.607519099688617e-05}\n",
      "{'feature_107_Tire_Size_23.5\"_importance': 6.927197650431379e-05}\n",
      "{'feature_108_Transmission_Powershuttle_importance': 6.89046405913859e-05}\n",
      "{'feature_109_Tire_Size_15.5\"_importance': 6.32051611786152e-05}\n",
      "{'feature_110_Stick_Length_12\\' 8\"_importance': 6.0693372168643305e-05}\n",
      "{'feature_111_Pad_Type_None or Unspecified_importance': 5.5991326484681514e-05}\n",
      "{'feature_112_Undercarriage_Pad_Width_18 inch_importance': 5.4740566726224876e-05}\n",
      "{'feature_113_Stick_Length_8\\' 2\"_importance': 5.392211436140411e-05}\n",
      "{'feature_114_Stick_Length_12\\' 4\"_importance': 5.2005940627215476e-05}\n",
      "{'feature_115_Undercarriage_Pad_Width_20 inch_importance': 5.0335078125528675e-05}\n",
      "{'feature_116_Blade_Type_No_importance': 3.932682145287478e-05}\n",
      "{'feature_117_Undercarriage_Pad_Width_16 inch_importance': 3.234573435607984e-05}\n",
      "{'feature_118_Pad_Type_Reversible_importance': 3.16937990678635e-05}\n",
      "{'feature_119_Turbocharged_Yes_importance': 3.080084232286651e-05}\n",
      "{'feature_120_Tire_Size_13\"_importance': 2.9944697633242363e-05}\n",
      "{'feature_121_Grouser_Tracks_Yes_importance': 2.968055854167772e-05}\n",
      "{'feature_122_Pad_Type_Street_importance': 2.9546071446204572e-05}\n",
      "{\"feature_123_Blade_Width_13'_importance\": 2.9172038855922262e-05}\n",
      "{'feature_124_Stick_Length_11\\' 10\"_importance': 2.7163461619281673e-05}\n",
      "{'feature_125_Stick_Length_8\\' 10\"_importance': 2.6893009755217157e-05}\n",
      "{'feature_126_Hydraulics_Base + 3 Function_importance': 2.5393014738375107e-05}\n",
      "{'feature_127_Stick_Length_15\\' 9\"_importance': 2.2356239556191555e-05}\n",
      "{'feature_128_Hydraulics_Base + 2 Function_importance': 1.795965269021081e-05}\n",
      "{'feature_129_Stick_Length_8\\' 4\"_importance': 1.3519427451573302e-05}\n",
      "{'feature_130_Differential_Type_No Spin_importance': 1.2724565854324301e-05}\n",
      "{'feature_131_Blade_Type_Landfill_importance': 1.0411534474009822e-05}\n",
      "{'feature_132_Hydraulics_Flow_Standard_importance': 9.567404227766148e-06}\n",
      "{'feature_133_Hydraulics_Base + 4 Function_importance': 9.366679289497705e-06}\n",
      "{'feature_134_Undercarriage_Pad_Width_22 inch_importance': 9.019514080735478e-06}\n",
      "{'feature_135_Undercarriage_Pad_Width_26 inch_importance': 8.970933906814347e-06}\n",
      "{'feature_136_Tire_Size_23.1\"_importance': 8.86337605379699e-06}\n",
      "{'feature_137_Stick_Length_9\\' 5\"_importance': 8.315835590795145e-06}\n",
      "{'feature_138_Coupler_System_Yes_importance': 7.971979187397576e-06}\n",
      "{'feature_139_Undercarriage_Pad_Width_31 inch_importance': 7.841293813527104e-06}\n",
      "{'feature_140_Travel_Controls_Pedal_importance': 7.31515909960091e-06}\n",
      "{'feature_141_Transmission_Direct Drive_importance': 7.024332218700946e-06}\n",
      "{'feature_142_Hydraulics_Base + 5 Function_importance': 6.2918575079690616e-06}\n",
      "{'feature_143_Tire_Size_7.0\"_importance': 5.761480480825475e-06}\n",
      "{'feature_144_Stick_Length_13\\' 7\"_importance': 5.084555427731748e-06}\n",
      "{'feature_145_Undercarriage_Pad_Width_33 inch_importance': 4.095005863753441e-06}\n",
      "{'feature_146_Steering_Controls_Four Wheel Standard_importance': 4.02992426767865e-06}\n",
      "{'feature_147_Hydraulics_Base + 6 Function_importance': 3.661759777558964e-06}\n",
      "{'feature_148_Undercarriage_Pad_Width_27 inch_importance': 3.3760058897366608e-06}\n",
      "{'feature_149_Stick_Length_15\\' 4\"_importance': 3.265385824501046e-06}\n",
      "{\"feature_150_Blade_Width_<12'_importance\": 2.4495604959957e-06}\n",
      "{'feature_151_Enclosure_EROPS AC_importance': 2.1527578311201885e-06}\n",
      "{'feature_152_Hydraulics_None or Unspecified_importance': 2.017895598048393e-06}\n",
      "{'feature_153_Stick_Length_6\\' 3\"_importance': 1.2431663948820138e-06}\n",
      "{'feature_154_Transmission_Autoshift_importance': 9.082932084227495e-07}\n",
      "{'feature_155_Stick_Length_14\\' 1\"_importance': 7.249486863963477e-07}\n",
      "{'feature_156_Blade_Type_Coal_importance': 7.069292206544309e-07}\n",
      "{'feature_157_Stick_Length_19\\' 8\"_importance': 6.193069816836594e-07}\n",
      "{'feature_158_Undercarriage_Pad_Width_15 inch_importance': 5.154019269347487e-07}\n",
      "{'feature_159_Enclosure_NO ROPS_importance': 3.1890863504081583e-07}\n",
      "{'feature_160_Differential_Type_Locking_importance': 2.2627469854924588e-07}\n",
      "{'feature_161_Hydraulics_Flow_None or Unspecified_importance': 2.2379880423516433e-07}\n",
      "{'feature_162_Undercarriage_Pad_Width_25 inch_importance': 2.1160716360816834e-07}\n",
      "{'feature_163_Stick_Length_13\\' 9\"_importance': 8.773361119321497e-08}\n",
      "{'feature_164_Steering_Controls_Wheel_importance': 5.672691515450271e-08}\n",
      "{'feature_165_Backhoe_Mounting_Yes_importance': 2.3903577634808744e-08}\n",
      "{'feature_166_Enclosure_None or Unspecified_importance': 0.0}\n",
      "{'feature_167_Undercarriage_Pad_Width_31.5 inch_importance': 0.0}\n",
      "{'feature_168_Stick_Length_13\\' 10\"_importance': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# Optional: Log feature importances\n",
    "importances = rf_model.feature_importances_\n",
    "\n",
    "# Create a zip of feature names and feature importances\n",
    "feature_zip = zip(rf_model.feature_names_in_, rf_model.feature_importances_)\n",
    "\n",
    "# Sort the zip by feature importance in descending order\n",
    "sorted_feature_zip = sorted(feature_zip, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature names, importances, and index\n",
    "for idx, (feat, importance) in enumerate(sorted_feature_zip, 1):\n",
    "  print({f\"feature_{idx}_{feat}_importance\" : importance})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "528198ad-9b0b-4d08-9265-329659dad23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_YearMade(df, model_id):\n",
    "    valid_years = df.loc[(df['ModelID'] == model_id) & (df['YearMade'] > 1000), 'YearMade']\n",
    "    return int(valid_years.median()) if not valid_years.empty else None  # Return None if no valid years exist\n",
    "\n",
    "def update_YearMade(df):\n",
    "    print(\"update_YearMade START\")\n",
    "    \n",
    "    # Compute median YearMade for each ModelID\n",
    "    model_medians = df.loc[df['YearMade'] > 1000].groupby('ModelID')['YearMade'].median()\n",
    "    \n",
    "    # Update YearMade where it is 1000 using ModelID median\n",
    "    df.loc[df['YearMade'] == 1000, 'YearMade'] = df['ModelID'].map(model_medians)\n",
    "    \n",
    "    # Compute overall median YearMade excluding 1000 values\n",
    "    overall_median = df.loc[df['YearMade'] > 1000, 'YearMade'].median()\n",
    "    \n",
    "    # Replace any remaining 1000 values with overall median\n",
    "    df['YearMade'].fillna(overall_median, inplace=True)\n",
    "        \n",
    "    # Bucketize YearMade\n",
    "    bins = [0, 1980, 1995, 2005, 2010]\n",
    "    labels = [\"Before 1980\", \"1980-1995\", \"1996-2005\", \"2006-2010\"]\n",
    "    df['YearMade_Bucket'] = pd.cut(df['YearMade'], bins=bins, labels=labels, right=True)\n",
    "    \n",
    "    print(\"update_YearMade END\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc05ffdb-f7eb-418b-bf67-c791429cc015",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eitanb\\AppData\\Local\\Temp\\ipykernel_31536\\4082236078.py:9: DtypeWarning: Columns (13,39,40,41) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update_YearMade START\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eitanb\\AppData\\Local\\Temp\\ipykernel_31536\\4082236078.py:25: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[1980. 1980. 1994. ... 2003. 2004. 2004.]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[df['YearMade'] == 1000, 'YearMade'] = df['ModelID'].map(model_medians)\n",
      "C:\\Users\\eitanb\\AppData\\Local\\Temp\\ipykernel_31536\\4082236078.py:31: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['YearMade'].fillna(overall_median, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update_YearMade END\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4901.082367667127, 7917.90265227888)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-import necessary libraries due to execution reset\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "\n",
    "# Reload the dataset\n",
    "ProductSize_path=r'C:\\Users\\eitanb\\Documents\\DS\\ML\\ML_project\\DATA/ProductSize_Null_fix_model.pkl'\n",
    "file_path_train = r\"C:\\Users\\eitanb\\Documents\\DS\\ML\\ML_project\\DATA/Train.csv\"\n",
    "df = pd.read_csv(file_path_train)\n",
    "\n",
    "# Convert 'saledate' to datetime and extract year, month, and day\n",
    "df['saledate'] = pd.to_datetime(df['saledate'], errors='coerce')\n",
    "df['sale_year'] = df['saledate'].dt.year\n",
    "df['sale_month'] = df['saledate'].dt.month\n",
    "df['sale_day'] = df['saledate'].dt.day\n",
    "\n",
    "# Define function to update 'YearMade' and create 'YearMade_Bucket'\n",
    "def update_YearMade(df):\n",
    "    print(\"update_YearMade START\")\n",
    "    \n",
    "    # Compute median YearMade for each ModelID\n",
    "    model_medians = df.loc[df['YearMade'] > 1000].groupby('ModelID')['YearMade'].median()\n",
    "    \n",
    "    # Update YearMade where it is 1000 using ModelID median\n",
    "    df.loc[df['YearMade'] == 1000, 'YearMade'] = df['ModelID'].map(model_medians)\n",
    "    \n",
    "    # Compute overall median YearMade excluding 1000 values\n",
    "    overall_median = df.loc[df['YearMade'] > 1000, 'YearMade'].median()\n",
    "    \n",
    "    # Replace any remaining 1000 values with overall median\n",
    "    df['YearMade'].fillna(overall_median, inplace=True)\n",
    "        \n",
    "    # Bucketize YearMade\n",
    "    bins = [0, 1980, 1995, 2005, 2010, 2025]\n",
    "    labels = [\"Before 1980\", \"1980-1995\", \"1996-2005\", \"2006-2010\", \"2011-2025\"]\n",
    "    df['YearMade_Bucket'] = pd.cut(df['YearMade'], bins=bins, labels=labels, right=True)\n",
    "    \n",
    "    print(\"update_YearMade END\")\n",
    "    return df\n",
    "\n",
    "# Apply YearMade update\n",
    "df = update_YearMade(df)\n",
    "\n",
    "# Filter dataset to only include the last 5 years\n",
    "recent_years = df['sale_year'].dropna().unique()\n",
    "recent_years.sort()\n",
    "selected_years = recent_years[-5:]\n",
    "df_filtered = df[df['sale_year'].isin(selected_years)]\n",
    "\n",
    "# Drop irrelevant columns: 'SalesID', 'saledate'\n",
    "high_cardinality_cols = [col for col in df_filtered.select_dtypes(include=['object']).columns if df_filtered[col].nunique() > 50]\n",
    "df_filtered = df_filtered.drop(columns=['SalesID', 'saledate'] + high_cardinality_cols, errors='ignore')\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = df_filtered.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Apply Ordinal Encoding to 'YearMade_Bucket'\n",
    "year_bucket_encoder = OrdinalEncoder(categories=[[\"Before 1980\", \"1980-1995\", \"1996-2005\", \"2006-2010\", \"2011-2025\"]])\n",
    "df_filtered[\"YearMade_Bucket\"] = year_bucket_encoder.fit_transform(df_filtered[[\"YearMade_Bucket\"]])\n",
    "\n",
    "# Identify high, moderate, and low cardinality categorical columns\n",
    "high_cardinality_cols = [col for col in categorical_cols if df_filtered[col].nunique() >= 50]\n",
    "moderate_cardinality_cols = [col for col in categorical_cols if 10 <= df_filtered[col].nunique() < 50]\n",
    "low_cardinality_cols = [col for col in categorical_cols if df_filtered[col].nunique() < 10]\n",
    "\n",
    "# Fill missing categorical values with \"Unknown\"\n",
    "for col in categorical_cols:\n",
    "    df_filtered[col] = df_filtered[col].fillna(\"Unknown\")\n",
    "\n",
    "# Encoding strategy\n",
    "\n",
    "# Apply One-Hot Encoding (OHE) to low-cardinality categories\n",
    "df_encoded = pd.get_dummies(df_filtered, columns=low_cardinality_cols, drop_first=True)\n",
    "\n",
    "# Apply Ordinal Encoding to moderate-cardinality categories\n",
    "if moderate_cardinality_cols:\n",
    "    ord_encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "    df_encoded[moderate_cardinality_cols] = ord_encoder.fit_transform(df_encoded[moderate_cardinality_cols])\n",
    "\n",
    "# Apply Frequency Encoding to high-cardinality categories\n",
    "for col in high_cardinality_cols:\n",
    "    freq_encoding = df_encoded[col].value_counts().to_dict()\n",
    "    df_encoded[col] = df_encoded[col].map(freq_encoding)\n",
    "\n",
    "# Drop rows with missing SalePrice (target variable)\n",
    "df_encoded = df_encoded.dropna(subset=['SalePrice'])\n",
    "\n",
    "# Split into features and target\n",
    "X = df_encoded.drop(columns=['SalePrice'])\n",
    "y = df_encoded['SalePrice']\n",
    "\n",
    "# Split data into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "y_pred = rf_model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mse ** 0.5\n",
    "\n",
    "# Display model performance\n",
    "mae, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3fbe4d2f-fca0-40d3-b3b0-dd1650ba7759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_product_size(df):\n",
    "    \"\"\"\n",
    "    Function to preprocess and impute missing ProductSize values based on fiProductClassDesc.\n",
    "    \n",
    "    Steps:\n",
    "    1. Extract Product Type and Metric Tons / Horsepower from fiProductClassDesc.\n",
    "    2. Identify Unit Type (Metric Tons, Horsepower, or Lb Operating Capacity).\n",
    "    3. Convert Metric Tons / Horsepower to numerical values.\n",
    "    4. Use Metric Tons to impute missing ProductSize values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Extract Product Type and Size Descriptor\n",
    "    def split_product_desc(desc):\n",
    "        if pd.isna(desc):\n",
    "            return pd.NA, pd.NA\n",
    "        match = re.search(r'(.+?)-\\s*([\\d\\.]+.*)', desc)\n",
    "        if match:\n",
    "            return match.group(1).strip(), match.group(2).strip()\n",
    "        return desc.strip(), pd.NA  # If no match, return full description as product\n",
    "    \n",
    "    df[['Product_Type', 'Metric_Tons_HP']] = df['fiProductClassDesc'].apply(lambda x: pd.Series(split_product_desc(x)))\n",
    "    \n",
    "    # Step 2: Identify Unit Type\n",
    "    def extract_unit_type(desc):\n",
    "        if pd.isna(desc):\n",
    "            return pd.NA\n",
    "        if \"Metric Tons\" in desc:\n",
    "            return \"Metric Tons\"\n",
    "        elif \"Horsepower\" in desc:\n",
    "            return \"Horsepower\"\n",
    "        elif \"Lb Operating Capacity\" in desc:\n",
    "            return \"Lb Operating Capacity\"\n",
    "        return pd.NA\n",
    "    \n",
    "    df[\"Unit_Type\"] = df[\"Metric_Tons_HP\"].apply(extract_unit_type)\n",
    "    \n",
    "    # Step 3: Convert Metric Tons to numerical values\n",
    "    def extract_numeric_range(value):\n",
    "        \"\"\"Extracts the average value from a range like '12.0 to 14.0 Metric Tons'.\"\"\"\n",
    "        if pd.isna(value) or not isinstance(value, str):\n",
    "            return np.nan\n",
    "        numbers = re.findall(r'[\\d\\.]+', value)\n",
    "        if len(numbers) == 2:  # If range exists, take the average\n",
    "            return (float(numbers[0]) + float(numbers[1])) / 2\n",
    "        elif len(numbers) == 1:  # If only one number exists, use it\n",
    "            return float(numbers[0])\n",
    "        return np.nan\n",
    "    \n",
    "    df['Metric_Tons_Value'] = df['Metric_Tons_HP'].apply(lambda x: extract_numeric_range(x) if isinstance(x, str) else np.nan)\n",
    "    \n",
    "    # Step 4: Impute missing ProductSize values using Metric Tons\n",
    "    def impute_product_size(row):\n",
    "        if pd.isna(row['ProductSize']) and not pd.isna(row['Metric_Tons_Value']):\n",
    "            if row['Metric_Tons_Value'] <= 5:\n",
    "                return 'Mini'\n",
    "            elif 5 < row['Metric_Tons_Value'] <= 75:\n",
    "                return 'Compact'\n",
    "            elif 20 < row['Metric_Tons_Value'] <= 50:\n",
    "                return 'Large / Medium'\n",
    "            elif 75 < row['Metric_Tons_Value'] <= 200:\n",
    "                return 'Medium'\n",
    "            elif row['Metric_Tons_Value'] > 200:\n",
    "                return 'Large'\n",
    "        return row['ProductSize']\n",
    "    \n",
    "    df['ProductSize_Imputed'] = df.apply(impute_product_size, axis=1)\n",
    "    df.drop(columns='ProductSize',inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "107909de-742b-444a-b788-a0e2ddfd38f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eitanb\\AppData\\Local\\Temp\\ipykernel_20204\\3281401804.py:9: DtypeWarning: Columns (13,39,40,41) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update_YearMade START\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eitanb\\AppData\\Local\\Temp\\ipykernel_20204\\3281401804.py:26: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[1980. 1980. 1994. ... 2003. 2004. 2004.]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[df['YearMade'] == 1000, 'YearMade'] = df['ModelID'].map(model_medians)\n",
      "C:\\Users\\eitanb\\AppData\\Local\\Temp\\ipykernel_20204\\3281401804.py:32: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['YearMade'].fillna(overall_median, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update_YearMade END\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4688.707301831825, 7572.598774518791)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-import necessary libraries due to execution reset\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "\n",
    "# Reload the dataset\n",
    "ProductSize_path=r'C:\\Users\\eitanb\\Documents\\DS\\ML\\ML_project\\DATA/ProductSize_Null_fix_model.pkl'\n",
    "file_path_train = r\"C:\\Users\\eitanb\\Documents\\DS\\ML\\ML_project\\DATA/Train.csv\"\n",
    "df = pd.read_csv(file_path_train)\n",
    "\n",
    "# Convert 'saledate' to datetime and extract year, month, and day\n",
    "df['saledate'] = pd.to_datetime(df['saledate'], errors='coerce')\n",
    "df['sale_year'] = df['saledate'].dt.year\n",
    "df['sale_month'] = df['saledate'].dt.month\n",
    "df['sale_day'] = df['saledate'].dt.day\n",
    "def preprocess_product_size(df):\n",
    "    \"\"\"\n",
    "    Function to preprocess and impute missing ProductSize values based on fiProductClassDesc.\n",
    "    \n",
    "    Steps:\n",
    "    1. Extract Product Type and Metric Tons / Horsepower from fiProductClassDesc.\n",
    "    2. Identify Unit Type (Metric Tons, Horsepower, or Lb Operating Capacity).\n",
    "    3. Convert Metric Tons / Horsepower to numerical values.\n",
    "    4. Use Metric Tons to impute missing ProductSize values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Extract Product Type and Size Descriptor\n",
    "    def split_product_desc(desc):\n",
    "        if pd.isna(desc):\n",
    "            return pd.NA, pd.NA\n",
    "        match = re.search(r'(.+?)-\\s*([\\d\\.]+.*)', desc)\n",
    "        if match:\n",
    "            return match.group(1).strip(), match.group(2).strip()\n",
    "        return desc.strip(), pd.NA  # If no match, return full description as product\n",
    "    \n",
    "    df[['Product_Type', 'Metric_Tons_HP']] = df['fiProductClassDesc'].apply(lambda x: pd.Series(split_product_desc(x)))\n",
    "    \n",
    "    # Step 2: Identify Unit Type\n",
    "    def extract_unit_type(desc):\n",
    "        if pd.isna(desc):\n",
    "            return pd.NA\n",
    "        if \"Metric Tons\" in desc:\n",
    "            return \"Metric Tons\"\n",
    "        elif \"Horsepower\" in desc:\n",
    "            return \"Horsepower\"\n",
    "        elif \"Lb Operating Capacity\" in desc:\n",
    "            return \"Lb Operating Capacity\"\n",
    "        return pd.NA\n",
    "    \n",
    "    df[\"Unit_Type\"] = df[\"Metric_Tons_HP\"].apply(extract_unit_type)\n",
    "    \n",
    "    # Step 3: Convert Metric Tons to numerical values\n",
    "    def extract_numeric_range(value):\n",
    "        \"\"\"Extracts the average value from a range like '12.0 to 14.0 Metric Tons'.\"\"\"\n",
    "        if pd.isna(value) or not isinstance(value, str):\n",
    "            return np.nan\n",
    "        numbers = re.findall(r'[\\d\\.]+', value)\n",
    "        if len(numbers) == 2:  # If range exists, take the average\n",
    "            return (float(numbers[0]) + float(numbers[1])) / 2\n",
    "        elif len(numbers) == 1:  # If only one number exists, use it\n",
    "            return float(numbers[0])\n",
    "        return np.nan\n",
    "    \n",
    "    df['Metric_Tons_Value'] = df['Metric_Tons_HP'].apply(lambda x: extract_numeric_range(x) if isinstance(x, str) else np.nan)\n",
    "    \n",
    "    # Step 4: Impute missing ProductSize values using Metric Tons\n",
    "    def impute_product_size(row):\n",
    "        if pd.isna(row['ProductSize']) and not pd.isna(row['Metric_Tons_Value']):\n",
    "            if row['Metric_Tons_Value'] <= 5:\n",
    "                return 'Mini'\n",
    "            elif 5 < row['Metric_Tons_Value'] <= 75:\n",
    "                return 'Compact'\n",
    "            elif 20 < row['Metric_Tons_Value'] <= 50:\n",
    "                return 'Large / Medium'\n",
    "            elif 75 < row['Metric_Tons_Value'] <= 200:\n",
    "                return 'Medium'\n",
    "            elif row['Metric_Tons_Value'] > 200:\n",
    "                return 'Large'\n",
    "        return row['ProductSize']\n",
    "    \n",
    "    df['ProductSize_Imputed'] = df.apply(impute_product_size, axis=1)\n",
    "    df.drop(columns='ProductSize',inplace=True)\n",
    "    \n",
    "    return df\n",
    "df=preprocess_product_size(df)\n",
    "# Define function to update 'YearMade' and create 'YearMade_Bucket'\n",
    "def update_YearMade(df):\n",
    "    print(\"update_YearMade START\")\n",
    "    \n",
    "    # Compute median YearMade for each ModelID\n",
    "    model_medians = df.loc[df['YearMade'] > 1000].groupby('ModelID')['YearMade'].median()\n",
    "    \n",
    "    # Update YearMade where it is 1000 using ModelID median\n",
    "    df.loc[df['YearMade'] == 1000, 'YearMade'] = df['ModelID'].map(model_medians)\n",
    "    \n",
    "    # Compute overall median YearMade excluding 1000 values\n",
    "    overall_median = df.loc[df['YearMade'] > 1000, 'YearMade'].median()\n",
    "    \n",
    "    # Replace any remaining 1000 values with overall median\n",
    "    df['YearMade'].fillna(overall_median, inplace=True)\n",
    "        \n",
    "    # Bucketize YearMade\n",
    "    bins = [0, 1980, 1995, 2005, 2010, 2025]\n",
    "    labels = [\"Before 1980\", \"1980-1995\", \"1996-2005\", \"2006-2010\", \"2011-2025\"]\n",
    "    df['YearMade_Bucket'] = pd.cut(df['YearMade'], bins=bins, labels=labels, right=True)\n",
    "    \n",
    "    print(\"update_YearMade END\")\n",
    "    return df\n",
    "\n",
    "# Apply YearMade update\n",
    "df = update_YearMade(df)\n",
    "\n",
    "# Filter dataset to only include the last 5 years\n",
    "recent_years = df['sale_year'].dropna().unique()\n",
    "recent_years.sort()\n",
    "selected_years = recent_years[-5:]\n",
    "df_filtered = df[df['sale_year'].isin(selected_years)]\n",
    "\n",
    "# Drop irrelevant columns: 'SalesID', 'saledate'\n",
    "high_cardinality_cols = [col for col in df_filtered.select_dtypes(include=['object']).columns if df_filtered[col].nunique() > 50]\n",
    "df_filtered = df_filtered.drop(columns=['SalesID', 'saledate'] + high_cardinality_cols, errors='ignore')\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = df_filtered.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Apply Ordinal Encoding to 'YearMade_Bucket'\n",
    "year_bucket_encoder = OrdinalEncoder(categories=[[\"Before 1980\", \"1980-1995\", \"1996-2005\", \"2006-2010\", \"2011-2025\"]])\n",
    "df_filtered[\"YearMade_Bucket\"] = year_bucket_encoder.fit_transform(df_filtered[[\"YearMade_Bucket\"]])\n",
    "\n",
    "# Identify high, moderate, and low cardinality categorical columns\n",
    "high_cardinality_cols = [col for col in categorical_cols if df_filtered[col].nunique() >= 50]\n",
    "moderate_cardinality_cols = [col for col in categorical_cols if 10 <= df_filtered[col].nunique() < 50]\n",
    "low_cardinality_cols = [col for col in categorical_cols if df_filtered[col].nunique() < 10]\n",
    "\n",
    "# Fill missing categorical values with \"Unknown\"\n",
    "for col in categorical_cols:\n",
    "    df_filtered[col] = df_filtered[col].fillna(\"Unknown\")\n",
    "\n",
    "# Encoding strategy\n",
    "\n",
    "# Apply One-Hot Encoding (OHE) to low-cardinality categories\n",
    "df_encoded = pd.get_dummies(df_filtered, columns=low_cardinality_cols, drop_first=True)\n",
    "\n",
    "# Apply Ordinal Encoding to moderate-cardinality categories\n",
    "if moderate_cardinality_cols:\n",
    "    ord_encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "    df_encoded[moderate_cardinality_cols] = ord_encoder.fit_transform(df_encoded[moderate_cardinality_cols])\n",
    "\n",
    "# Apply Frequency Encoding to high-cardinality categories\n",
    "for col in high_cardinality_cols:\n",
    "    freq_encoding = df_encoded[col].value_counts().to_dict()\n",
    "    df_encoded[col] = df_encoded[col].map(freq_encoding)\n",
    "\n",
    "# Drop rows with missing SalePrice (target variable)\n",
    "df_encoded = df_encoded.dropna(subset=['SalePrice'])\n",
    "\n",
    "# Split into features and target\n",
    "X = df_encoded.drop(columns=['SalePrice'])\n",
    "y = df_encoded['SalePrice']\n",
    "\n",
    "# Split data into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "y_pred = rf_model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mse ** 0.5\n",
    "\n",
    "# Display model performance\n",
    "mae, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9f602268-71b0-4aaa-9509-746eb84525b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eitanb\\AppData\\Local\\Temp\\ipykernel_20204\\2475275052.py:3: DtypeWarning: Columns (13,39,40,41) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path_train)\n"
     ]
    }
   ],
   "source": [
    "ProductSize_path=r'C:\\Users\\eitanb\\Documents\\DS\\ML\\ML_project\\DATA/ProductSize_Null_fix_model.pkl'\n",
    "file_path_train = r\"C:\\Users\\eitanb\\Documents\\DS\\ML\\ML_project\\DATA/Train.csv\"\n",
    "df = pd.read_csv(file_path_train)\n",
    "\n",
    "def extract_horsepower(df):\n",
    "    \"\"\"\n",
    "    Extracts and imputes missing Horsepower values from fiProductClassDesc.\n",
    "    Optimized for performance using vectorized operations.\n",
    "    \"\"\"\n",
    "    def extract_numeric_range(value):\n",
    "        \"\"\"Extracts the average horsepower value from a range like '100 to 120 Horsepower'.\"\"\"\n",
    "        if pd.isna(value) or not isinstance(value, str):\n",
    "            return np.nan\n",
    "        numbers = re.findall(r'[\\d\\.]+', value)\n",
    "        if len(numbers) == 2:\n",
    "            return (float(numbers[0]) + float(numbers[1])) / 2\n",
    "        elif len(numbers) == 1:\n",
    "            return float(numbers[0])\n",
    "        return np.nan\n",
    "    \n",
    "    # Identify rows with Horsepower in fiProductClassDesc\n",
    "    mask_horsepower = df['fiProductClassDesc'].str.contains(\"Horsepower\", na=False)\n",
    "    \n",
    "    # Extract horsepower values\n",
    "    df.loc[mask_horsepower, 'Horsepower_Unit_Type'] = 'Horsepower'\n",
    "    df.loc[mask_horsepower, 'Extracted_Horsepower'] = df.loc[mask_horsepower, 'fiProductClassDesc'].apply(extract_numeric_range)\n",
    "    \n",
    "    # Handle 'Variable' and 'No' values in Engine_Horsepower\n",
    "    df.loc[df['Engine_Horsepower'].isin(['Variable', 'No']), 'Engine_Horsepower'] = np.nan\n",
    "    \n",
    "    # Impute missing Engine_Horsepower values where Extracted_Horsepower is available\n",
    "    df['Engine_Horsepower_Imputed'] = df['Engine_Horsepower'].combine_first(df['Extracted_Horsepower'])\n",
    "    \n",
    "    # Fill remaining NaN values with median horsepower\n",
    "    #df['Engine_Horsepower_Imputed'].fillna(df['Engine_Horsepower_Imputed'].median(), inplace=True)\n",
    "    df.drop(columns='Engine_Horsepower',inplace=True)\n",
    "    \n",
    "    return df\n",
    "def preprocess_product_size(df):\n",
    "    \"\"\"\n",
    "    Function to preprocess and impute missing ProductSize values based on fiProductClassDesc.\n",
    "    \n",
    "    Steps:\n",
    "    1. Extract Product Type and Metric Tons / Horsepower from fiProductClassDesc.\n",
    "    2. Identify Unit Type (Metric Tons, Horsepower, or Lb Operating Capacity).\n",
    "    3. Convert Metric Tons / Horsepower to numerical values.\n",
    "    4. Use Metric Tons to impute missing ProductSize values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Extract Product Type and Size Descriptor\n",
    "    def split_product_desc(desc):\n",
    "        if pd.isna(desc):\n",
    "            return pd.NA, pd.NA\n",
    "        match = re.search(r'(.+?)-\\s*([\\d\\.]+.*)', desc)\n",
    "        if match:\n",
    "            return match.group(1).strip(), match.group(2).strip()\n",
    "        return desc.strip(), pd.NA  # If no match, return full description as product\n",
    "    \n",
    "    df[['Product_Type', 'Metric_Tons_HP']] = df['fiProductClassDesc'].apply(lambda x: pd.Series(split_product_desc(x)))\n",
    "    \n",
    "    # Step 2: Identify Unit Type\n",
    "    def extract_unit_type(desc):\n",
    "        if pd.isna(desc):\n",
    "            return pd.NA\n",
    "        if \"Metric Tons\" in desc:\n",
    "            return \"Metric Tons\"\n",
    "        elif \"Horsepower\" in desc:\n",
    "            return \"Horsepower\"\n",
    "        elif \"Lb Operating Capacity\" in desc:\n",
    "            return \"Lb Operating Capacity\"\n",
    "        return pd.NA\n",
    "    \n",
    "    df[\"Unit_Type\"] = df[\"Metric_Tons_HP\"].apply(extract_unit_type)\n",
    "    \n",
    "    # Step 3: Convert Metric Tons to numerical values\n",
    "    def extract_numeric_range(value):\n",
    "        \"\"\"Extracts the average value from a range like '12.0 to 14.0 Metric Tons'.\"\"\"\n",
    "        if pd.isna(value) or not isinstance(value, str):\n",
    "            return np.nan\n",
    "        numbers = re.findall(r'[\\d\\.]+', value)\n",
    "        if len(numbers) == 2:  # If range exists, take the average\n",
    "            return (float(numbers[0]) + float(numbers[1])) / 2\n",
    "        elif len(numbers) == 1:  # If only one number exists, use it\n",
    "            return float(numbers[0])\n",
    "        return np.nan\n",
    "    \n",
    "    df['Metric_Tons_Value'] = df['Metric_Tons_HP'].apply(lambda x: extract_numeric_range(x) if isinstance(x, str) else np.nan)\n",
    "    \n",
    "    # Step 4: Impute missing ProductSize values using Metric Tons\n",
    "    def impute_product_size(row):\n",
    "        if pd.isna(row['ProductSize']) and not pd.isna(row['Metric_Tons_Value']) and (row['Metric_Tons_Value']=='Metric Tons'):\n",
    "            if row['Metric_Tons_Value']  <= 5:\n",
    "                return 'Mini'\n",
    "            elif 5 < row['Metric_Tons_Value'] <= 75:\n",
    "                return 'Compact'\n",
    "            elif 20 < row['Metric_Tons_Value'] <= 50:\n",
    "                return 'Large / Medium'\n",
    "            elif 75 < row['Metric_Tons_Value'] <= 200:\n",
    "                return 'Medium'\n",
    "            elif row['Metric_Tons_Value'] > 200:\n",
    "                return 'Large'\n",
    "        return row['ProductSize']\n",
    "    \n",
    "    df['ProductSize_Imputed'] = df.apply(impute_product_size, axis=1)\n",
    "    df.drop(columns='ProductSize',inplace=True)\n",
    "    \n",
    "    return df\n",
    "df1=preprocess_product_size(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9da9fed-8baf-4a2b-823a-a29465a113d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eitanb\\AppData\\Local\\Temp\\ipykernel_18780\\4042277343.py:113: DtypeWarning: Columns (13,39,40,41) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path_train)\n",
      "C:\\Users\\eitanb\\AppData\\Local\\Temp\\ipykernel_18780\\4042277343.py:32: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  df['Engine_Horsepower_Imputed'] = df['Engine_Horsepower'].combine_first(df['Extracted_Horsepower'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update_YearMade START\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eitanb\\AppData\\Local\\Temp\\ipykernel_18780\\4042277343.py:135: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[1980. 1980. 1994. ... 2003. 2004. 2004.]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[df['YearMade'] == 1000, 'YearMade'] = df['ModelID'].map(model_medians)\n",
      "C:\\Users\\eitanb\\AppData\\Local\\Temp\\ipykernel_18780\\4042277343.py:141: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['YearMade'].fillna(overall_median, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update_YearMade END\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4690.262789670813, 7587.100811299758)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-import necessary libraries due to execution reset\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "def extract_horsepower(df):\n",
    "    \"\"\"\n",
    "    Extracts and imputes missing Horsepower values from fiProductClassDesc.\n",
    "    Optimized for performance using vectorized operations.\n",
    "    \"\"\"\n",
    "    def extract_numeric_range(value):\n",
    "        \"\"\"Extracts the average horsepower value from a range like '100 to 120 Horsepower'.\"\"\"\n",
    "        if pd.isna(value) or not isinstance(value, str):\n",
    "            return np.nan\n",
    "        numbers = re.findall(r'[\\d\\.]+', value)\n",
    "        if len(numbers) == 2:\n",
    "            return (float(numbers[0]) + float(numbers[1])) / 2\n",
    "        elif len(numbers) == 1:\n",
    "            return float(numbers[0])\n",
    "        return np.nan\n",
    "    \n",
    "    # Identify rows with Horsepower in fiProductClassDesc\n",
    "    mask_horsepower = df['fiProductClassDesc'].str.contains(\"Horsepower\", na=False)\n",
    "    \n",
    "    # Extract horsepower values\n",
    "    df.loc[mask_horsepower, 'Horsepower_Unit_Type'] = 'Horsepower'\n",
    "    df.loc[mask_horsepower, 'Extracted_Horsepower'] = df.loc[mask_horsepower, 'fiProductClassDesc'].apply(extract_numeric_range)\n",
    "    \n",
    "    # Handle 'Variable' and 'No' values in Engine_Horsepower\n",
    "    df.loc[df['Engine_Horsepower'].isin(['Variable', 'No']), 'Engine_Horsepower'] = np.nan\n",
    "    \n",
    "    # Impute missing Engine_Horsepower values where Extracted_Horsepower is available\n",
    "    df['Engine_Horsepower_Imputed'] = df['Engine_Horsepower'].combine_first(df['Extracted_Horsepower'])\n",
    "    \n",
    "    # Fill remaining NaN values with median horsepower\n",
    "    #df['Engine_Horsepower_Imputed'].fillna(df['Engine_Horsepower_Imputed'].median(), inplace=True)\n",
    "    df.drop(columns='Engine_Horsepower',inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def preprocess_product_size(df):\n",
    "    \"\"\"\n",
    "    Function to preprocess and impute missing ProductSize values based on fiProductClassDesc.\n",
    "    \n",
    "    Steps:\n",
    "    1. Extract Product Type and Metric Tons / Horsepower from fiProductClassDesc.\n",
    "    2. Identify Unit Type (Metric Tons, Horsepower, or Lb Operating Capacity).\n",
    "    3. Convert Metric Tons / Horsepower to numerical values.\n",
    "    4. Use Metric Tons to impute missing ProductSize values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Extract Product Type and Size Descriptor\n",
    "    def split_product_desc(desc):\n",
    "        if pd.isna(desc):\n",
    "            return pd.NA, pd.NA\n",
    "        match = re.search(r'(.+?)-\\s*([\\d\\.]+.*)', desc)\n",
    "        if match:\n",
    "            return match.group(1).strip(), match.group(2).strip()\n",
    "        return desc.strip(), pd.NA  # If no match, return full description as product\n",
    "    \n",
    "    df[['Product_Type', 'Metric_Tons_HP']] = df['fiProductClassDesc'].apply(lambda x: pd.Series(split_product_desc(x)))\n",
    "    \n",
    "    # Step 2: Identify Unit Type\n",
    "    def extract_unit_type(desc):\n",
    "        if pd.isna(desc):\n",
    "            return pd.NA\n",
    "        if \"Metric Tons\" in desc:\n",
    "            return \"Metric Tons\"\n",
    "        elif \"Horsepower\" in desc:\n",
    "            return \"Horsepower\"\n",
    "        elif \"Lb Operating Capacity\" in desc:\n",
    "            return \"Lb Operating Capacity\"\n",
    "        return pd.NA\n",
    "    \n",
    "    df[\"Unit_Type\"] = df[\"Metric_Tons_HP\"].apply(extract_unit_type)\n",
    "    \n",
    "    # Step 3: Convert Metric Tons to numerical values\n",
    "    def extract_numeric_range(value):\n",
    "        \"\"\"Extracts the average value from a range like '12.0 to 14.0 Metric Tons'.\"\"\"\n",
    "        if pd.isna(value) or not isinstance(value, str):\n",
    "            return np.nan\n",
    "        numbers = re.findall(r'[\\d\\.]+', value)\n",
    "        if len(numbers) == 2:  # If range exists, take the average\n",
    "            return (float(numbers[0]) + float(numbers[1])) / 2\n",
    "        elif len(numbers) == 1:  # If only one number exists, use it\n",
    "            return float(numbers[0])\n",
    "        return np.nan\n",
    "    \n",
    "    df['Metric_Tons_Value'] = df['Metric_Tons_HP'].apply(lambda x: extract_numeric_range(x) if isinstance(x, str) else np.nan)\n",
    "    \n",
    "    # Step 4: Impute missing ProductSize values using Metric Tons\n",
    "    def impute_product_size(row):\n",
    "        if pd.isna(row['ProductSize']) and not pd.isna(row['Metric_Tons_Value']) and (row['Metric_Tons_Value']=='Metric Tons'):\n",
    "            if row['Metric_Tons_Value']  <= 5:\n",
    "                return 'Mini'\n",
    "            elif 5 < row['Metric_Tons_Value'] <= 75:\n",
    "                return 'Compact'\n",
    "            elif 20 < row['Metric_Tons_Value'] <= 50:\n",
    "                return 'Large / Medium'\n",
    "            elif 75 < row['Metric_Tons_Value'] <= 200:\n",
    "                return 'Medium'\n",
    "            elif row['Metric_Tons_Value'] > 200:\n",
    "                return 'Large'\n",
    "        return row['ProductSize']\n",
    "    \n",
    "    df['ProductSize_Imputed'] = df.apply(impute_product_size, axis=1)\n",
    "    df.drop(columns='ProductSize',inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Reload the dataset\n",
    "ProductSize_path=r'C:\\Users\\eitanb\\Documents\\DS\\ML\\ML_project\\DATA/ProductSize_Null_fix_model.pkl'\n",
    "file_path_train = r\"C:\\Users\\eitanb\\Documents\\DS\\ML\\ML_project\\DATA/Train.csv\"\n",
    "df = pd.read_csv(file_path_train)\n",
    "#df=df[['SalesID','saledate',\"Enclosure\",\"YearMade\",\"ModelID\",\"MachineID\",\"Ripper\",\"ProductGroup\",\"ProductSize\",\"ProductGroupDesc\",\"Tire_Size\",\"MachineHoursCurrentMeter\",\"Blade_Type\",\"ProductGroupDesc\",\"Travel_Controls\",\"auctioneerID\",\"datasource\",\"Drive_System\",\"Pushblock\",\"Tire_Size\",\"Ride_Control\",\"UsageBand\",\"Coupler\",\"Engine_Horsepower_Imputed\"]].copy()\n",
    "\n",
    "# Convert 'saledate' to datetime and extract year, month, and day\n",
    "df['saledate'] = pd.to_datetime(df['saledate'], errors='coerce')\n",
    "df['sale_year'] = df['saledate'].dt.year\n",
    "df['sale_month'] = df['saledate'].dt.month\n",
    "df['sale_day'] = df['saledate'].dt.day\n",
    "\n",
    "df=extract_horsepower(df)\n",
    "\n",
    "\n",
    "df=preprocess_product_size(df)\n",
    "\n",
    "# Define function to update 'YearMade' and create 'YearMade_Bucket'\n",
    "def update_YearMade(df):\n",
    "    print(\"update_YearMade START\")\n",
    "    \n",
    "    # Compute median YearMade for each ModelID\n",
    "    model_medians = df.loc[df['YearMade'] > 1000].groupby('ModelID')['YearMade'].median()\n",
    "    \n",
    "    # Update YearMade where it is 1000 using ModelID median\n",
    "    df.loc[df['YearMade'] == 1000, 'YearMade'] = df['ModelID'].map(model_medians)\n",
    "    \n",
    "    # Compute overall median YearMade excluding 1000 values\n",
    "    overall_median = df.loc[df['YearMade'] > 1000, 'YearMade'].median()\n",
    "    \n",
    "    # Replace any remaining 1000 values with overall median\n",
    "    df['YearMade'].fillna(overall_median, inplace=True)\n",
    "        \n",
    "    # Bucketize YearMade\n",
    "    bins = [0, 1980, 1995, 2005, 2010, 2025]\n",
    "    labels = [\"Before 1980\", \"1980-1995\", \"1996-2005\", \"2006-2010\", \"2011-2025\"]\n",
    "    df['YearMade_Bucket'] = pd.cut(df['YearMade'], bins=bins, labels=labels, right=True)\n",
    "    \n",
    "    print(\"update_YearMade END\")\n",
    "    return df\n",
    "\n",
    "# Apply YearMade update\n",
    "df = update_YearMade(df)\n",
    "\n",
    "# Filter dataset to only include the last 5 years\n",
    "recent_years = df['sale_year'].dropna().unique()\n",
    "recent_years.sort()\n",
    "#selected_years = recent_years[-5:]\n",
    "#df_filtered = df[df['sale_year'].isin(selected_years)]\n",
    "\n",
    "# Drop irrelevant columns: 'SalesID', 'saledate'\n",
    "high_cardinality_cols = [col for col in df_filtered.select_dtypes(include=['object']).columns if df_filtered[col].nunique() > 50]\n",
    "df_filtered = df_filtered.drop(columns=['SalesID', 'saledate'] + high_cardinality_cols, errors='ignore')\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = df_filtered.select_dtypes(include=['object']).columns\n",
    "\n",
    "df_filtered[\"YearMade_Bucket\"] = df_filtered[\"YearMade_Bucket\"].astype(str).fillna(\"Unknown\")\n",
    "\n",
    "# Define category order and apply Ordinal Encoding\n",
    "year_bucket_encoder = OrdinalEncoder(\n",
    "    categories=[[\"Before 1980\", \"1980-1995\", \"1996-2005\", \"2006-2010\", \"2011-2025\", \"Unknown\"]],\n",
    "    handle_unknown=\"use_encoded_value\",\n",
    "    unknown_value=-1\n",
    ")\n",
    "\n",
    "df_filtered[\"YearMade_Bucket\"] = year_bucket_encoder.fit_transform(df_filtered[[\"YearMade_Bucket\"]])\n",
    "# Apply Ordinal Encoding to 'YearMade_Bucket'\n",
    "#year_bucket_encoder = OrdinalEncoder(categories=[[\"Before 1980\", \"1980-1995\", \"1996-2005\", \"2006-2010\", \"2011-2025\"]])\n",
    "#df_filtered[\"YearMade_Bucket\"] = year_bucket_encoder.fit_transform(df_filtered[[\"YearMade_Bucket\"]])\n",
    "\n",
    "# Identify high, moderate, and low cardinality categorical columns\n",
    "high_cardinality_cols = [col for col in categorical_cols if df_filtered[col].nunique() >= 50]\n",
    "moderate_cardinality_cols = [col for col in categorical_cols if 10 <= df_filtered[col].nunique() < 50]\n",
    "low_cardinality_cols = [col for col in categorical_cols if df_filtered[col].nunique() < 10]\n",
    "\n",
    "# Fill missing categorical values with \"Unknown\"\n",
    "for col in categorical_cols:\n",
    "    df_filtered[col] = df_filtered[col].fillna(\"Unknown\")\n",
    "\n",
    "# Encoding strategy\n",
    "\n",
    "# Apply One-Hot Encoding (OHE) to low-cardinality categories\n",
    "df_encoded = pd.get_dummies(df_filtered, columns=low_cardinality_cols, drop_first=True)\n",
    "\n",
    "# Apply Ordinal Encoding to moderate-cardinality categories\n",
    "if moderate_cardinality_cols:\n",
    "    ord_encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "    df_encoded[moderate_cardinality_cols] = ord_encoder.fit_transform(df_encoded[moderate_cardinality_cols])\n",
    "\n",
    "# Apply Frequency Encoding to high-cardinality categories\n",
    "for col in high_cardinality_cols:\n",
    "    freq_encoding = df_encoded[col].value_counts().to_dict()\n",
    "    df_encoded[col] = df_encoded[col].map(freq_encoding)\n",
    "\n",
    "# Drop rows with missing SalePrice (target variable)\n",
    "df_encoded = df_encoded.dropna(subset=['SalePrice'])\n",
    "\n",
    "# Split into features and target\n",
    "X = df_encoded.drop(columns=['SalePrice'])\n",
    "y = df_encoded['SalePrice']\n",
    "\n",
    "# Split data into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "y_pred = rf_model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mse ** 0.5\n",
    "\n",
    "# Display model performance\n",
    "mae, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e167e01-583c-42a0-b65e-b933c97cd8dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eitanb\\AppData\\Local\\Temp\\ipykernel_12692\\1994876309.py:123: DtypeWarning: Columns (13,39,40,41) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path_train)\n",
      "C:\\Users\\eitanb\\AppData\\Local\\Temp\\ipykernel_12692\\1994876309.py:32: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  df['Engine_Horsepower_Imputed'] = df['Engine_Horsepower'].combine_first(df['Extracted_Horsepower'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update_YearMade START\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eitanb\\AppData\\Local\\Temp\\ipykernel_12692\\1994876309.py:145: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[1980. 1980. 1994. ... 2003. 2004. 2004.]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[df['YearMade'] == 1000, 'YearMade'] = df['ModelID'].map(model_medians)\n",
      "C:\\Users\\eitanb\\AppData\\Local\\Temp\\ipykernel_12692\\1994876309.py:151: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['YearMade'].fillna(overall_median, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update_YearMade END\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4833.026361105839, 7785.955971072109)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-import necessary libraries due to execution reset\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "def extract_horsepower(df):\n",
    "    \"\"\"\n",
    "    Extracts and imputes missing Horsepower values from fiProductClassDesc.\n",
    "    Optimized for performance using vectorized operations.\n",
    "    \"\"\"\n",
    "    def extract_numeric_range(value):\n",
    "        \"\"\"Extracts the average horsepower value from a range like '100 to 120 Horsepower'.\"\"\"\n",
    "        if pd.isna(value) or not isinstance(value, str):\n",
    "            return np.nan\n",
    "        numbers = re.findall(r'[\\d\\.]+', value)\n",
    "        if len(numbers) == 2:\n",
    "            return (float(numbers[0]) + float(numbers[1])) / 2\n",
    "        elif len(numbers) == 1:\n",
    "            return float(numbers[0])\n",
    "        return np.nan\n",
    "    \n",
    "    # Identify rows with Horsepower in fiProductClassDesc\n",
    "    mask_horsepower = df['fiProductClassDesc'].str.contains(\"Horsepower\", na=False)\n",
    "    \n",
    "    # Extract horsepower values\n",
    "    df.loc[mask_horsepower, 'Horsepower_Unit_Type'] = 'Horsepower'\n",
    "    df.loc[mask_horsepower, 'Extracted_Horsepower'] = df.loc[mask_horsepower, 'fiProductClassDesc'].apply(extract_numeric_range)\n",
    "    \n",
    "    # Handle 'Variable' and 'No' values in Engine_Horsepower\n",
    "    df.loc[df['Engine_Horsepower'].isin(['Variable', 'No']), 'Engine_Horsepower'] = np.nan\n",
    "    \n",
    "    # Impute missing Engine_Horsepower values where Extracted_Horsepower is available\n",
    "    df['Engine_Horsepower_Imputed'] = df['Engine_Horsepower'].combine_first(df['Extracted_Horsepower'])\n",
    "    \n",
    "    # Fill remaining NaN values with median horsepower\n",
    "    #df['Engine_Horsepower_Imputed'].fillna(df['Engine_Horsepower_Imputed'].median(), inplace=True)\n",
    "    df.drop(columns='Engine_Horsepower',inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def preprocess_product_size(df):\n",
    "    \"\"\"\n",
    "    Function to preprocess and impute missing ProductSize values based on fiProductClassDesc.\n",
    "    \n",
    "    Steps:\n",
    "    1. Extract Product Type and Metric Tons / Horsepower from fiProductClassDesc.\n",
    "    2. Identify Unit Type (Metric Tons, Horsepower, or Lb Operating Capacity).\n",
    "    3. Convert Metric Tons / Horsepower to numerical values.\n",
    "    4. Use Metric Tons to impute missing ProductSize values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Extract Product Type and Size Descriptor\n",
    "    def split_product_desc(desc):\n",
    "        if pd.isna(desc):\n",
    "            return pd.NA, pd.NA\n",
    "        match = re.search(r'(.+?)-\\s*([\\d\\.]+.*)', desc)\n",
    "        if match:\n",
    "            return match.group(1).strip(), match.group(2).strip()\n",
    "        return desc.strip(), pd.NA  # If no match, return full description as product\n",
    "    \n",
    "    df[['Product_Type', 'Metric_Tons_HP']] = df['fiProductClassDesc'].apply(lambda x: pd.Series(split_product_desc(x)))\n",
    "    \n",
    "    # Step 2: Identify Unit Type\n",
    "    def extract_unit_type(desc):\n",
    "        if pd.isna(desc):\n",
    "            return pd.NA\n",
    "        if \"Metric Tons\" in desc:\n",
    "            return \"Metric Tons\"\n",
    "        elif \"Horsepower\" in desc:\n",
    "            return \"Horsepower\"\n",
    "        elif \"Lb Operating Capacity\" in desc:\n",
    "            return \"Lb Operating Capacity\"\n",
    "        return pd.NA\n",
    "    \n",
    "    df[\"Unit_Type\"] = df[\"Metric_Tons_HP\"].apply(extract_unit_type)\n",
    "    \n",
    "    # Step 3: Convert Metric Tons to numerical values\n",
    "    def extract_numeric_range(value):\n",
    "        \"\"\"Extracts the average value from a range like '12.0 to 14.0 Metric Tons'.\"\"\"\n",
    "        if pd.isna(value) or not isinstance(value, str):\n",
    "            return np.nan\n",
    "        numbers = re.findall(r'[\\d\\.]+', value)\n",
    "        if len(numbers) == 2:  # If range exists, take the average\n",
    "            return (float(numbers[0]) + float(numbers[1])) / 2\n",
    "        elif len(numbers) == 1:  # If only one number exists, use it\n",
    "            return float(numbers[0])\n",
    "        return np.nan\n",
    "    \n",
    "    df['Metric_Tons_Value'] = df['Metric_Tons_HP'].apply(lambda x: extract_numeric_range(x) if isinstance(x, str) else np.nan)\n",
    "    \n",
    "    # Step 4: Impute missing ProductSize values using Metric Tons\n",
    "    def impute_product_size(row):\n",
    "        if pd.isna(row['ProductSize']) and not pd.isna(row['Metric_Tons_Value']) and (row['Metric_Tons_Value']=='Metric Tons'):\n",
    "            if row['Metric_Tons_Value']  <= 5:\n",
    "                return 'Mini'\n",
    "            elif 5 < row['Metric_Tons_Value'] <= 75:\n",
    "                return 'Compact'\n",
    "            elif 20 < row['Metric_Tons_Value'] <= 50:\n",
    "                return 'Large / Medium'\n",
    "            elif 75 < row['Metric_Tons_Value'] <= 200:\n",
    "                return 'Medium'\n",
    "            elif row['Metric_Tons_Value'] > 200:\n",
    "                return 'Large'\n",
    "        return row['ProductSize']\n",
    "    \n",
    "    df['ProductSize_Imputed'] = df.apply(impute_product_size, axis=1)\n",
    "    df.drop(columns=['ProductSize','Metric_Tons_Value','Metric_Tons_HP','Unit_Type'],inplace=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    return df\n",
    "def Enclosure_fun(df):\n",
    "    list1=['EROPS','OROPS','EROPS AC']\n",
    "    list2=['NO ROPS','None or Unspecified']\n",
    "    df['Enclosure_cat']=0\n",
    "    df.loc[df['Enclosure'].isin(list1), 'Enclosure_cat'] = 1\n",
    "    df.loc[df['Enclosure'].isin(list2), 'Enclosure_cat'] = 0\n",
    "    df.loc[df['Enclosure']=='EROPS w AC','Enclosure_cat']=2\n",
    "    df.drop(columns='Enclosure',inplace=True)\n",
    "    return df\n",
    "# Reload the dataset\n",
    "ProductSize_path=r'C:\\Users\\eitanb\\Documents\\DS\\ML\\ML_project\\DATA/ProductSize_Null_fix_model.pkl'\n",
    "file_path_train = r\"C:\\Users\\eitanb\\Documents\\DS\\ML\\ML_project\\DATA/Train.csv\"\n",
    "df = pd.read_csv(file_path_train)\n",
    "#df=df[['SalesID','saledate',\"Enclosure\",\"YearMade\",\"ModelID\",\"MachineID\",\"Ripper\",\"ProductGroup\",\"ProductSize\",\"ProductGroupDesc\",\"Tire_Size\",\"MachineHoursCurrentMeter\",\"Blade_Type\",\"ProductGroupDesc\",\"Travel_Controls\",\"auctioneerID\",\"datasource\",\"Drive_System\",\"Pushblock\",\"Tire_Size\",\"Ride_Control\",\"UsageBand\",\"Coupler\",\"Engine_Horsepower_Imputed\"]].copy()\n",
    "\n",
    "# Convert 'saledate' to datetime and extract year, month, and day\n",
    "df['saledate'] = pd.to_datetime(df['saledate'], errors='coerce')\n",
    "df['sale_year'] = df['saledate'].dt.year\n",
    "df['sale_month'] = df['saledate'].dt.month\n",
    "df['sale_day'] = df['saledate'].dt.day\n",
    "df=Enclosure_fun(df)\n",
    "df=extract_horsepower(df)\n",
    "\n",
    "\n",
    "df=preprocess_product_size(df)\n",
    "\n",
    "# Define function to update 'YearMade' and create 'YearMade_Bucket'\n",
    "def update_YearMade(df):\n",
    "    print(\"update_YearMade START\")\n",
    "    \n",
    "    # Compute median YearMade for each ModelID\n",
    "    model_medians = df.loc[df['YearMade'] > 1000].groupby('ModelID')['YearMade'].median()\n",
    "    \n",
    "    # Update YearMade where it is 1000 using ModelID median\n",
    "    df.loc[df['YearMade'] == 1000, 'YearMade'] = df['ModelID'].map(model_medians)\n",
    "    \n",
    "    # Compute overall median YearMade excluding 1000 values\n",
    "    overall_median = df.loc[df['YearMade'] > 1000, 'YearMade'].median()\n",
    "    \n",
    "    # Replace any remaining 1000 values with overall median\n",
    "    df['YearMade'].fillna(overall_median, inplace=True)\n",
    "        \n",
    "    # Bucketize YearMade\n",
    "    bins = [0, 1980, 1995, 2005, 2010, 2025]\n",
    "    labels = [\"Before 1980\", \"1980-1995\", \"1996-2005\", \"2006-2010\", \"2011-2025\"]\n",
    "    df['YearMade_Bucket'] = pd.cut(df['YearMade'], bins=bins, labels=labels, right=True)\n",
    "    \n",
    "    print(\"update_YearMade END\")\n",
    "    return df\n",
    "\n",
    "# Apply YearMade update\n",
    "df = update_YearMade(df)\n",
    "\n",
    "df.drop(columns='fiProductClassDesc',inplace=True)\n",
    "\n",
    "# Filter dataset to only include the last 5 years\n",
    "recent_years = df['sale_year'].dropna().unique()\n",
    "recent_years.sort()\n",
    "selected_years = recent_years[-5:]\n",
    "df_filtered = df[df['sale_year'].isin(selected_years)]\n",
    "\n",
    "# Drop irrelevant columns: 'SalesID', 'saledate'\n",
    "high_cardinality_cols = [col for col in df_filtered.select_dtypes(include=['object']).columns if df_filtered[col].nunique() > 50]\n",
    "df_filtered = df_filtered.drop(columns=['SalesID', 'saledate'] + high_cardinality_cols, errors='ignore')\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = df_filtered.select_dtypes(include=['object']).columns\n",
    "\n",
    "df_filtered[\"YearMade_Bucket\"] = df_filtered[\"YearMade_Bucket\"].astype(str).fillna(\"Unknown\")\n",
    "\n",
    "# Define category order and apply Ordinal Encoding\n",
    "year_bucket_encoder = OrdinalEncoder(\n",
    "    categories=[[\"Before 1980\", \"1980-1995\", \"1996-2005\", \"2006-2010\", \"2011-2025\", \"Unknown\"]],\n",
    "    handle_unknown=\"use_encoded_value\",\n",
    "    unknown_value=-1\n",
    ")\n",
    "\n",
    "df_filtered[\"YearMade_Bucket\"] = year_bucket_encoder.fit_transform(df_filtered[[\"YearMade_Bucket\"]])\n",
    "# Apply Ordinal Encoding to 'YearMade_Bucket'\n",
    "#year_bucket_encoder = OrdinalEncoder(categories=[[\"Before 1980\", \"1980-1995\", \"1996-2005\", \"2006-2010\", \"2011-2025\"]])\n",
    "#df_filtered[\"YearMade_Bucket\"] = year_bucket_encoder.fit_transform(df_filtered[[\"YearMade_Bucket\"]])\n",
    "\n",
    "# Identify high, moderate, and low cardinality categorical columns\n",
    "high_cardinality_cols = [col for col in categorical_cols if df_filtered[col].nunique() >= 50]\n",
    "moderate_cardinality_cols = [col for col in categorical_cols if 10 <= df_filtered[col].nunique() < 50]\n",
    "low_cardinality_cols = [col for col in categorical_cols if df_filtered[col].nunique() < 10]\n",
    "\n",
    "# Fill missing categorical values with \"Unknown\"\n",
    "for col in categorical_cols:\n",
    "    df_filtered[col] = df_filtered[col].fillna(\"Unknown\")\n",
    "\n",
    "# Encoding strategy\n",
    "\n",
    "# Apply One-Hot Encoding (OHE) to low-cardinality categories\n",
    "df_encoded = pd.get_dummies(df_filtered, columns=low_cardinality_cols, drop_first=True)\n",
    "\n",
    "# Apply Ordinal Encoding to moderate-cardinality categories\n",
    "if moderate_cardinality_cols:\n",
    "    ord_encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "    df_encoded[moderate_cardinality_cols] = ord_encoder.fit_transform(df_encoded[moderate_cardinality_cols])\n",
    "\n",
    "# Apply Frequency Encoding to high-cardinality categories\n",
    "for col in high_cardinality_cols:\n",
    "    freq_encoding = df_encoded[col].value_counts().to_dict()\n",
    "    df_encoded[col] = df_encoded[col].map(freq_encoding)\n",
    "\n",
    "# Drop rows with missing SalePrice (target variable)\n",
    "df_encoded = df_encoded.dropna(subset=['SalePrice'])\n",
    "\n",
    "# Split into features and target\n",
    "X = df_encoded.drop(columns=['SalePrice'])\n",
    "y = df_encoded['SalePrice']\n",
    "\n",
    "# Split data into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "y_pred = rf_model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mse ** 0.5\n",
    "\n",
    "# Display model performance\n",
    "mae, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f335eadf-74e5-4510-96bc-64a7e9d953a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feature_1_Enclosure_cat_importance': 0.28921846791011036}\n",
      "{'feature_2_YearMade_importance': 0.17637686207943865}\n",
      "{'feature_3_ModelID_importance': 0.10128183415154628}\n",
      "{'feature_4_Engine_Horsepower_Imputed_importance': 0.07265473670191194}\n",
      "{'feature_5_Extracted_Horsepower_importance': 0.06481618771286045}\n",
      "{'feature_6_ProductSize_Imputed_Large / Medium_importance': 0.05829905999977042}\n",
      "{'feature_7_sale_year_importance': 0.03754676387293485}\n",
      "{'feature_8_MachineID_importance': 0.03002420773810382}\n",
      "{'feature_9_sale_month_importance': 0.017387648234200925}\n",
      "{'feature_10_sale_day_importance': 0.015795638656916917}\n",
      "{'feature_11_MachineHoursCurrentMeter_importance': 0.012463876084173646}\n",
      "{'feature_12_ProductSize_Imputed_Mini_importance': 0.012124578230267354}\n",
      "{'feature_13_ProductGroup_SSL_importance': 0.010411929293020669}\n",
      "{'feature_14_ProductGroupDesc_Skid Steer Loaders_importance': 0.00861604672765212}\n",
      "{'feature_15_ProductSize_Imputed_Small_importance': 0.008268458145916589}\n",
      "{'feature_16_auctioneerID_importance': 0.007833956802351272}\n",
      "{'feature_17_Forks_Unknown_importance': 0.007054151178720208}\n",
      "{'feature_18_ProductSize_Imputed_Medium_importance': 0.00659104658819828}\n",
      "{'feature_19_YearMade_Bucket_importance': 0.006428204531860264}\n",
      "{'feature_20_Hydraulics_importance': 0.003134757430274705}\n",
      "{'feature_21_datasource_importance': 0.0030567319313128385}\n",
      "{'feature_22_Blade_Type_importance': 0.002828585691357918}\n",
      "{'feature_23_Steering_Controls_Conventional_importance': 0.0025764132981856503}\n",
      "{'feature_24_Ride_Control_None or Unspecified_importance': 0.0025466583303670053}\n",
      "{'feature_25_Travel_Controls_Differential Steer_importance': 0.0023254441458698785}\n",
      "{'feature_26_Tire_Size_importance': 0.002311859350176278}\n",
      "{'feature_27_Stick_Length_importance': 0.0022235291724412624}\n",
      "{'feature_28_UsageBand_Low_importance': 0.0018105738612237475}\n",
      "{'feature_29_Ripper_None or Unspecified_importance': 0.0017082754651186387}\n",
      "{'feature_30_Coupler_None or Unspecified_importance': 0.00130337955350201}\n",
      "{'feature_31_UsageBand_Medium_importance': 0.0012348413152678459}\n",
      "{'feature_32_Ride_Control_Unknown_importance': 0.0012242382458508227}\n",
      "{'feature_33_Ripper_Yes_importance': 0.0011821760615620953}\n",
      "{'feature_34_Undercarriage_Pad_Width_importance': 0.001079916721984357}\n",
      "{'feature_35_Pushblock_Yes_importance': 0.0009519877104666788}\n",
      "{'feature_36_ProductSize_Imputed_Unknown_importance': 0.0009430432878894488}\n",
      "{'feature_37_Drive_System_No_importance': 0.0009286823856330773}\n",
      "{'feature_38_Thumb_None or Unspecified_importance': 0.0009110776930046785}\n",
      "{'feature_39_Product_Type_importance': 0.0008841192409522725}\n",
      "{'feature_40_Coupler_Manual_importance': 0.0008418670003303769}\n",
      "{'feature_41_Coupler_System_Unknown_importance': 0.000800731471392276}\n",
      "{'feature_42_UsageBand_Unknown_importance': 0.0007594599156727339}\n",
      "{'feature_43_ProductGroupDesc_Motor Graders_importance': 0.0007412756516684288}\n",
      "{'feature_44_Travel_Controls_None or Unspecified_importance': 0.0007411157107502475}\n",
      "{'feature_45_Grouser_Type_Triple_importance': 0.0007102679324988134}\n",
      "{'feature_46_Transmission_Standard_importance': 0.0006949821789680534}\n",
      "{'feature_47_ProductGroup_MG_importance': 0.0006863211138061236}\n",
      "{'feature_48_Scarifier_Yes_importance': 0.0006691398660364265}\n",
      "{'feature_49_Ride_Control_Yes_importance': 0.0006656863651191232}\n",
      "{'feature_50_Track_Type_Steel_importance': 0.0006562016169857578}\n",
      "{'feature_51_Pattern_Changer_Yes_importance': 0.0006480628846348371}\n",
      "{'feature_52_Ripper_Single Shank_importance': 0.0006259598658852499}\n",
      "{'feature_53_Blade_Width_Unknown_importance': 0.0005481569513063181}\n",
      "{'feature_54_Forks_Yes_importance': 0.0005425429391388144}\n",
      "{'feature_55_Tip_Control_Unknown_importance': 0.0004975344004312868}\n",
      "{'feature_56_Scarifier_Unknown_importance': 0.0004895923960207336}\n",
      "{'feature_57_Thumb_Manual_importance': 0.0004631503773269696}\n",
      "{'feature_58_Backhoe_Mounting_Unknown_importance': 0.0004566783972741086}\n",
      "{'feature_59_Transmission_Unknown_importance': 0.0004566299438158755}\n",
      "{'feature_60_Drive_System_Unknown_importance': 0.0004304545467691622}\n",
      "{'feature_61_Transmission_Powershift_importance': 0.0004057626086904712}\n",
      "{'feature_62_ProductSize_Imputed_Large_importance': 0.0004014517448657356}\n",
      "{'feature_63_Pushblock_Unknown_importance': 0.00039327279050842707}\n",
      "{'feature_64_Enclosure_Type_Unknown_importance': 0.0003803455835275989}\n",
      "{'feature_65_Blade_Extension_Unknown_importance': 0.0003685251535499987}\n",
      "{'feature_66_Transmission_None or Unspecified_importance': 0.0003599725467654271}\n",
      "{'feature_67_Drive_System_Two Wheel Drive_importance': 0.0003494459195454377}\n",
      "{\"feature_68_Blade_Width_16'_importance\": 0.0003416774504009022}\n",
      "{'feature_69_Enclosure_Type_Low Profile_importance': 0.00033731349725035455}\n",
      "{'feature_70_Ripper_Unknown_importance': 0.0003203536986518878}\n",
      "{'feature_71_Differential_Type_Standard_importance': 0.00030810855255492697}\n",
      "{'feature_72_Stick_Standard_importance': 0.0002994727082064262}\n",
      "{\"feature_73_Blade_Width_14'_importance\": 0.0002881496001626918}\n",
      "{'feature_74_Coupler_Unknown_importance': 0.0002772793548385611}\n",
      "{'feature_75_Enclosure_Type_None or Unspecified_importance': 0.0002741893495935988}\n",
      "{'feature_76_Pattern_Changer_None or Unspecified_importance': 0.00026434649178085084}\n",
      "{'feature_77_Tip_Control_Sideshift & Tip_importance': 0.00025703585213517044}\n",
      "{'feature_78_Pad_Type_None or Unspecified_importance': 0.00024858462032403757}\n",
      "{'feature_79_Drive_System_Four Wheel Drive_importance': 0.00023757297423178516}\n",
      "{'feature_80_Travel_Controls_Unknown_importance': 0.00022626834038074023}\n",
      "{'feature_81_Travel_Controls_Finger Tip_importance': 0.00021510998492622942}\n",
      "{'feature_82_Blade_Width_None or Unspecified_importance': 0.00020294493104992326}\n",
      "{'feature_83_ProductGroupDesc_Track Type Tractors_importance': 0.00018259278005306866}\n",
      "{'feature_84_Transmission_Hydrostatic_importance': 0.00018177954140911795}\n",
      "{'feature_85_ProductGroupDesc_Wheel Loader_importance': 0.00016252274545565672}\n",
      "{'feature_86_Travel_Controls_Lever_importance': 0.00014109577847326956}\n",
      "{'feature_87_ProductGroup_WL_importance': 0.0001360265263921992}\n",
      "{'feature_88_ProductGroup_TTT_importance': 0.00013170669630608561}\n",
      "{'feature_89_Steering_Controls_Unknown_importance': 9.424522984243579e-05}\n",
      "{'feature_90_Tip_Control_Tip_importance': 8.885865002709175e-05}\n",
      "{'feature_91_Differential_Type_Unknown_importance': 8.768071144488872e-05}\n",
      "{'feature_92_Blade_Extension_Yes_importance': 5.243363545506391e-05}\n",
      "{'feature_93_Grouser_Type_Unknown_importance': 3.9880234478212845e-05}\n",
      "{'feature_94_Hydraulics_Flow_Unknown_importance': 3.5280623002060355e-05}\n",
      "{'feature_95_Pattern_Changer_Unknown_importance': 3.428858260294437e-05}\n",
      "{'feature_96_Track_Type_Unknown_importance': 3.053146175358191e-05}\n",
      "{'feature_97_Transmission_Powershuttle_importance': 3.0371750367111943e-05}\n",
      "{'feature_98_Thumb_Unknown_importance': 2.9510619001533503e-05}\n",
      "{'feature_99_Turbocharged_Yes_importance': 2.82048441019015e-05}\n",
      "{'feature_100_ProductGroupDesc_Track Excavators_importance': 2.5281376288701808e-05}\n",
      "{'feature_101_Travel_Controls_2 Pedal_importance': 2.4945173105205215e-05}\n",
      "{'feature_102_Grouser_Tracks_Yes_importance': 2.324722792263331e-05}\n",
      "{'feature_103_Horsepower_Unit_Type_Unknown_importance': 2.288216309672351e-05}\n",
      "{'feature_104_Grouser_Tracks_Unknown_importance': 2.2163651646838813e-05}\n",
      "{'feature_105_Pad_Type_Reversible_importance': 2.036281504248084e-05}\n",
      "{'feature_106_Differential_Type_No Spin_importance': 1.980893863296026e-05}\n",
      "{'feature_107_Pad_Type_Street_importance': 1.9727379007454482e-05}\n",
      "{'feature_108_ProductGroup_TEX_importance': 1.6964699828698442e-05}\n",
      "{\"feature_109_Blade_Width_13'_importance\": 1.6007779197890947e-05}\n",
      "{'feature_110_Hydraulics_Flow_Standard_importance': 1.0370917171032682e-05}\n",
      "{'feature_111_Transmission_Direct Drive_importance': 9.274516774280501e-06}\n",
      "{'feature_112_Stick_Unknown_importance': 7.979384570003981e-06}\n",
      "{'feature_113_Coupler_System_Yes_importance': 7.331038990999004e-06}\n",
      "{'feature_114_Pad_Type_Unknown_importance': 6.238030148890106e-06}\n",
      "{'feature_115_Turbocharged_Unknown_importance': 6.18247082854662e-06}\n",
      "{'feature_116_Travel_Controls_Pedal_importance': 5.373187869746043e-06}\n",
      "{'feature_117_Steering_Controls_Four Wheel Standard_importance': 2.663815197680765e-06}\n",
      "{\"feature_118_Blade_Width_<12'_importance\": 1.6806110692034693e-06}\n",
      "{'feature_119_Transmission_Autoshift_importance': 9.26460038043628e-07}\n",
      "{'feature_120_Steering_Controls_Wheel_importance': 3.448790870343982e-07}\n",
      "{'feature_121_Hydraulics_Flow_None or Unspecified_importance': 1.445168342579979e-07}\n",
      "{'feature_122_Backhoe_Mounting_Yes_importance': 1.1517881610595511e-07}\n",
      "{'feature_123_Differential_Type_Locking_importance': 3.256649929779156e-08}\n"
     ]
    }
   ],
   "source": [
    "# Optional: Log feature importances\n",
    "importances = rf_model.feature_importances_\n",
    "\n",
    "# Create a zip of feature names and feature importances\n",
    "feature_zip = zip(rf_model.feature_names_in_, rf_model.feature_importances_)\n",
    "\n",
    "# Sort the zip by feature importance in descending order\n",
    "sorted_feature_zip = sorted(feature_zip, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature names, importances, and index\n",
    "for idx, (feat, importance) in enumerate(sorted_feature_zip, 1):\n",
    "  print({f\"feature_{idx}_{feat}_importance\" : importance})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "587e9c47-03a2-4621-9a29-650a74ff1260",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eitanb\\AppData\\Local\\Temp\\ipykernel_12692\\1407107489.py:135: DtypeWarning: Columns (13,39,40,41) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path_train)\n",
      "C:\\Users\\eitanb\\AppData\\Local\\Temp\\ipykernel_12692\\1407107489.py:32: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  df['Engine_Horsepower_Imputed'] = df['Engine_Horsepower'].combine_first(df['Extracted_Horsepower'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update_YearMade START\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eitanb\\AppData\\Local\\Temp\\ipykernel_12692\\1407107489.py:161: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[1980. 1980. 1994. ... 2003. 2004. 2004.]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[df['YearMade'] == 1000, 'YearMade'] = df['ModelID'].map(model_medians)\n",
      "C:\\Users\\eitanb\\AppData\\Local\\Temp\\ipykernel_12692\\1407107489.py:167: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['YearMade'].fillna(overall_median, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update_YearMade END\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4833.200946416145, 7782.836218132495)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-import necessary libraries due to execution reset\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "def extract_horsepower(df):\n",
    "    \"\"\"\n",
    "    Extracts and imputes missing Horsepower values from fiProductClassDesc.\n",
    "    Optimized for performance using vectorized operations.\n",
    "    \"\"\"\n",
    "    def extract_numeric_range(value):\n",
    "        \"\"\"Extracts the average horsepower value from a range like '100 to 120 Horsepower'.\"\"\"\n",
    "        if pd.isna(value) or not isinstance(value, str):\n",
    "            return np.nan\n",
    "        numbers = re.findall(r'[\\d\\.]+', value)\n",
    "        if len(numbers) == 2:\n",
    "            return (float(numbers[0]) + float(numbers[1])) / 2\n",
    "        elif len(numbers) == 1:\n",
    "            return float(numbers[0])\n",
    "        return np.nan\n",
    "    \n",
    "    # Identify rows with Horsepower in fiProductClassDesc\n",
    "    mask_horsepower = df['fiProductClassDesc'].str.contains(\"Horsepower\", na=False)\n",
    "    \n",
    "    # Extract horsepower values\n",
    "    df.loc[mask_horsepower, 'Horsepower_Unit_Type'] = 'Horsepower'\n",
    "    df.loc[mask_horsepower, 'Extracted_Horsepower'] = df.loc[mask_horsepower, 'fiProductClassDesc'].apply(extract_numeric_range)\n",
    "    \n",
    "    # Handle 'Variable' and 'No' values in Engine_Horsepower\n",
    "    df.loc[df['Engine_Horsepower'].isin(['Variable', 'No']), 'Engine_Horsepower'] = np.nan\n",
    "    \n",
    "    # Impute missing Engine_Horsepower values where Extracted_Horsepower is available\n",
    "    df['Engine_Horsepower_Imputed'] = df['Engine_Horsepower'].combine_first(df['Extracted_Horsepower'])\n",
    "    \n",
    "    # Fill remaining NaN values with median horsepower\n",
    "    #df['Engine_Horsepower_Imputed'].fillna(df['Engine_Horsepower_Imputed'].median(), inplace=True)\n",
    "    df.drop(columns='Engine_Horsepower',inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def preprocess_product_size(df):\n",
    "    \"\"\"\n",
    "    Function to preprocess and impute missing ProductSize values based on fiProductClassDesc.\n",
    "    \n",
    "    Steps:\n",
    "    1. Extract Product Type and Metric Tons / Horsepower from fiProductClassDesc.\n",
    "    2. Identify Unit Type (Metric Tons, Horsepower, or Lb Operating Capacity).\n",
    "    3. Convert Metric Tons / Horsepower to numerical values.\n",
    "    4. Use Metric Tons to impute missing ProductSize values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Extract Product Type and Size Descriptor\n",
    "    def split_product_desc(desc):\n",
    "        if pd.isna(desc):\n",
    "            return pd.NA, pd.NA\n",
    "        match = re.search(r'(.+?)-\\s*([\\d\\.]+.*)', desc)\n",
    "        if match:\n",
    "            return match.group(1).strip(), match.group(2).strip()\n",
    "        return desc.strip(), pd.NA  # If no match, return full description as product\n",
    "    \n",
    "    df[['Product_Type', 'Metric_Tons_HP']] = df['fiProductClassDesc'].apply(lambda x: pd.Series(split_product_desc(x)))\n",
    "    \n",
    "    # Step 2: Identify Unit Type\n",
    "    def extract_unit_type(desc):\n",
    "        if pd.isna(desc):\n",
    "            return pd.NA\n",
    "        if \"Metric Tons\" in desc:\n",
    "            return \"Metric Tons\"\n",
    "        elif \"Horsepower\" in desc:\n",
    "            return \"Horsepower\"\n",
    "        elif \"Lb Operating Capacity\" in desc:\n",
    "            return \"Lb Operating Capacity\"\n",
    "        return pd.NA\n",
    "    \n",
    "    df[\"Unit_Type\"] = df[\"Metric_Tons_HP\"].apply(extract_unit_type)\n",
    "    \n",
    "    # Step 3: Convert Metric Tons to numerical values\n",
    "    def extract_numeric_range(value):\n",
    "        \"\"\"Extracts the average value from a range like '12.0 to 14.0 Metric Tons'.\"\"\"\n",
    "        if pd.isna(value) or not isinstance(value, str):\n",
    "            return np.nan\n",
    "        numbers = re.findall(r'[\\d\\.]+', value)\n",
    "        if len(numbers) == 2:  # If range exists, take the average\n",
    "            return (float(numbers[0]) + float(numbers[1])) / 2\n",
    "        elif len(numbers) == 1:  # If only one number exists, use it\n",
    "            return float(numbers[0])\n",
    "        return np.nan\n",
    "    \n",
    "    df['Metric_Tons_Value'] = df['Metric_Tons_HP'].apply(lambda x: extract_numeric_range(x) if isinstance(x, str) else np.nan)\n",
    "    \n",
    "    # Step 4: Impute missing ProductSize values using Metric Tons\n",
    "    def impute_product_size(row):\n",
    "        if pd.isna(row['ProductSize']) and not pd.isna(row['Metric_Tons_Value']) and (row['Metric_Tons_Value']=='Metric Tons'):\n",
    "            if row['Metric_Tons_Value']  <= 5:\n",
    "                return 'Mini'\n",
    "            elif 5 < row['Metric_Tons_Value'] <= 75:\n",
    "                return 'Compact'\n",
    "            elif 20 < row['Metric_Tons_Value'] <= 50:\n",
    "                return 'Large / Medium'\n",
    "            elif 75 < row['Metric_Tons_Value'] <= 200:\n",
    "                return 'Medium'\n",
    "            elif row['Metric_Tons_Value'] > 200:\n",
    "                return 'Large'\n",
    "        return row['ProductSize']\n",
    "    \n",
    "    df['ProductSize_Imputed'] = df.apply(impute_product_size, axis=1)\n",
    "    df.drop(columns=['ProductSize','Metric_Tons_Value','Metric_Tons_HP','Unit_Type'],inplace=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    return df\n",
    "def Enclosure_fun(df):\n",
    "    list1=['EROPS','OROPS','EROPS AC']\n",
    "    list2=['NO ROPS','None or Unspecified']\n",
    "    df['Enclosure_cat']=0\n",
    "    df.loc[df['Enclosure'].isin(list1), 'Enclosure_cat'] = 1\n",
    "    df.loc[df['Enclosure'].isin(list2), 'Enclosure_cat'] = 0\n",
    "    df.loc[df['Enclosure']=='EROPS w AC','Enclosure_cat']=2\n",
    "    df.drop(columns='Enclosure',inplace=True)\n",
    "    return df\n",
    "# Define categories based on observed price trends\n",
    "def categorize_hydraulics(value):\n",
    "    if value in [\"Missing\", \"Auxiliary\", \"Standard\"]:\n",
    "        return \"Basic\"\n",
    "    elif value in [\"2 Valve\", \"None or Unspecified\", \"3 Valve\"]:\n",
    "        return \"Mid-Level\"\n",
    "    else:\n",
    "        return \"Advanced\"\n",
    "\n",
    "# Apply categorization\n",
    "df[\"Hydraulics_Category\"] = df[\"Hydraulics\"].apply(categorize_hydraulics)\n",
    "\n",
    "# Reload the dataset\n",
    "ProductSize_path=r'C:\\Users\\eitanb\\Documents\\DS\\ML\\ML_project\\DATA/ProductSize_Null_fix_model.pkl'\n",
    "file_path_train = r\"C:\\Users\\eitanb\\Documents\\DS\\ML\\ML_project\\DATA/Train.csv\"\n",
    "df = pd.read_csv(file_path_train)\n",
    "#df=df[['SalesID','saledate',\"Enclosure\",\"YearMade\",\"ModelID\",\"MachineID\",\"Ripper\",\"ProductGroup\",\"ProductSize\",\"ProductGroupDesc\",\"Tire_Size\",\"MachineHoursCurrentMeter\",\"Blade_Type\",\"ProductGroupDesc\",\"Travel_Controls\",\"auctioneerID\",\"datasource\",\"Drive_System\",\"Pushblock\",\"Tire_Size\",\"Ride_Control\",\"UsageBand\",\"Coupler\",\"Engine_Horsepower_Imputed\"]].copy()\n",
    "\n",
    "# Convert 'saledate' to datetime and extract year, month, and day\n",
    "df['saledate'] = pd.to_datetime(df['saledate'], errors='coerce')\n",
    "df['sale_year'] = df['saledate'].dt.year\n",
    "df['sale_month'] = df['saledate'].dt.month\n",
    "df['sale_day'] = df['saledate'].dt.day\n",
    "\n",
    "# Apply categorization\n",
    "df[\"Hydraulics_Category\"] = df[\"Hydraulics\"].apply(categorize_hydraulics)\n",
    "\n",
    "df=Enclosure_fun(df)\n",
    "df=extract_horsepower(df)\n",
    "\n",
    "\n",
    "df=preprocess_product_size(df)\n",
    "\n",
    "# Define function to update 'YearMade' and create 'YearMade_Bucket'\n",
    "def update_YearMade(df):\n",
    "    print(\"update_YearMade START\")\n",
    "    \n",
    "    # Compute median YearMade for each ModelID\n",
    "    model_medians = df.loc[df['YearMade'] > 1000].groupby('ModelID')['YearMade'].median()\n",
    "    \n",
    "    # Update YearMade where it is 1000 using ModelID median\n",
    "    df.loc[df['YearMade'] == 1000, 'YearMade'] = df['ModelID'].map(model_medians)\n",
    "    \n",
    "    # Compute overall median YearMade excluding 1000 values\n",
    "    overall_median = df.loc[df['YearMade'] > 1000, 'YearMade'].median()\n",
    "    \n",
    "    # Replace any remaining 1000 values with overall median\n",
    "    df['YearMade'].fillna(overall_median, inplace=True)\n",
    "        \n",
    "    # Bucketize YearMade\n",
    "    bins = [0, 1980, 1995, 2005, 2010, 2025]\n",
    "    labels = [\"Before 1980\", \"1980-1995\", \"1996-2005\", \"2006-2010\", \"2011-2025\"]\n",
    "    df['YearMade_Bucket'] = pd.cut(df['YearMade'], bins=bins, labels=labels, right=True)\n",
    "    \n",
    "    print(\"update_YearMade END\")\n",
    "    return df\n",
    "\n",
    "# Apply YearMade update\n",
    "df = update_YearMade(df)\n",
    "\n",
    "df.drop(columns=['fiProductClassDesc','Hydraulics'],inplace=True)\n",
    "\n",
    "\n",
    "# Filter dataset to only include the last 5 years\n",
    "recent_years = df['sale_year'].dropna().unique()\n",
    "recent_years.sort()\n",
    "selected_years = recent_years[-5:]\n",
    "df_filtered = df[df['sale_year'].isin(selected_years)]\n",
    "\n",
    "# Drop irrelevant columns: 'SalesID', 'saledate'\n",
    "high_cardinality_cols = [col for col in df_filtered.select_dtypes(include=['object']).columns if df_filtered[col].nunique() > 50]\n",
    "df_filtered = df_filtered.drop(columns=['SalesID', 'saledate'] + high_cardinality_cols, errors='ignore')\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = df_filtered.select_dtypes(include=['object']).columns\n",
    "\n",
    "df_filtered[\"YearMade_Bucket\"] = df_filtered[\"YearMade_Bucket\"].astype(str).fillna(\"Unknown\")\n",
    "\n",
    "# Define category order and apply Ordinal Encoding\n",
    "year_bucket_encoder = OrdinalEncoder(\n",
    "    categories=[[\"Before 1980\", \"1980-1995\", \"1996-2005\", \"2006-2010\", \"2011-2025\", \"Unknown\"]],\n",
    "    handle_unknown=\"use_encoded_value\",\n",
    "    unknown_value=-1\n",
    ")\n",
    "\n",
    "df_filtered[\"YearMade_Bucket\"] = year_bucket_encoder.fit_transform(df_filtered[[\"YearMade_Bucket\"]])\n",
    "# Apply Ordinal Encoding to 'YearMade_Bucket'\n",
    "#year_bucket_encoder = OrdinalEncoder(categories=[[\"Before 1980\", \"1980-1995\", \"1996-2005\", \"2006-2010\", \"2011-2025\"]])\n",
    "#df_filtered[\"YearMade_Bucket\"] = year_bucket_encoder.fit_transform(df_filtered[[\"YearMade_Bucket\"]])\n",
    "\n",
    "# Identify high, moderate, and low cardinality categorical columns\n",
    "high_cardinality_cols = [col for col in categorical_cols if df_filtered[col].nunique() >= 50]\n",
    "moderate_cardinality_cols = [col for col in categorical_cols if 10 <= df_filtered[col].nunique() < 50]\n",
    "low_cardinality_cols = [col for col in categorical_cols if df_filtered[col].nunique() < 10]\n",
    "\n",
    "# Fill missing categorical values with \"Unknown\"\n",
    "for col in categorical_cols:\n",
    "    df_filtered[col] = df_filtered[col].fillna(\"Unknown\")\n",
    "\n",
    "# Encoding strategy\n",
    "\n",
    "# Apply One-Hot Encoding (OHE) to low-cardinality categories\n",
    "df_encoded = pd.get_dummies(df_filtered, columns=low_cardinality_cols, drop_first=True)\n",
    "\n",
    "# Apply Ordinal Encoding to moderate-cardinality categories\n",
    "if moderate_cardinality_cols:\n",
    "    ord_encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "    df_encoded[moderate_cardinality_cols] = ord_encoder.fit_transform(df_encoded[moderate_cardinality_cols])\n",
    "\n",
    "# Apply Frequency Encoding to high-cardinality categories\n",
    "for col in high_cardinality_cols:\n",
    "    freq_encoding = df_encoded[col].value_counts().to_dict()\n",
    "    df_encoded[col] = df_encoded[col].map(freq_encoding)\n",
    "\n",
    "# Drop rows with missing SalePrice (target variable)\n",
    "df_encoded = df_encoded.dropna(subset=['SalePrice'])\n",
    "\n",
    "# Split into features and target\n",
    "X = df_encoded.drop(columns=['SalePrice'])\n",
    "y = df_encoded['SalePrice']\n",
    "\n",
    "# Split data into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "y_pred = rf_model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mse ** 0.5\n",
    "\n",
    "# Display model performance\n",
    "mae, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "005c6ef2-b984-4f42-8ba9-a77897013618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feature_1_Enclosure_cat_importance': 0.28921994640432164}\n",
      "{'feature_2_YearMade_importance': 0.17652927949578853}\n",
      "{'feature_3_ModelID_importance': 0.1015162105349354}\n",
      "{'feature_4_Extracted_Horsepower_importance': 0.07111184812668449}\n",
      "{'feature_5_Engine_Horsepower_Imputed_importance': 0.06632425226911177}\n",
      "{'feature_6_ProductSize_Imputed_Large / Medium_importance': 0.05829994934360587}\n",
      "{'feature_7_sale_year_importance': 0.037558305778334175}\n",
      "{'feature_8_MachineID_importance': 0.030305171386477717}\n",
      "{'feature_9_sale_month_importance': 0.017495608912622744}\n",
      "{'feature_10_sale_day_importance': 0.0160042991561733}\n",
      "{'feature_11_MachineHoursCurrentMeter_importance': 0.012586992701690608}\n",
      "{'feature_12_ProductSize_Imputed_Mini_importance': 0.01212976145386422}\n",
      "{'feature_13_ProductGroupDesc_Skid Steer Loaders_importance': 0.009914426648220755}\n",
      "{'feature_14_ProductGroup_SSL_importance': 0.00911290226458472}\n",
      "{'feature_15_ProductSize_Imputed_Small_importance': 0.008259533608617971}\n",
      "{'feature_16_auctioneerID_importance': 0.007898678995268377}\n",
      "{'feature_17_Forks_Unknown_importance': 0.007070646165796703}\n",
      "{'feature_18_ProductSize_Imputed_Medium_importance': 0.006526332137919823}\n",
      "{'feature_19_YearMade_Bucket_importance': 0.0063064634072705385}\n",
      "{'feature_20_datasource_importance': 0.003108429674391968}\n",
      "{'feature_21_Blade_Type_importance': 0.0028040399804386005}\n",
      "{'feature_22_Steering_Controls_Conventional_importance': 0.0026260667922486515}\n",
      "{'feature_23_Ride_Control_None or Unspecified_importance': 0.002526761090035153}\n",
      "{'feature_24_Travel_Controls_Differential Steer_importance': 0.002342782766124241}\n",
      "{'feature_25_Tire_Size_importance': 0.0023274223475595332}\n",
      "{'feature_26_Stick_Length_importance': 0.002256135714533182}\n",
      "{'feature_27_UsageBand_Low_importance': 0.0018197728939698655}\n",
      "{'feature_28_Ripper_None or Unspecified_importance': 0.0017316688562648672}\n",
      "{'feature_29_Hydraulics_Category_Mid-Level_importance': 0.0014573572423950674}\n",
      "{'feature_30_Coupler_None or Unspecified_importance': 0.0013276227282230082}\n",
      "{'feature_31_Ride_Control_Unknown_importance': 0.0013273894548668256}\n",
      "{'feature_32_UsageBand_Medium_importance': 0.0012475238524255604}\n",
      "{'feature_33_Ripper_Yes_importance': 0.001176531390321391}\n",
      "{'feature_34_Product_Type_importance': 0.0011446017864693134}\n",
      "{'feature_35_Undercarriage_Pad_Width_importance': 0.00109613927109294}\n",
      "{'feature_36_ProductSize_Imputed_Unknown_importance': 0.0010068158889589569}\n",
      "{'feature_37_Pushblock_Yes_importance': 0.0009652102270613289}\n",
      "{'feature_38_Thumb_None or Unspecified_importance': 0.0009349160252444023}\n",
      "{'feature_39_Drive_System_No_importance': 0.0008802015216549628}\n",
      "{'feature_40_Coupler_Manual_importance': 0.0008659756116576495}\n",
      "{'feature_41_ProductGroupDesc_Motor Graders_importance': 0.0008624024277848213}\n",
      "{'feature_42_UsageBand_Unknown_importance': 0.0007774393933628497}\n",
      "{'feature_43_Travel_Controls_None or Unspecified_importance': 0.0007223797964101444}\n",
      "{'feature_44_Grouser_Type_Triple_importance': 0.0007191141523086975}\n",
      "{'feature_45_Transmission_Standard_importance': 0.0006875150972503071}\n",
      "{'feature_46_Scarifier_Yes_importance': 0.000666031987458924}\n",
      "{'feature_47_Track_Type_Steel_importance': 0.000661493701343353}\n",
      "{'feature_48_Ride_Control_Yes_importance': 0.0006588815354840628}\n",
      "{'feature_49_Pattern_Changer_Yes_importance': 0.0006499983416581708}\n",
      "{'feature_50_Coupler_System_Unknown_importance': 0.0006419562308388325}\n",
      "{'feature_51_Ripper_Single Shank_importance': 0.0006343962172087379}\n",
      "{'feature_52_ProductGroup_MG_importance': 0.0005864789556139023}\n",
      "{'feature_53_Blade_Extension_Unknown_importance': 0.000580980572725053}\n",
      "{'feature_54_Forks_Yes_importance': 0.0005466128248381945}\n",
      "{'feature_55_Backhoe_Mounting_Unknown_importance': 0.00048185105638231477}\n",
      "{'feature_56_Tip_Control_Unknown_importance': 0.00047641102501999594}\n",
      "{'feature_57_Thumb_Manual_importance': 0.00047058526824722216}\n",
      "{'feature_58_Blade_Width_Unknown_importance': 0.0004533624384855121}\n",
      "{'feature_59_Pushblock_Unknown_importance': 0.0004420385708217254}\n",
      "{'feature_60_Scarifier_Unknown_importance': 0.0004018285353166666}\n",
      "{'feature_61_ProductSize_Imputed_Large_importance': 0.00040072276608649275}\n",
      "{'feature_62_Transmission_Powershift_importance': 0.0003941777291608211}\n",
      "{'feature_63_Transmission_None or Unspecified_importance': 0.0003736834015581831}\n",
      "{'feature_64_Ripper_Unknown_importance': 0.00036621203042465665}\n",
      "{'feature_65_Enclosure_Type_Low Profile_importance': 0.0003593776565598168}\n",
      "{\"feature_66_Blade_Width_16'_importance\": 0.0003518681631263398}\n",
      "{'feature_67_Drive_System_Two Wheel Drive_importance': 0.0003467688960918651}\n",
      "{'feature_68_Differential_Type_Standard_importance': 0.00033128077825283613}\n",
      "{'feature_69_Transmission_Unknown_importance': 0.00032331869566495144}\n",
      "{'feature_70_Enclosure_Type_Unknown_importance': 0.0003048476201012138}\n",
      "{'feature_71_Stick_Standard_importance': 0.00029918611465140713}\n",
      "{'feature_72_Drive_System_Unknown_importance': 0.00029665674648731886}\n",
      "{\"feature_73_Blade_Width_14'_importance\": 0.0002940475052738222}\n",
      "{'feature_74_Enclosure_Type_None or Unspecified_importance': 0.00029034474942747423}\n",
      "{'feature_75_Pattern_Changer_None or Unspecified_importance': 0.0002793831615267703}\n",
      "{'feature_76_Tip_Control_Sideshift & Tip_importance': 0.00026144581136743045}\n",
      "{'feature_77_Coupler_Unknown_importance': 0.0002553911502168456}\n",
      "{'feature_78_Pad_Type_None or Unspecified_importance': 0.00025053047974080925}\n",
      "{'feature_79_Drive_System_Four Wheel Drive_importance': 0.00024702547835442085}\n",
      "{'feature_80_Travel_Controls_Unknown_importance': 0.00024274927815591955}\n",
      "{'feature_81_Travel_Controls_Finger Tip_importance': 0.00021831263778849215}\n",
      "{'feature_82_Hydraulics_Category_Basic_importance': 0.0002154577945323806}\n",
      "{'feature_83_Blade_Width_None or Unspecified_importance': 0.00020756007163689683}\n",
      "{'feature_84_ProductGroup_WL_importance': 0.0001996001540948804}\n",
      "{'feature_85_ProductGroup_TTT_importance': 0.00018388343911879019}\n",
      "{'feature_86_Transmission_Hydrostatic_importance': 0.00017992558721308897}\n",
      "{'feature_87_ProductGroupDesc_Track Type Tractors_importance': 0.00015492012039817948}\n",
      "{'feature_88_ProductGroupDesc_Wheel Loader_importance': 0.0001510996526893773}\n",
      "{'feature_89_Travel_Controls_Lever_importance': 0.00014698716240671192}\n",
      "{'feature_90_Tip_Control_Tip_importance': 9.439417706383014e-05}\n",
      "{'feature_91_Steering_Controls_Unknown_importance': 8.604359137987652e-05}\n",
      "{'feature_92_Differential_Type_Unknown_importance': 8.533687646639227e-05}\n",
      "{'feature_93_Blade_Extension_Yes_importance': 5.522525304001061e-05}\n",
      "{'feature_94_Grouser_Type_Unknown_importance': 4.235662986375592e-05}\n",
      "{'feature_95_Pattern_Changer_Unknown_importance': 3.681092575690738e-05}\n",
      "{'feature_96_Transmission_Powershuttle_importance': 3.07813661026515e-05}\n",
      "{'feature_97_Thumb_Unknown_importance': 3.0215550805405465e-05}\n",
      "{'feature_98_Hydraulics_Flow_Unknown_importance': 2.8759908480493748e-05}\n",
      "{'feature_99_Track_Type_Unknown_importance': 2.8372276730229303e-05}\n",
      "{'feature_100_Turbocharged_Yes_importance': 2.8160977410079723e-05}\n",
      "{'feature_101_ProductGroup_TEX_importance': 2.4324963605174426e-05}\n",
      "{'feature_102_Grouser_Tracks_Yes_importance': 2.3723534143624693e-05}\n",
      "{'feature_103_Grouser_Tracks_Unknown_importance': 2.3698701753567883e-05}\n",
      "{'feature_104_Horsepower_Unit_Type_Unknown_importance': 2.1480231524265516e-05}\n",
      "{'feature_105_Pad_Type_Reversible_importance': 2.0162254532804892e-05}\n",
      "{'feature_106_Pad_Type_Street_importance': 1.997177202414736e-05}\n",
      "{'feature_107_Travel_Controls_2 Pedal_importance': 1.9117422258777836e-05}\n",
      "{'feature_108_ProductGroupDesc_Track Excavators_importance': 1.7927261417609673e-05}\n",
      "{'feature_109_Differential_Type_No Spin_importance': 1.7453505993713025e-05}\n",
      "{\"feature_110_Blade_Width_13'_importance\": 1.5169450878118402e-05}\n",
      "{'feature_111_Transmission_Direct Drive_importance': 1.0877321818054355e-05}\n",
      "{'feature_112_Hydraulics_Flow_Standard_importance': 1.0689885869849589e-05}\n",
      "{'feature_113_Coupler_System_Yes_importance': 7.295106173539579e-06}\n",
      "{'feature_114_Turbocharged_Unknown_importance': 6.3724334013795365e-06}\n",
      "{'feature_115_Pad_Type_Unknown_importance': 6.114986021618955e-06}\n",
      "{'feature_116_Travel_Controls_Pedal_importance': 5.731295393711063e-06}\n",
      "{'feature_117_Stick_Unknown_importance': 5.288633175612617e-06}\n",
      "{'feature_118_Steering_Controls_Four Wheel Standard_importance': 2.604553840994028e-06}\n",
      "{\"feature_119_Blade_Width_<12'_importance\": 1.5274513624697439e-06}\n",
      "{'feature_120_Transmission_Autoshift_importance': 9.096389933732901e-07}\n",
      "{'feature_121_Steering_Controls_Wheel_importance': 2.1398155268453376e-07}\n",
      "{'feature_122_Hydraulics_Flow_None or Unspecified_importance': 1.8078715189455293e-07}\n",
      "{'feature_123_Backhoe_Mounting_Yes_importance': 1.1954574604368187e-07}\n",
      "{'feature_124_Differential_Type_Locking_importance': 9.088037281368423e-08}\n"
     ]
    }
   ],
   "source": [
    "# Optional: Log feature importances\n",
    "importances = rf_model.feature_importances_\n",
    "\n",
    "# Create a zip of feature names and feature importances\n",
    "feature_zip = zip(rf_model.feature_names_in_, rf_model.feature_importances_)\n",
    "\n",
    "# Sort the zip by feature importance in descending order\n",
    "sorted_feature_zip = sorted(feature_zip, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature names, importances, and index\n",
    "for idx, (feat, importance) in enumerate(sorted_feature_zip, 1):\n",
    "  print({f\"feature_{idx}_{feat}_importance\" : importance})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e23ab6c-5744-4d6a-8036-fa31b73bee86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eitanb\\AppData\\Local\\Temp\\ipykernel_12692\\3362339656.py:201: DtypeWarning: Columns (13,39,40,41) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path_train)\n",
      "C:\\Users\\eitanb\\AppData\\Local\\Temp\\ipykernel_12692\\3362339656.py:99: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  df['Engine_Horsepower_Imputed'] = df['Engine_Horsepower'].combine_first(df['Extracted_Horsepower'])\n",
      "C:\\Users\\eitanb\\AppData\\Local\\Temp\\ipykernel_12692\\3362339656.py:66: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"Predicted_Model_Category\"].fillna(\"Unknown\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update_YearMade START\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eitanb\\AppData\\Local\\Temp\\ipykernel_12692\\3362339656.py:238: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[1980. 1980. 1994. ... 2003. 2004. 2004.]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[df['YearMade'] == 1000, 'YearMade'] = df['ModelID'].map(model_medians)\n",
      "C:\\Users\\eitanb\\AppData\\Local\\Temp\\ipykernel_12692\\3362339656.py:244: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['YearMade'].fillna(overall_median, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update_YearMade END\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4669.066033377101, 7427.552140505497)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-import necessary libraries due to execution reset\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Define price-based categories for ProductGroup\n",
    "def categorize_product_group(price):\n",
    "    if price < 20000:\n",
    "        return \"Low Price Group\"\n",
    "    elif 20000 <= price < 40000:\n",
    "        return \"Mid Price Group\"\n",
    "    else:\n",
    "        return \"High Price Group\"\n",
    "\n",
    "\n",
    "def create_model_category_mapping(training_df):\n",
    "    \"\"\"\n",
    "    Creates a mapping of ModelID to Model_Category based on fiModelDesc and SalePrice in training data.\n",
    "    \n",
    "    Parameters:\n",
    "    training_df (pd.DataFrame): Training dataset containing fiModelDesc and SalePrice.\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary mapping ModelID to Model_Category.\n",
    "    \"\"\"\n",
    "    # Compute average SalePrice per fiModelDesc in training data\n",
    "    model_avg_price = training_df.groupby(\"fiModelDesc\")[\"SalePrice\"].mean()\n",
    "    \n",
    "    # Define price categories\n",
    "    def categorize_price(price):\n",
    "        if price < 20000:\n",
    "            return \"Low Price Models\"\n",
    "        elif 20000 <= price < 60000:\n",
    "            return \"Mid Price Models\"\n",
    "        else:\n",
    "            return \"High Price Models\"\n",
    "    \n",
    "    # Map fiModelDesc to categories\n",
    "    model_category_mapping = model_avg_price.apply(categorize_price).to_dict()\n",
    "    \n",
    "    # Create ModelID to category mapping using fiModelDesc\n",
    "    training_df[\"Model_Category\"] = training_df[\"fiModelDesc\"].map(model_category_mapping)\n",
    "    modelid_to_category = training_df.set_index(\"ModelID\")[\"Model_Category\"].to_dict()\n",
    "    \n",
    "    return modelid_to_category, model_category_mapping\n",
    "\n",
    "def categorize_model_id(df, modelid_to_category, model_category_mapping):\n",
    "    \"\"\"\n",
    "    Categorizes ModelID based on precomputed price categories.\n",
    "    If ModelID is not found, falls back to fiModelDesc categorization.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing ModelID and fiModelDesc columns.\n",
    "    modelid_to_category (dict): Mapping of ModelID to Model_Category.\n",
    "    model_category_mapping (dict): Mapping of fiModelDesc to Model_Category for fallback.\n",
    "    \n",
    "    Returns:\n",
    "    pd.Series: A Series with categorized model price labels.\n",
    "    \"\"\"\n",
    "    df[\"Predicted_Model_Category\"] = df[\"ModelID\"].map(modelid_to_category)\n",
    "    \n",
    "    # Handle missing ModelID by checking fiModelDesc mapping\n",
    "    missing_mask = df[\"Predicted_Model_Category\"].isna()\n",
    "    df.loc[missing_mask, \"Predicted_Model_Category\"] = df.loc[missing_mask, \"fiModelDesc\"].map(model_category_mapping)\n",
    "    \n",
    "    # Handle any remaining missing values by assigning 'Unknown'\n",
    "    df[\"Predicted_Model_Category\"].fillna(\"Unknown\", inplace=True)\n",
    "    \n",
    "    return df[\"Predicted_Model_Category\"]\n",
    "\n",
    "\n",
    "\n",
    "def extract_horsepower(df):\n",
    "    \"\"\"\n",
    "    Extracts and imputes missing Horsepower values from fiProductClassDesc.\n",
    "    Optimized for performance using vectorized operations.\n",
    "    \"\"\"\n",
    "    def extract_numeric_range(value):\n",
    "        \"\"\"Extracts the average horsepower value from a range like '100 to 120 Horsepower'.\"\"\"\n",
    "        if pd.isna(value) or not isinstance(value, str):\n",
    "            return np.nan\n",
    "        numbers = re.findall(r'[\\d\\.]+', value)\n",
    "        if len(numbers) == 2:\n",
    "            return (float(numbers[0]) + float(numbers[1])) / 2\n",
    "        elif len(numbers) == 1:\n",
    "            return float(numbers[0])\n",
    "        return np.nan\n",
    "    \n",
    "    # Identify rows with Horsepower in fiProductClassDesc\n",
    "    mask_horsepower = df['fiProductClassDesc'].str.contains(\"Horsepower\", na=False)\n",
    "    \n",
    "    # Extract horsepower values\n",
    "    df.loc[mask_horsepower, 'Horsepower_Unit_Type'] = 'Horsepower'\n",
    "    df.loc[mask_horsepower, 'Extracted_Horsepower'] = df.loc[mask_horsepower, 'fiProductClassDesc'].apply(extract_numeric_range)\n",
    "    \n",
    "    # Handle 'Variable' and 'No' values in Engine_Horsepower\n",
    "    df.loc[df['Engine_Horsepower'].isin(['Variable', 'No']), 'Engine_Horsepower'] = np.nan\n",
    "    \n",
    "    # Impute missing Engine_Horsepower values where Extracted_Horsepower is available\n",
    "    df['Engine_Horsepower_Imputed'] = df['Engine_Horsepower'].combine_first(df['Extracted_Horsepower'])\n",
    "    \n",
    "    # Fill remaining NaN values with median horsepower\n",
    "    #df['Engine_Horsepower_Imputed'].fillna(df['Engine_Horsepower_Imputed'].median(), inplace=True)\n",
    "    df.drop(columns='Engine_Horsepower',inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def preprocess_product_size(df):\n",
    "    \"\"\"\n",
    "    Function to preprocess and impute missing ProductSize values based on fiProductClassDesc.\n",
    "    \n",
    "    Steps:\n",
    "    1. Extract Product Type and Metric Tons / Horsepower from fiProductClassDesc.\n",
    "    2. Identify Unit Type (Metric Tons, Horsepower, or Lb Operating Capacity).\n",
    "    3. Convert Metric Tons / Horsepower to numerical values.\n",
    "    4. Use Metric Tons to impute missing ProductSize values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Extract Product Type and Size Descriptor\n",
    "    def split_product_desc(desc):\n",
    "        if pd.isna(desc):\n",
    "            return pd.NA, pd.NA\n",
    "        match = re.search(r'(.+?)-\\s*([\\d\\.]+.*)', desc)\n",
    "        if match:\n",
    "            return match.group(1).strip(), match.group(2).strip()\n",
    "        return desc.strip(), pd.NA  # If no match, return full description as product\n",
    "    \n",
    "    df[['Product_Type', 'Metric_Tons_HP']] = df['fiProductClassDesc'].apply(lambda x: pd.Series(split_product_desc(x)))\n",
    "    \n",
    "    # Step 2: Identify Unit Type\n",
    "    def extract_unit_type(desc):\n",
    "        if pd.isna(desc):\n",
    "            return pd.NA\n",
    "        if \"Metric Tons\" in desc:\n",
    "            return \"Metric Tons\"\n",
    "        elif \"Horsepower\" in desc:\n",
    "            return \"Horsepower\"\n",
    "        elif \"Lb Operating Capacity\" in desc:\n",
    "            return \"Lb Operating Capacity\"\n",
    "        return pd.NA\n",
    "    \n",
    "    df[\"Unit_Type\"] = df[\"Metric_Tons_HP\"].apply(extract_unit_type)\n",
    "    \n",
    "    # Step 3: Convert Metric Tons to numerical values\n",
    "    def extract_numeric_range(value):\n",
    "        \"\"\"Extracts the average value from a range like '12.0 to 14.0 Metric Tons'.\"\"\"\n",
    "        if pd.isna(value) or not isinstance(value, str):\n",
    "            return np.nan\n",
    "        numbers = re.findall(r'[\\d\\.]+', value)\n",
    "        if len(numbers) == 2:  # If range exists, take the average\n",
    "            return (float(numbers[0]) + float(numbers[1])) / 2\n",
    "        elif len(numbers) == 1:  # If only one number exists, use it\n",
    "            return float(numbers[0])\n",
    "        return np.nan\n",
    "    \n",
    "    df['Metric_Tons_Value'] = df['Metric_Tons_HP'].apply(lambda x: extract_numeric_range(x) if isinstance(x, str) else np.nan)\n",
    "    \n",
    "    # Step 4: Impute missing ProductSize values using Metric Tons\n",
    "    def impute_product_size(row):\n",
    "        if pd.isna(row['ProductSize']) and not pd.isna(row['Metric_Tons_Value']) and (row['Metric_Tons_Value']=='Metric Tons'):\n",
    "            if row['Metric_Tons_Value']  <= 5:\n",
    "                return 'Mini'\n",
    "            elif 5 < row['Metric_Tons_Value'] <= 75:\n",
    "                return 'Compact'\n",
    "            elif 20 < row['Metric_Tons_Value'] <= 50:\n",
    "                return 'Large / Medium'\n",
    "            elif 75 < row['Metric_Tons_Value'] <= 200:\n",
    "                return 'Medium'\n",
    "            elif row['Metric_Tons_Value'] > 200:\n",
    "                return 'Large'\n",
    "        return row['ProductSize']\n",
    "    \n",
    "    df['ProductSize_Imputed'] = df.apply(impute_product_size, axis=1)\n",
    "    df.drop(columns=['ProductSize','Metric_Tons_Value','Metric_Tons_HP','Unit_Type'],inplace=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    return df\n",
    "def Enclosure_fun(df):\n",
    "    list1=['EROPS','OROPS','EROPS AC']\n",
    "    list2=['NO ROPS','None or Unspecified']\n",
    "    df['Enclosure_cat']=0\n",
    "    df.loc[df['Enclosure'].isin(list1), 'Enclosure_cat'] = 1\n",
    "    df.loc[df['Enclosure'].isin(list2), 'Enclosure_cat'] = 0\n",
    "    df.loc[df['Enclosure']=='EROPS w AC','Enclosure_cat']=2\n",
    "    df.drop(columns='Enclosure',inplace=True)\n",
    "    return df\n",
    "# Define categories based on observed price trends\n",
    "def categorize_hydraulics(value):\n",
    "    if value in [\"Missing\", \"Auxiliary\", \"Standard\"]:\n",
    "        return \"Basic\"\n",
    "    elif value in [\"2 Valve\", \"None or Unspecified\", \"3 Valve\"]:\n",
    "        return \"Mid-Level\"\n",
    "    else:\n",
    "        return \"Advanced\"\n",
    "\n",
    "\n",
    "\n",
    "# Reload the dataset\n",
    "ProductSize_path=r'C:\\Users\\eitanb\\Documents\\DS\\ML\\ML_project\\DATA/ProductSize_Null_fix_model.pkl'\n",
    "file_path_train = r\"C:\\Users\\eitanb\\Documents\\DS\\ML\\ML_project\\DATA/Train.csv\"\n",
    "df = pd.read_csv(file_path_train)\n",
    "#df=df[['SalesID','saledate',\"Enclosure\",\"YearMade\",\"ModelID\",\"MachineID\",\"Ripper\",\"ProductGroup\",\"ProductSize\",\"ProductGroupDesc\",\"Tire_Size\",\"MachineHoursCurrentMeter\",\"Blade_Type\",\"ProductGroupDesc\",\"Travel_Controls\",\"auctioneerID\",\"datasource\",\"Drive_System\",\"Pushblock\",\"Tire_Size\",\"Ride_Control\",\"UsageBand\",\"Coupler\",\"Engine_Horsepower_Imputed\"]].copy()\n",
    "\n",
    "# Convert 'saledate' to datetime and extract year, month, and day\n",
    "df['saledate'] = pd.to_datetime(df['saledate'], errors='coerce')\n",
    "df['sale_year'] = df['saledate'].dt.year\n",
    "df['sale_month'] = df['saledate'].dt.month\n",
    "df['sale_day'] = df['saledate'].dt.day\n",
    "\n",
    "# Apply categorization\n",
    "df[\"Hydraulics_Category\"] = df[\"Hydraulics\"].apply(categorize_hydraulics)\n",
    "\n",
    "df=Enclosure_fun(df)\n",
    "df=extract_horsepower(df)\n",
    "\n",
    "# Create mappings from training data only in saleprice \n",
    "modelid_to_category, model_category_mapping = create_model_category_mapping(df) # need to upload the modelid_to_category, model_category_mapping in the test\n",
    "\n",
    "# Apply categorization to test data\n",
    "df[\"Predicted_Model_Category\"] = categorize_model_id(df, modelid_to_category, model_category_mapping) # need to upload the modelid_to_category, model_category_mapping in the test\n",
    "\n",
    "# Compute the average price for each ProductGroup\n",
    "\n",
    "product_group_avg_price = df.groupby(\"ProductGroup\")[\"SalePrice\"].mean()\n",
    "\n",
    "# Map each ProductGroup to a price category\n",
    "df[\"ProductGroup_Category\"] = df[\"ProductGroup\"].map(product_group_avg_price).apply(categorize_product_group)# need to upload the categorize_product_group in the test\n",
    "df=preprocess_product_size(df)\n",
    "\n",
    "# Define function to update 'YearMade' and create 'YearMade_Bucket'\n",
    "def update_YearMade(df):\n",
    "    print(\"update_YearMade START\")\n",
    "    \n",
    "    # Compute median YearMade for each ModelID\n",
    "    model_medians = df.loc[df['YearMade'] > 1000].groupby('ModelID')['YearMade'].median()\n",
    "    \n",
    "    # Update YearMade where it is 1000 using ModelID median\n",
    "    df.loc[df['YearMade'] == 1000, 'YearMade'] = df['ModelID'].map(model_medians)\n",
    "    \n",
    "    # Compute overall median YearMade excluding 1000 values\n",
    "    overall_median = df.loc[df['YearMade'] > 1000, 'YearMade'].median()\n",
    "    \n",
    "    # Replace any remaining 1000 values with overall median\n",
    "    df['YearMade'].fillna(overall_median, inplace=True)\n",
    "        \n",
    "    # Bucketize YearMade\n",
    "    bins = [0, 1980, 1995, 2005, 2010, 2025]\n",
    "    labels = [\"Before 1980\", \"1980-1995\", \"1996-2005\", \"2006-2010\", \"2011-2025\"]\n",
    "    df['YearMade_Bucket'] = pd.cut(df['YearMade'], bins=bins, labels=labels, right=True)\n",
    "    \n",
    "    print(\"update_YearMade END\")\n",
    "    return df\n",
    "\n",
    "# Apply YearMade update\n",
    "df = update_YearMade(df)\n",
    "\n",
    "#df.drop(columns=['fiProductClassDesc','Hydraulics','fiSecondaryDesc','auctioneerID','fiModelDesc','fiBaseModel','ProductGroupDesc','MachineID','ProductGroup'],inplace=True)\n",
    "\n",
    "\n",
    "# Filter dataset to only include the last 5 years\n",
    "recent_years = df['sale_year'].dropna().unique()\n",
    "recent_years.sort()\n",
    "selected_years = recent_years[-5:]\n",
    "df_filtered = df[df['sale_year'].isin(selected_years)]\n",
    "\n",
    "# Drop irrelevant columns: 'SalesID', 'saledate'\n",
    "high_cardinality_cols = [col for col in df_filtered.select_dtypes(include=['object']).columns if df_filtered[col].nunique() > 50]\n",
    "df_filtered = df_filtered.drop(columns=['SalesID', 'saledate'] + high_cardinality_cols, errors='ignore')\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = df_filtered.select_dtypes(include=['object']).columns\n",
    "\n",
    "df_filtered[\"YearMade_Bucket\"] = df_filtered[\"YearMade_Bucket\"].astype(str).fillna(\"Unknown\")\n",
    "\n",
    "# Define category order and apply Ordinal Encoding\n",
    "year_bucket_encoder = OrdinalEncoder(\n",
    "    categories=[[\"Before 1980\", \"1980-1995\", \"1996-2005\", \"2006-2010\", \"2011-2025\", \"Unknown\"]],\n",
    "    handle_unknown=\"use_encoded_value\",\n",
    "    unknown_value=-1\n",
    ")\n",
    "\n",
    "df_filtered[\"YearMade_Bucket\"] = year_bucket_encoder.fit_transform(df_filtered[[\"YearMade_Bucket\"]])\n",
    "# Apply Ordinal Encoding to 'YearMade_Bucket'\n",
    "#year_bucket_encoder = OrdinalEncoder(categories=[[\"Before 1980\", \"1980-1995\", \"1996-2005\", \"2006-2010\", \"2011-2025\"]])\n",
    "#df_filtered[\"YearMade_Bucket\"] = year_bucket_encoder.fit_transform(df_filtered[[\"YearMade_Bucket\"]])\n",
    "\n",
    "# Identify high, moderate, and low cardinality categorical columns\n",
    "high_cardinality_cols = [col for col in categorical_cols if df_filtered[col].nunique() >= 50]\n",
    "moderate_cardinality_cols = [col for col in categorical_cols if 10 <= df_filtered[col].nunique() < 50]\n",
    "low_cardinality_cols = [col for col in categorical_cols if df_filtered[col].nunique() < 10]\n",
    "\n",
    "# Fill missing categorical values with \"Unknown\"\n",
    "for col in categorical_cols:\n",
    "    df_filtered[col] = df_filtered[col].fillna(\"Unknown\")\n",
    "\n",
    "# Encoding strategy\n",
    "\n",
    "# Apply One-Hot Encoding (OHE) to low-cardinality categories\n",
    "df_encoded = pd.get_dummies(df_filtered, columns=low_cardinality_cols, drop_first=True)\n",
    "\n",
    "# Apply Ordinal Encoding to moderate-cardinality categories\n",
    "if moderate_cardinality_cols:\n",
    "    ord_encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "    df_encoded[moderate_cardinality_cols] = ord_encoder.fit_transform(df_encoded[moderate_cardinality_cols])\n",
    "\n",
    "# Apply Frequency Encoding to high-cardinality categories\n",
    "for col in high_cardinality_cols:\n",
    "    freq_encoding = df_encoded[col].value_counts().to_dict()\n",
    "    df_encoded[col] = df_encoded[col].map(freq_encoding)\n",
    "\n",
    "# Drop rows with missing SalePrice (target variable)\n",
    "df_encoded = df_encoded.dropna(subset=['SalePrice'])\n",
    "\n",
    "# Split into features and target\n",
    "X = df_encoded.drop(columns=['SalePrice'])\n",
    "y = df_encoded['SalePrice']\n",
    "\n",
    "# Split data into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "y_pred = rf_model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mse ** 0.5\n",
    "\n",
    "# Display model performance\n",
    "mae, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12c44bbb-65e0-4103-b6de-ce512290b960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feature_1_Enclosure_cat_importance': 0.2892227476528825}\n",
      "{'feature_2_Model_Category_Mid Price Models_importance': 0.09991882770386451}\n",
      "{'feature_3_Model_Category_Low Price Models_importance': 0.09986489428263046}\n",
      "{'feature_4_Predicted_Model_Category_Low Price Models_importance': 0.09774432475715128}\n",
      "{'feature_5_Predicted_Model_Category_Mid Price Models_importance': 0.09288049208433057}\n",
      "{'feature_6_YearMade_importance': 0.061722771285000866}\n",
      "{'feature_7_ModelID_importance': 0.03995959510981458}\n",
      "{'feature_8_sale_year_importance': 0.03188214953969669}\n",
      "{'feature_9_MachineID_importance': 0.020630266840347355}\n",
      "{'feature_10_Engine_Horsepower_Imputed_importance': 0.0160786865685698}\n",
      "{'feature_11_sale_month_importance': 0.016056225844659454}\n",
      "{'feature_12_sale_day_importance': 0.014334765789286658}\n",
      "{'feature_13_Extracted_Horsepower_importance': 0.014030403423741022}\n",
      "{'feature_14_MachineHoursCurrentMeter_importance': 0.011001714799535735}\n",
      "{'feature_15_ProductSize_Imputed_Large / Medium_importance': 0.00860141584575969}\n",
      "{'feature_16_auctioneerID_importance': 0.007160831431432565}\n",
      "{'feature_17_ProductSize_Imputed_Mini_importance': 0.005392726543444144}\n",
      "{'feature_18_YearMade_Bucket_importance': 0.005390659124486528}\n",
      "{'feature_19_Product_Type_importance': 0.004934998041066809}\n",
      "{'feature_20_ProductGroup_Category_Mid Price Group_importance': 0.0035038140585102696}\n",
      "{'feature_21_ProductGroup_MG_importance': 0.0032097552178663147}\n",
      "{'feature_22_Blade_Type_importance': 0.0031917366118968497}\n",
      "{'feature_23_Drive_System_No_importance': 0.00304006825453793}\n",
      "{'feature_24_Ripper_Yes_importance': 0.0027945580697215145}\n",
      "{'feature_25_ProductGroupDesc_Motor Graders_importance': 0.0026627244268433497}\n",
      "{'feature_26_ProductSize_Imputed_Small_importance': 0.0026625268287565965}\n",
      "{'feature_27_datasource_importance': 0.0025559903507275878}\n",
      "{'feature_28_Stick_Length_importance': 0.0022945313146581045}\n",
      "{'feature_29_Hydraulics_importance': 0.002219562333449401}\n",
      "{'feature_30_Ripper_None or Unspecified_importance': 0.0019181736417435414}\n",
      "{'feature_31_Pad_Type_None or Unspecified_importance': 0.0018655017747092196}\n",
      "{'feature_32_Forks_Unknown_importance': 0.001828028728257171}\n",
      "{'feature_33_UsageBand_Low_importance': 0.001754091850300064}\n",
      "{'feature_34_Travel_Controls_Differential Steer_importance': 0.0017019512733351869}\n",
      "{'feature_35_Tire_Size_importance': 0.0016195893333956033}\n",
      "{'feature_36_Drive_System_Unknown_importance': 0.001196572052884726}\n",
      "{'feature_37_UsageBand_Medium_importance': 0.001192929517076555}\n",
      "{'feature_38_Coupler_None or Unspecified_importance': 0.0010441964725355602}\n",
      "{'feature_39_Undercarriage_Pad_Width_importance': 0.0009823518001580503}\n",
      "{'feature_40_Hydraulics_Flow_Standard_importance': 0.0009020357184670129}\n",
      "{'feature_41_Thumb_None or Unspecified_importance': 0.000894618821121079}\n",
      "{'feature_42_Coupler_Manual_importance': 0.0008075740411910765}\n",
      "{'feature_43_UsageBand_Unknown_importance': 0.0006908018419122364}\n",
      "{'feature_44_Ride_Control_None or Unspecified_importance': 0.0006697223943494738}\n",
      "{'feature_45_Ride_Control_Yes_importance': 0.0006609989501902446}\n",
      "{'feature_46_Transmission_Standard_importance': 0.000632406030438499}\n",
      "{'feature_47_Ripper_Single Shank_importance': 0.0005806360720780142}\n",
      "{'feature_48_Grouser_Type_Triple_importance': 0.0005745115930933274}\n",
      "{'feature_49_Forks_Yes_importance': 0.0005429620495722463}\n",
      "{'feature_50_Travel_Controls_None or Unspecified_importance': 0.0005285313410022125}\n",
      "{'feature_51_ProductSize_Imputed_Large_importance': 0.0004972384937649694}\n",
      "{'feature_52_Blade_Extension_Unknown_importance': 0.0004816081937482736}\n",
      "{'feature_53_ProductSize_Imputed_Medium_importance': 0.0004808630914215052}\n",
      "{'feature_54_Enclosure_Type_Unknown_importance': 0.0004691735741903364}\n",
      "{'feature_55_Thumb_Manual_importance': 0.0004678523217356547}\n",
      "{'feature_56_Tip_Control_Unknown_importance': 0.00044904655820312677}\n",
      "{'feature_57_ProductSize_Imputed_Unknown_importance': 0.00040380580284024467}\n",
      "{'feature_58_Coupler_Unknown_importance': 0.00038734212340370695}\n",
      "{'feature_59_Scarifier_Yes_importance': 0.0003859251026560714}\n",
      "{'feature_60_Pattern_Changer_None or Unspecified_importance': 0.00037788472874837744}\n",
      "{'feature_61_Blade_Width_Unknown_importance': 0.0003681424004878798}\n",
      "{'feature_62_Track_Type_Steel_importance': 0.00034951099734362157}\n",
      "{'feature_63_Scarifier_Unknown_importance': 0.0003455364596422887}\n",
      "{'feature_64_Pushblock_Yes_importance': 0.0003215342565457805}\n",
      "{'feature_65_Transmission_Unknown_importance': 0.00031876871307594327}\n",
      "{'feature_66_Pattern_Changer_Yes_importance': 0.00030751981873146513}\n",
      "{'feature_67_Transmission_Powershift_importance': 0.0003057977633171881}\n",
      "{'feature_68_Differential_Type_Standard_importance': 0.00025715233640954103}\n",
      "{'feature_69_Blade_Width_None or Unspecified_importance': 0.0002483311017024987}\n",
      "{'feature_70_Enclosure_Type_Low Profile_importance': 0.00024215011043904016}\n",
      "{'feature_71_Transmission_None or Unspecified_importance': 0.00024150624609673194}\n",
      "{'feature_72_Ripper_Unknown_importance': 0.00023777905558079756}\n",
      "{'feature_73_Steering_Controls_Conventional_importance': 0.0002354274636625863}\n",
      "{'feature_74_Drive_System_Two Wheel Drive_importance': 0.00022903956472212114}\n",
      "{'feature_75_Pushblock_Unknown_importance': 0.00022623445679042964}\n",
      "{'feature_76_Tip_Control_Sideshift & Tip_importance': 0.00022551380834000452}\n",
      "{'feature_77_Hydraulics_Category_Mid-Level_importance': 0.00022323795345731269}\n",
      "{'feature_78_Drive_System_Four Wheel Drive_importance': 0.0002212337183634148}\n",
      "{\"feature_79_Blade_Width_16'_importance\": 0.00021863249950897722}\n",
      "{'feature_80_Travel_Controls_Finger Tip_importance': 0.0002030984963813508}\n",
      "{'feature_81_Stick_Standard_importance': 0.00018774494316214989}\n",
      "{\"feature_82_Blade_Width_14'_importance\": 0.00017375295267564196}\n",
      "{'feature_83_ProductGroupDesc_Track Type Tractors_importance': 0.0001730778079128687}\n",
      "{'feature_84_ProductGroup_TTT_importance': 0.0001699598486759163}\n",
      "{'feature_85_Backhoe_Mounting_Unknown_importance': 0.00016752234686448969}\n",
      "{'feature_86_Enclosure_Type_None or Unspecified_importance': 0.00016127884306685908}\n",
      "{'feature_87_Ride_Control_Unknown_importance': 0.00012801254838804588}\n",
      "{'feature_88_Travel_Controls_Unknown_importance': 0.00012575060529481136}\n",
      "{'feature_89_Transmission_Hydrostatic_importance': 0.0001239241157092547}\n",
      "{'feature_90_Tip_Control_Tip_importance': 0.00010428308798931727}\n",
      "{'feature_91_Hydraulics_Category_Basic_importance': 9.934511969760908e-05}\n",
      "{'feature_92_Travel_Controls_Lever_importance': 9.170878702569117e-05}\n",
      "{'feature_93_ProductGroupDesc_Wheel Loader_importance': 8.964294947389068e-05}\n",
      "{'feature_94_ProductGroup_WL_importance': 7.949308512246884e-05}\n",
      "{'feature_95_Steering_Controls_Unknown_importance': 7.823195435390985e-05}\n",
      "{'feature_96_Differential_Type_Unknown_importance': 6.5922146276173e-05}\n",
      "{'feature_97_Pattern_Changer_Unknown_importance': 4.791949393283583e-05}\n",
      "{'feature_98_ProductGroupDesc_Skid Steer Loaders_importance': 4.553013167114179e-05}\n",
      "{'feature_99_Track_Type_Unknown_importance': 4.5187636166432975e-05}\n",
      "{'feature_100_Thumb_Unknown_importance': 4.4716138679960754e-05}\n",
      "{'feature_101_Grouser_Type_Unknown_importance': 4.3896993754990604e-05}\n",
      "{'feature_102_Horsepower_Unit_Type_Unknown_importance': 4.236988182636274e-05}\n",
      "{'feature_103_Blade_Extension_Yes_importance': 4.191598872426797e-05}\n",
      "{'feature_104_ProductGroup_TEX_importance': 4.042642345802711e-05}\n",
      "{'feature_105_Transmission_Powershuttle_importance': 3.52551840396505e-05}\n",
      "{'feature_106_Coupler_System_Unknown_importance': 3.240661997365026e-05}\n",
      "{'feature_107_Turbocharged_Yes_importance': 3.16718324095009e-05}\n",
      "{'feature_108_ProductGroupDesc_Track Excavators_importance': 2.921422262647881e-05}\n",
      "{'feature_109_ProductGroup_Category_Low Price Group_importance': 2.744395904326957e-05}\n",
      "{'feature_110_Pad_Type_Reversible_importance': 2.340529218940555e-05}\n",
      "{'feature_111_Grouser_Tracks_Yes_importance': 2.252811994050512e-05}\n",
      "{'feature_112_ProductGroup_SSL_importance': 2.0791032059303998e-05}\n",
      "{\"feature_113_Blade_Width_13'_importance\": 1.9836266437806525e-05}\n",
      "{'feature_114_Pad_Type_Street_importance': 1.790332638757811e-05}\n",
      "{'feature_115_Stick_Unknown_importance': 1.6807049276233708e-05}\n",
      "{'feature_116_Pad_Type_Unknown_importance': 1.5153879099734164e-05}\n",
      "{'feature_117_Travel_Controls_2 Pedal_importance': 1.45275268779449e-05}\n",
      "{'feature_118_Turbocharged_Unknown_importance': 1.2102830400702408e-05}\n",
      "{'feature_119_Differential_Type_No Spin_importance': 1.185612987336614e-05}\n",
      "{'feature_120_Transmission_Direct Drive_importance': 8.289630818583423e-06}\n",
      "{'feature_121_Coupler_System_Yes_importance': 7.797886568168856e-06}\n",
      "{'feature_122_Hydraulics_Flow_Unknown_importance': 7.270574659588676e-06}\n",
      "{'feature_123_Travel_Controls_Pedal_importance': 5.520805924907519e-06}\n",
      "{'feature_124_Grouser_Tracks_Unknown_importance': 4.376348510578516e-06}\n",
      "{'feature_125_Steering_Controls_Four Wheel Standard_importance': 1.976675366077409e-06}\n",
      "{'feature_126_Transmission_Autoshift_importance': 1.2504450075801583e-06}\n",
      "{\"feature_127_Blade_Width_<12'_importance\": 1.2070963660621803e-06}\n",
      "{'feature_128_Steering_Controls_Wheel_importance': 6.252200215009473e-07}\n",
      "{'feature_129_Backhoe_Mounting_Yes_importance': 2.3805710529688087e-07}\n",
      "{'feature_130_Hydraulics_Flow_None or Unspecified_importance': 7.552527238215308e-08}\n",
      "{'feature_131_Differential_Type_Locking_importance': 1.7990049457476377e-08}\n"
     ]
    }
   ],
   "source": [
    "# Optional: Log feature importances\n",
    "importances = rf_model.feature_importances_\n",
    "\n",
    "# Create a zip of feature names and feature importances\n",
    "feature_zip = zip(rf_model.feature_names_in_, rf_model.feature_importances_)\n",
    "\n",
    "# Sort the zip by feature importance in descending order\n",
    "sorted_feature_zip = sorted(feature_zip, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature names, importances, and index\n",
    "for idx, (feat, importance) in enumerate(sorted_feature_zip, 1):\n",
    "  print({f\"feature_{idx}_{feat}_importance\" : importance})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f02c851-0bf5-49f0-abe9-bc270d74a73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_category_mapping(training_df):\n",
    "    \"\"\"\n",
    "    Creates a mapping of ModelID to Model_Category based on fiModelDesc and SalePrice in training data.\n",
    "    \n",
    "    Parameters:\n",
    "    training_df (pd.DataFrame): Training dataset containing fiModelDesc and SalePrice.\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary mapping ModelID to Model_Category.\n",
    "    \"\"\"\n",
    "    # Compute average SalePrice per fiModelDesc in training data\n",
    "    model_avg_price = training_df.groupby(\"fiModelDesc\")[\"SalePrice\"].mean()\n",
    "    \n",
    "    # Define price categories\n",
    "    def categorize_price(price):\n",
    "        if price < 20000:\n",
    "            return \"Low Price Models\"\n",
    "        elif 20000 <= price < 60000:\n",
    "            return \"Mid Price Models\"\n",
    "        else:\n",
    "            return \"High Price Models\"\n",
    "    \n",
    "    # Map fiModelDesc to categories\n",
    "    model_category_mapping = model_avg_price.apply(categorize_price).to_dict()\n",
    "    \n",
    "    # Create ModelID to category mapping using fiModelDesc\n",
    "    training_df[\"Model_Category\"] = training_df[\"fiModelDesc\"].map(model_category_mapping)\n",
    "    modelid_to_category = training_df.set_index(\"ModelID\")[\"Model_Category\"].to_dict()\n",
    "    \n",
    "    return modelid_to_category, model_category_mapping\n",
    "\n",
    "def categorize_model_id(df, modelid_to_category, model_category_mapping):\n",
    "    \"\"\"\n",
    "    Categorizes ModelID based on precomputed price categories.\n",
    "    If ModelID is not found, falls back to fiModelDesc categorization.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing ModelID and fiModelDesc columns.\n",
    "    modelid_to_category (dict): Mapping of ModelID to Model_Category.\n",
    "    model_category_mapping (dict): Mapping of fiModelDesc to Model_Category for fallback.\n",
    "    \n",
    "    Returns:\n",
    "    pd.Series: A Series with categorized model price labels.\n",
    "    \"\"\"\n",
    "    df[\"Predicted_Model_Category\"] = df[\"ModelID\"].map(modelid_to_category)\n",
    "    \n",
    "    # Handle missing ModelID by checking fiModelDesc mapping\n",
    "    missing_mask = df[\"Predicted_Model_Category\"].isna()\n",
    "    df.loc[missing_mask, \"Predicted_Model_Category\"] = df.loc[missing_mask, \"fiModelDesc\"].map(model_category_mapping)\n",
    "    \n",
    "    # Handle any remaining missing values by assigning 'Unknown'\n",
    "    df[\"Predicted_Model_Category\"].fillna(\"Unknown\", inplace=True)\n",
    "    \n",
    "    return df[\"Predicted_Model_Category\"]\n",
    "\n",
    "# Example usage:\n",
    "# modelid_to_category, model_category_mapping = create_model_category_mapping(training_df)\n",
    "# test_df[\"Predicted_Model_Category\"] = categorize_model_id(test_df, modelid_to_category, model_category_mapping)\n",
    "# Create mappings from training data\n",
    "modelid_to_category, model_category_mapping = create_model_category_mapping(df)\n",
    "\n",
    "# Apply categorization to test data\n",
    "df[\"Predicted_Model_Category\"] = categorize_model_id(df, modelid_to_category, model_category_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c984fff-1440-414a-930e-09b9047005f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define price-based categories for ProductGroup\n",
    "def categorize_product_group(price):\n",
    "    if price < 20000:\n",
    "        return \"Low Price Group\"\n",
    "    elif 20000 <= price < 40000:\n",
    "        return \"Mid Price Group\"\n",
    "    else:\n",
    "        return \"High Price Group\"\n",
    "\n",
    "# Compute the average price for each ProductGroup\n",
    "product_group_avg_price = df.groupby(\"ProductGroup\")[\"SalePrice\"].mean()\n",
    "\n",
    "# Map each ProductGroup to a price category\n",
    "df[\"ProductGroup_Category\"] = df[\"ProductGroup\"].map(product_group_avg_price).apply(categorize_product_group)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d2776a-0ede-4439-954f-383a0a2134ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define price-based categories for Steering_Controls\n",
    "def categorize_steering_controls(value):\n",
    "    if value in [\"Four Wheel Standard\"]:\n",
    "        return \"Low Price Steering\"\n",
    "    elif value in [\"Conventional\"]:\n",
    "        return \"Mid Price Steering\"\n",
    "    elif value in [\"Command Control\"]:\n",
    "        return \"High Price Steering\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "# Apply categorization\n",
    "df[\"Steering_Controls_Category\"] = df[\"Steering_Controls\"].apply(categorize_steering_controls)\n",
    "\n",
    "# Verify the distribution of new categories\n",
    "df[\"Steering_Controls_Category\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c3b1e1-a009-43af-acc1-ae29008ded72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define price-based categories for Transmission\n",
    "def categorize_transmission(value):\n",
    "    if value in [\"Powershuttle\", \"Standard\", \"Direct Drive\"]:\n",
    "        return \"Low Price Transmission\"\n",
    "    elif value in [\"Autoshift\", \"Hydrostatic\"]:\n",
    "        return \"Mid Price Transmission\"\n",
    "    elif value in [\"Powershift\", \"None or Unspecified\", \"AutoShift\"]:\n",
    "        return \"High Price Transmission\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "# Apply categorization\n",
    "df[\"Transmission_Category\"] = df[\"Transmission\"].apply(categorize_transmission)\n",
    "\n",
    "# Verify the distribution of new categories\n",
    "df[\"Transmission_Category\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f07177d0-f559-41ec-aa81-7955783744e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eitanb\\AppData\\Local\\Temp\\ipykernel_12692\\35781373.py:236: DtypeWarning: Columns (13,39,40,41) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path_train)\n",
      "C:\\Users\\eitanb\\AppData\\Local\\Temp\\ipykernel_12692\\35781373.py:134: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  df['Engine_Horsepower_Imputed'] = df['Engine_Horsepower'].combine_first(df['Extracted_Horsepower'])\n",
      "C:\\Users\\eitanb\\AppData\\Local\\Temp\\ipykernel_12692\\35781373.py:101: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"Predicted_Model_Category\"].fillna(\"Unknown\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update_YearMade START\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eitanb\\AppData\\Local\\Temp\\ipykernel_12692\\35781373.py:279: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[1980. 1980. 1994. ... 2003. 2004. 2004.]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[df['YearMade'] == 1000, 'YearMade'] = df['ModelID'].map(model_medians)\n",
      "C:\\Users\\eitanb\\AppData\\Local\\Temp\\ipykernel_12692\\35781373.py:285: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['YearMade'].fillna(overall_median, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update_YearMade END\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4666.387783170206, 7416.27935846593)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-import necessary libraries due to execution reset\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Define price-based categories for Track_Type\n",
    "def categorize_track_type(value):\n",
    "    if value in [\"Rubber\"]:\n",
    "        return \"Low Price Track\"\n",
    "    elif value in [\"Steel\"]:\n",
    "        return \"High Price Track\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "\n",
    "# Define price-based categories for Transmission\n",
    "def categorize_transmission(value):\n",
    "    if value in [\"Powershuttle\", \"Standard\", \"Direct Drive\"]:\n",
    "        return \"Low Price Transmission\"\n",
    "    elif value in [\"Autoshift\", \"Hydrostatic\"]:\n",
    "        return \"Mid Price Transmission\"\n",
    "    elif value in [\"Powershift\", \"None or Unspecified\", \"AutoShift\"]:\n",
    "        return \"High Price Transmission\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "\n",
    "# Define price-based categories for Steering_Controls\n",
    "def categorize_steering_controls(value):\n",
    "    if value in [\"Four Wheel Standard\"]:\n",
    "        return \"Low Price Steering\"\n",
    "    elif value in [\"Conventional\"]:\n",
    "        return \"Mid Price Steering\"\n",
    "    elif value in [\"Command Control\"]:\n",
    "        return \"High Price Steering\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "\n",
    "\n",
    "# Define price-based categories for ProductGroup\n",
    "def categorize_product_group(price):\n",
    "    if price < 20000:\n",
    "        return \"Low Price Group\"\n",
    "    elif 20000 <= price < 40000:\n",
    "        return \"Mid Price Group\"\n",
    "    else:\n",
    "        return \"High Price Group\"\n",
    "\n",
    "\n",
    "def create_model_category_mapping(training_df):\n",
    "    \"\"\"\n",
    "    Creates a mapping of ModelID to Model_Category based on fiModelDesc and SalePrice in training data.\n",
    "    \n",
    "    Parameters:\n",
    "    training_df (pd.DataFrame): Training dataset containing fiModelDesc and SalePrice.\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary mapping ModelID to Model_Category.\n",
    "    \"\"\"\n",
    "    # Compute average SalePrice per fiModelDesc in training data\n",
    "    model_avg_price = training_df.groupby(\"fiModelDesc\")[\"SalePrice\"].mean()\n",
    "    \n",
    "    # Define price categories\n",
    "    def categorize_price(price):\n",
    "        if price < 20000:\n",
    "            return \"Low Price Models\"\n",
    "        elif 20000 <= price < 60000:\n",
    "            return \"Mid Price Models\"\n",
    "        else:\n",
    "            return \"High Price Models\"\n",
    "    \n",
    "    # Map fiModelDesc to categories\n",
    "    model_category_mapping = model_avg_price.apply(categorize_price).to_dict()\n",
    "    \n",
    "    # Create ModelID to category mapping using fiModelDesc\n",
    "    training_df[\"Model_Category\"] = training_df[\"fiModelDesc\"].map(model_category_mapping)\n",
    "    modelid_to_category = training_df.set_index(\"ModelID\")[\"Model_Category\"].to_dict()\n",
    "    \n",
    "    return modelid_to_category, model_category_mapping\n",
    "\n",
    "def categorize_model_id(df, modelid_to_category, model_category_mapping):\n",
    "    \"\"\"\n",
    "    Categorizes ModelID based on precomputed price categories.\n",
    "    If ModelID is not found, falls back to fiModelDesc categorization.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing ModelID and fiModelDesc columns.\n",
    "    modelid_to_category (dict): Mapping of ModelID to Model_Category.\n",
    "    model_category_mapping (dict): Mapping of fiModelDesc to Model_Category for fallback.\n",
    "    \n",
    "    Returns:\n",
    "    pd.Series: A Series with categorized model price labels.\n",
    "    \"\"\"\n",
    "    df[\"Predicted_Model_Category\"] = df[\"ModelID\"].map(modelid_to_category)\n",
    "    \n",
    "    # Handle missing ModelID by checking fiModelDesc mapping\n",
    "    missing_mask = df[\"Predicted_Model_Category\"].isna()\n",
    "    df.loc[missing_mask, \"Predicted_Model_Category\"] = df.loc[missing_mask, \"fiModelDesc\"].map(model_category_mapping)\n",
    "    \n",
    "    # Handle any remaining missing values by assigning 'Unknown'\n",
    "    df[\"Predicted_Model_Category\"].fillna(\"Unknown\", inplace=True)\n",
    "    \n",
    "    return df[\"Predicted_Model_Category\"]\n",
    "\n",
    "\n",
    "\n",
    "def extract_horsepower(df):\n",
    "    \"\"\"\n",
    "    Extracts and imputes missing Horsepower values from fiProductClassDesc.\n",
    "    Optimized for performance using vectorized operations.\n",
    "    \"\"\"\n",
    "    def extract_numeric_range(value):\n",
    "        \"\"\"Extracts the average horsepower value from a range like '100 to 120 Horsepower'.\"\"\"\n",
    "        if pd.isna(value) or not isinstance(value, str):\n",
    "            return np.nan\n",
    "        numbers = re.findall(r'[\\d\\.]+', value)\n",
    "        if len(numbers) == 2:\n",
    "            return (float(numbers[0]) + float(numbers[1])) / 2\n",
    "        elif len(numbers) == 1:\n",
    "            return float(numbers[0])\n",
    "        return np.nan\n",
    "    \n",
    "    # Identify rows with Horsepower in fiProductClassDesc\n",
    "    mask_horsepower = df['fiProductClassDesc'].str.contains(\"Horsepower\", na=False)\n",
    "    \n",
    "    # Extract horsepower values\n",
    "    df.loc[mask_horsepower, 'Horsepower_Unit_Type'] = 'Horsepower'\n",
    "    df.loc[mask_horsepower, 'Extracted_Horsepower'] = df.loc[mask_horsepower, 'fiProductClassDesc'].apply(extract_numeric_range)\n",
    "    \n",
    "    # Handle 'Variable' and 'No' values in Engine_Horsepower\n",
    "    df.loc[df['Engine_Horsepower'].isin(['Variable', 'No']), 'Engine_Horsepower'] = np.nan\n",
    "    \n",
    "    # Impute missing Engine_Horsepower values where Extracted_Horsepower is available\n",
    "    df['Engine_Horsepower_Imputed'] = df['Engine_Horsepower'].combine_first(df['Extracted_Horsepower'])\n",
    "    \n",
    "    # Fill remaining NaN values with median horsepower\n",
    "    #df['Engine_Horsepower_Imputed'].fillna(df['Engine_Horsepower_Imputed'].median(), inplace=True)\n",
    "    df.drop(columns='Engine_Horsepower',inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def preprocess_product_size(df):\n",
    "    \"\"\"\n",
    "    Function to preprocess and impute missing ProductSize values based on fiProductClassDesc.\n",
    "    \n",
    "    Steps:\n",
    "    1. Extract Product Type and Metric Tons / Horsepower from fiProductClassDesc.\n",
    "    2. Identify Unit Type (Metric Tons, Horsepower, or Lb Operating Capacity).\n",
    "    3. Convert Metric Tons / Horsepower to numerical values.\n",
    "    4. Use Metric Tons to impute missing ProductSize values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Extract Product Type and Size Descriptor\n",
    "    def split_product_desc(desc):\n",
    "        if pd.isna(desc):\n",
    "            return pd.NA, pd.NA\n",
    "        match = re.search(r'(.+?)-\\s*([\\d\\.]+.*)', desc)\n",
    "        if match:\n",
    "            return match.group(1).strip(), match.group(2).strip()\n",
    "        return desc.strip(), pd.NA  # If no match, return full description as product\n",
    "    \n",
    "    df[['Product_Type', 'Metric_Tons_HP']] = df['fiProductClassDesc'].apply(lambda x: pd.Series(split_product_desc(x)))\n",
    "    \n",
    "    # Step 2: Identify Unit Type\n",
    "    def extract_unit_type(desc):\n",
    "        if pd.isna(desc):\n",
    "            return pd.NA\n",
    "        if \"Metric Tons\" in desc:\n",
    "            return \"Metric Tons\"\n",
    "        elif \"Horsepower\" in desc:\n",
    "            return \"Horsepower\"\n",
    "        elif \"Lb Operating Capacity\" in desc:\n",
    "            return \"Lb Operating Capacity\"\n",
    "        return pd.NA\n",
    "    \n",
    "    df[\"Unit_Type\"] = df[\"Metric_Tons_HP\"].apply(extract_unit_type)\n",
    "    \n",
    "    # Step 3: Convert Metric Tons to numerical values\n",
    "    def extract_numeric_range(value):\n",
    "        \"\"\"Extracts the average value from a range like '12.0 to 14.0 Metric Tons'.\"\"\"\n",
    "        if pd.isna(value) or not isinstance(value, str):\n",
    "            return np.nan\n",
    "        numbers = re.findall(r'[\\d\\.]+', value)\n",
    "        if len(numbers) == 2:  # If range exists, take the average\n",
    "            return (float(numbers[0]) + float(numbers[1])) / 2\n",
    "        elif len(numbers) == 1:  # If only one number exists, use it\n",
    "            return float(numbers[0])\n",
    "        return np.nan\n",
    "    \n",
    "    df['Metric_Tons_Value'] = df['Metric_Tons_HP'].apply(lambda x: extract_numeric_range(x) if isinstance(x, str) else np.nan)\n",
    "    \n",
    "    # Step 4: Impute missing ProductSize values using Metric Tons\n",
    "    def impute_product_size(row):\n",
    "        if pd.isna(row['ProductSize']) and not pd.isna(row['Metric_Tons_Value']) and (row['Metric_Tons_Value']=='Metric Tons'):\n",
    "            if row['Metric_Tons_Value']  <= 5:\n",
    "                return 'Mini'\n",
    "            elif 5 < row['Metric_Tons_Value'] <= 75:\n",
    "                return 'Compact'\n",
    "            elif 20 < row['Metric_Tons_Value'] <= 50:\n",
    "                return 'Large / Medium'\n",
    "            elif 75 < row['Metric_Tons_Value'] <= 200:\n",
    "                return 'Medium'\n",
    "            elif row['Metric_Tons_Value'] > 200:\n",
    "                return 'Large'\n",
    "        return row['ProductSize']\n",
    "    \n",
    "    df['ProductSize_Imputed'] = df.apply(impute_product_size, axis=1)\n",
    "    df.drop(columns=['ProductSize','Metric_Tons_Value','Metric_Tons_HP','Unit_Type'],inplace=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    return df\n",
    "def Enclosure_fun(df):\n",
    "    list1=['EROPS','OROPS','EROPS AC']\n",
    "    list2=['NO ROPS','None or Unspecified']\n",
    "    df['Enclosure_cat']=0\n",
    "    df.loc[df['Enclosure'].isin(list1), 'Enclosure_cat'] = 1\n",
    "    df.loc[df['Enclosure'].isin(list2), 'Enclosure_cat'] = 0\n",
    "    df.loc[df['Enclosure']=='EROPS w AC','Enclosure_cat']=2\n",
    "    df.drop(columns='Enclosure',inplace=True)\n",
    "    return df\n",
    "# Define categories based on observed price trends\n",
    "def categorize_hydraulics(value):\n",
    "    if value in [\"Missing\", \"Auxiliary\", \"Standard\"]:\n",
    "        return \"Basic\"\n",
    "    elif value in [\"2 Valve\", \"None or Unspecified\", \"3 Valve\"]:\n",
    "        return \"Mid-Level\"\n",
    "    else:\n",
    "        return \"Advanced\"\n",
    "\n",
    "\n",
    "\n",
    "# Reload the dataset\n",
    "ProductSize_path=r'C:\\Users\\eitanb\\Documents\\DS\\ML\\ML_project\\DATA/ProductSize_Null_fix_model.pkl'\n",
    "file_path_train = r\"C:\\Users\\eitanb\\Documents\\DS\\ML\\ML_project\\DATA/Train.csv\"\n",
    "df = pd.read_csv(file_path_train)\n",
    "#df=df[['SalesID','saledate',\"Enclosure\",\"YearMade\",\"ModelID\",\"MachineID\",\"Ripper\",\"ProductGroup\",\"ProductSize\",\"ProductGroupDesc\",\"Tire_Size\",\"MachineHoursCurrentMeter\",\"Blade_Type\",\"ProductGroupDesc\",\"Travel_Controls\",\"auctioneerID\",\"datasource\",\"Drive_System\",\"Pushblock\",\"Tire_Size\",\"Ride_Control\",\"UsageBand\",\"Coupler\",\"Engine_Horsepower_Imputed\"]].copy()\n",
    "\n",
    "# Convert 'saledate' to datetime and extract year, month, and day\n",
    "df['saledate'] = pd.to_datetime(df['saledate'], errors='coerce')\n",
    "df['sale_year'] = df['saledate'].dt.year\n",
    "df['sale_month'] = df['saledate'].dt.month\n",
    "df['sale_day'] = df['saledate'].dt.day\n",
    "\n",
    "# Apply categorization\n",
    "df[\"Hydraulics_Category\"] = df[\"Hydraulics\"].apply(categorize_hydraulics)\n",
    "# Apply categorization\n",
    "df[\"Steering_Controls_Category\"] = df[\"Steering_Controls\"].apply(categorize_steering_controls)\n",
    "# Apply categorization\n",
    "df[\"Transmission_Category\"] = df[\"Transmission\"].apply(categorize_transmission)\n",
    "\n",
    "df=Enclosure_fun(df)\n",
    "df=extract_horsepower(df)\n",
    "\n",
    "# Create mappings from training data only in saleprice \n",
    "modelid_to_category, model_category_mapping = create_model_category_mapping(df) # need to upload the modelid_to_category, model_category_mapping in the test\n",
    "\n",
    "# Apply categorization to test data\n",
    "df[\"Predicted_Model_Category\"] = categorize_model_id(df, modelid_to_category, model_category_mapping) # need to upload the modelid_to_category, model_category_mapping in the test\n",
    "\n",
    "# Compute the average price for each ProductGroup\n",
    "# Apply categorization\n",
    "df[\"Track_Type_Category\"] = df[\"Track_Type\"].apply(categorize_track_type)\n",
    "\n",
    "product_group_avg_price = df.groupby(\"ProductGroup\")[\"SalePrice\"].mean()\n",
    "\n",
    "# Map each ProductGroup to a price category\n",
    "df[\"ProductGroup_Category\"] = df[\"ProductGroup\"].map(product_group_avg_price).apply(categorize_product_group)# need to upload the categorize_product_group in the test\n",
    "df=preprocess_product_size(df)\n",
    "\n",
    "# Define function to update 'YearMade' and create 'YearMade_Bucket'\n",
    "def update_YearMade(df):\n",
    "    print(\"update_YearMade START\")\n",
    "    \n",
    "    # Compute median YearMade for each ModelID\n",
    "    model_medians = df.loc[df['YearMade'] > 1000].groupby('ModelID')['YearMade'].median()\n",
    "    \n",
    "    # Update YearMade where it is 1000 using ModelID median\n",
    "    df.loc[df['YearMade'] == 1000, 'YearMade'] = df['ModelID'].map(model_medians)\n",
    "    \n",
    "    # Compute overall median YearMade excluding 1000 values\n",
    "    overall_median = df.loc[df['YearMade'] > 1000, 'YearMade'].median()\n",
    "    \n",
    "    # Replace any remaining 1000 values with overall median\n",
    "    df['YearMade'].fillna(overall_median, inplace=True)\n",
    "        \n",
    "    # Bucketize YearMade\n",
    "    bins = [0, 1980, 1995, 2005, 2010, 2025]\n",
    "    labels = [\"Before 1980\", \"1980-1995\", \"1996-2005\", \"2006-2010\", \"2011-2025\"]\n",
    "    df['YearMade_Bucket'] = pd.cut(df['YearMade'], bins=bins, labels=labels, right=True)\n",
    "    \n",
    "    print(\"update_YearMade END\")\n",
    "    return df\n",
    "\n",
    "# Apply YearMade update\n",
    "df = update_YearMade(df)\n",
    "\n",
    "#df.drop(columns=['fiProductClassDesc','Hydraulics','fiSecondaryDesc','auctioneerID','fiModelDesc','fiBaseModel','ProductGroupDesc','MachineID','ProductGroup'],inplace=True)\n",
    "\n",
    "\n",
    "# Filter dataset to only include the last 5 years\n",
    "recent_years = df['sale_year'].dropna().unique()\n",
    "recent_years.sort()\n",
    "selected_years = recent_years[-5:]\n",
    "df_filtered = df[df['sale_year'].isin(selected_years)]\n",
    "\n",
    "# Drop irrelevant columns: 'SalesID', 'saledate'\n",
    "high_cardinality_cols = [col for col in df_filtered.select_dtypes(include=['object']).columns if df_filtered[col].nunique() > 50]\n",
    "df_filtered = df_filtered.drop(columns=['SalesID', 'saledate'] + high_cardinality_cols, errors='ignore')\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = df_filtered.select_dtypes(include=['object']).columns\n",
    "\n",
    "df_filtered[\"YearMade_Bucket\"] = df_filtered[\"YearMade_Bucket\"].astype(str).fillna(\"Unknown\")\n",
    "\n",
    "# Define category order and apply Ordinal Encoding\n",
    "year_bucket_encoder = OrdinalEncoder(\n",
    "    categories=[[\"Before 1980\", \"1980-1995\", \"1996-2005\", \"2006-2010\", \"2011-2025\", \"Unknown\"]],\n",
    "    handle_unknown=\"use_encoded_value\",\n",
    "    unknown_value=-1\n",
    ")\n",
    "\n",
    "df_filtered[\"YearMade_Bucket\"] = year_bucket_encoder.fit_transform(df_filtered[[\"YearMade_Bucket\"]])\n",
    "# Apply Ordinal Encoding to 'YearMade_Bucket'\n",
    "#year_bucket_encoder = OrdinalEncoder(categories=[[\"Before 1980\", \"1980-1995\", \"1996-2005\", \"2006-2010\", \"2011-2025\"]])\n",
    "#df_filtered[\"YearMade_Bucket\"] = year_bucket_encoder.fit_transform(df_filtered[[\"YearMade_Bucket\"]])\n",
    "\n",
    "# Identify high, moderate, and low cardinality categorical columns\n",
    "high_cardinality_cols = [col for col in categorical_cols if df_filtered[col].nunique() >= 50]\n",
    "moderate_cardinality_cols = [col for col in categorical_cols if 10 <= df_filtered[col].nunique() < 50]\n",
    "low_cardinality_cols = [col for col in categorical_cols if df_filtered[col].nunique() < 10]\n",
    "\n",
    "# Fill missing categorical values with \"Unknown\"\n",
    "for col in categorical_cols:\n",
    "    df_filtered[col] = df_filtered[col].fillna(\"Unknown\")\n",
    "\n",
    "# Encoding strategy\n",
    "\n",
    "# Apply One-Hot Encoding (OHE) to low-cardinality categories\n",
    "df_encoded = pd.get_dummies(df_filtered, columns=low_cardinality_cols, drop_first=True)\n",
    "\n",
    "# Apply Ordinal Encoding to moderate-cardinality categories\n",
    "if moderate_cardinality_cols:\n",
    "    ord_encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "    df_encoded[moderate_cardinality_cols] = ord_encoder.fit_transform(df_encoded[moderate_cardinality_cols])\n",
    "\n",
    "# Apply Frequency Encoding to high-cardinality categories\n",
    "for col in high_cardinality_cols:\n",
    "    freq_encoding = df_encoded[col].value_counts().to_dict()\n",
    "    df_encoded[col] = df_encoded[col].map(freq_encoding)\n",
    "\n",
    "# Drop rows with missing SalePrice (target variable)\n",
    "df_encoded = df_encoded.dropna(subset=['SalePrice'])\n",
    "\n",
    "# Split into features and target\n",
    "X = df_encoded.drop(columns=['SalePrice'])\n",
    "y = df_encoded['SalePrice']\n",
    "\n",
    "# Split data into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "y_pred = rf_model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mse ** 0.5\n",
    "\n",
    "# Display model performance\n",
    "mae, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c037b6-c24e-4c95-a2e1-fcdfceacde19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define final categorization for Steering_Controls\n",
    "def categorize_steering_controls_final(value):\n",
    "    if value in [\"Four Wheel Standard\", \"Conventional\"]:\n",
    "        return \"Mid Price Steering\"\n",
    "    elif value in [\"Command Control\"]:\n",
    "        return \"High Price Steering\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "# Apply categorization\n",
    "df[\"Steering_Controls_Category\"] = df[\"Steering_Controls\"].apply(categorize_steering_controls_final)\n",
    "\n",
    "# Verify the distribution of the new categories\n",
    "df[\"Steering_Controls_Category\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9b83e8d3-e8bb-4e46-ba0b-7a6a9c3d1d25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Re-import necessary libraries due to execution reset\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Define final categorization for Steering_Controls\n",
    "def categorize_steering_controls_final(value):\n",
    "    if value in [\"Four Wheel Standard\", \"Conventional\"]:\n",
    "        return \"Mid Price Steering\"\n",
    "    elif value in [\"Command Control\"]:\n",
    "        return \"High Price Steering\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "\n",
    "\n",
    "# Define price-based categories for Track_Type\n",
    "def categorize_track_type(value):\n",
    "    if value in [\"Rubber\"]:\n",
    "        return \"Low Price Track\"\n",
    "    elif value in [\"Steel\"]:\n",
    "        return \"High Price Track\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "\n",
    "# Define price-based categories for Transmission\n",
    "def categorize_transmission(value):\n",
    "    if value in [\"Powershuttle\", \"Standard\", \"Direct Drive\"]:\n",
    "        return \"Low Price Transmission\"\n",
    "    elif value in [\"Autoshift\", \"Hydrostatic\"]:\n",
    "        return \"Mid Price Transmission\"\n",
    "    elif value in [\"Powershift\", \"None or Unspecified\", \"AutoShift\"]:\n",
    "        return \"High Price Transmission\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "\n",
    "# Define price-based categories for Steering_Controls\n",
    "def categorize_steering_controls(value):\n",
    "    if value in [\"Four Wheel Standard\"]:\n",
    "        return \"Low Price Steering\"\n",
    "    elif value in [\"Conventional\"]:\n",
    "        return \"Mid Price Steering\"\n",
    "    elif value in [\"Command Control\"]:\n",
    "        return \"High Price Steering\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "\n",
    "\n",
    "# Define price-based categories for ProductGroup\n",
    "def categorize_product_group(price):\n",
    "    if price < 20000:\n",
    "        return \"Low Price Group\"\n",
    "    elif 20000 <= price < 40000:\n",
    "        return \"Mid Price Group\"\n",
    "    else:\n",
    "        return \"High Price Group\"\n",
    "\n",
    "\n",
    "def create_model_category_mapping(df,model_avg_price):\n",
    "    \"\"\"\n",
    "    Creates a mapping of ModelID to Model_Category based on fiModelDesc and SalePrice in training data.\n",
    "    \n",
    "    Parameters:\n",
    "    training_df (pd.DataFrame): Training dataset containing fiModelDesc and SalePrice.\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary mapping ModelID to Model_Category.\n",
    "    \"\"\"\n",
    "    # Compute average SalePrice per fiModelDesc in training data\n",
    "    \n",
    "    \n",
    "    # Define price categories\n",
    "    def categorize_price(price):\n",
    "        if price < 20000:\n",
    "            return \"Low Price Models\"\n",
    "        elif 20000 <= price < 60000:\n",
    "            return \"Mid Price Models\"\n",
    "        else:\n",
    "            return \"High Price Models\"\n",
    "    \n",
    "    # Map fiModelDesc to categories\n",
    "    model_category_mapping = model_avg_price.apply(categorize_price).to_dict()\n",
    "    \n",
    "    # Create ModelID to category mapping using fiModelDesc\n",
    "    df[\"Model_Category\"] = df[\"fiModelDesc\"].map(model_category_mapping)\n",
    "    modelid_to_category = df.set_index(\"ModelID\")[\"Model_Category\"].to_dict()\n",
    "    \n",
    "    return modelid_to_category, model_category_mapping\n",
    "\n",
    "def categorize_model_id(df, modelid_to_category, model_category_mapping):\n",
    "    \"\"\"\n",
    "    Categorizes ModelID based on precomputed price categories.\n",
    "    If ModelID is not found, falls back to fiModelDesc categorization.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing ModelID and fiModelDesc columns.\n",
    "    modelid_to_category (dict): Mapping of ModelID to Model_Category.\n",
    "    model_category_mapping (dict): Mapping of fiModelDesc to Model_Category for fallback.\n",
    "    \n",
    "    Returns:\n",
    "    pd.Series: A Series with categorized model price labels.\n",
    "    \"\"\"\n",
    "    df[\"Predicted_Model_Category\"] = df[\"ModelID\"].map(modelid_to_category)\n",
    "    \n",
    "    # Handle missing ModelID by checking fiModelDesc mapping\n",
    "    missing_mask = df[\"Predicted_Model_Category\"].isna()\n",
    "    df.loc[missing_mask, \"Predicted_Model_Category\"] = df.loc[missing_mask, \"fiModelDesc\"].map(model_category_mapping)\n",
    "    \n",
    "    # Handle any remaining missing values by assigning 'Unknown'\n",
    "    df[\"Predicted_Model_Category\"].fillna(\"Unknown\", inplace=True)\n",
    "    \n",
    "    return df[\"Predicted_Model_Category\"]\n",
    "\n",
    "\n",
    "\n",
    "def extract_horsepower(df):\n",
    "    \"\"\"\n",
    "    Extracts and imputes missing Horsepower values from fiProductClassDesc.\n",
    "    Optimized for performance using vectorized operations.\n",
    "    \"\"\"\n",
    "    def extract_numeric_range(value):\n",
    "        \"\"\"Extracts the average horsepower value from a range like '100 to 120 Horsepower'.\"\"\"\n",
    "        if pd.isna(value) or not isinstance(value, str):\n",
    "            return np.nan\n",
    "        numbers = re.findall(r'[\\d\\.]+', value)\n",
    "        if len(numbers) == 2:\n",
    "            return (float(numbers[0]) + float(numbers[1])) / 2\n",
    "        elif len(numbers) == 1:\n",
    "            return float(numbers[0])\n",
    "        return np.nan\n",
    "    \n",
    "    # Identify rows with Horsepower in fiProductClassDesc\n",
    "    mask_horsepower = df['fiProductClassDesc'].str.contains(\"Horsepower\", na=False)\n",
    "    \n",
    "    # Extract horsepower values\n",
    "    df.loc[mask_horsepower, 'Horsepower_Unit_Type'] = 'Horsepower'\n",
    "    df.loc[mask_horsepower, 'Extracted_Horsepower'] = df.loc[mask_horsepower, 'fiProductClassDesc'].apply(extract_numeric_range)\n",
    "    \n",
    "    # Handle 'Variable' and 'No' values in Engine_Horsepower\n",
    "    df.loc[df['Engine_Horsepower'].isin(['Variable', 'No']), 'Engine_Horsepower'] = np.nan\n",
    "    \n",
    "    # Impute missing Engine_Horsepower values where Extracted_Horsepower is available\n",
    "    df['Engine_Horsepower_Imputed'] = df['Engine_Horsepower'].combine_first(df['Extracted_Horsepower'])\n",
    "    \n",
    "    # Fill remaining NaN values with median horsepower\n",
    "    #df['Engine_Horsepower_Imputed'].fillna(df['Engine_Horsepower_Imputed'].median(), inplace=True)\n",
    "    df.drop(columns='Engine_Horsepower',inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def preprocess_product_size(df):\n",
    "    \"\"\"\n",
    "    Function to preprocess and impute missing ProductSize values based on fiProductClassDesc.\n",
    "    \n",
    "    Steps:\n",
    "    1. Extract Product Type and Metric Tons / Horsepower from fiProductClassDesc.\n",
    "    2. Identify Unit Type (Metric Tons, Horsepower, or Lb Operating Capacity).\n",
    "    3. Convert Metric Tons / Horsepower to numerical values.\n",
    "    4. Use Metric Tons to impute missing ProductSize values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Extract Product Type and Size Descriptor\n",
    "    def split_product_desc(desc):\n",
    "        if pd.isna(desc):\n",
    "            return pd.NA, pd.NA\n",
    "        match = re.search(r'(.+?)-\\s*([\\d\\.]+.*)', desc)\n",
    "        if match:\n",
    "            return match.group(1).strip(), match.group(2).strip()\n",
    "        return desc.strip(), pd.NA  # If no match, return full description as product\n",
    "    \n",
    "    df[['Product_Type', 'Metric_Tons_HP']] = df['fiProductClassDesc'].apply(lambda x: pd.Series(split_product_desc(x)))\n",
    "    \n",
    "    # Step 2: Identify Unit Type\n",
    "    def extract_unit_type(desc):\n",
    "        if pd.isna(desc):\n",
    "            return pd.NA\n",
    "        if \"Metric Tons\" in desc:\n",
    "            return \"Metric Tons\"\n",
    "        elif \"Horsepower\" in desc:\n",
    "            return \"Horsepower\"\n",
    "        elif \"Lb Operating Capacity\" in desc:\n",
    "            return \"Lb Operating Capacity\"\n",
    "        return pd.NA\n",
    "    \n",
    "    df[\"Unit_Type\"] = df[\"Metric_Tons_HP\"].apply(extract_unit_type)\n",
    "    \n",
    "    # Step 3: Convert Metric Tons to numerical values\n",
    "    def extract_numeric_range(value):\n",
    "        \"\"\"Extracts the average value from a range like '12.0 to 14.0 Metric Tons'.\"\"\"\n",
    "        if pd.isna(value) or not isinstance(value, str):\n",
    "            return np.nan\n",
    "        numbers = re.findall(r'[\\d\\.]+', value)\n",
    "        if len(numbers) == 2:  # If range exists, take the average\n",
    "            return (float(numbers[0]) + float(numbers[1])) / 2\n",
    "        elif len(numbers) == 1:  # If only one number exists, use it\n",
    "            return float(numbers[0])\n",
    "        return np.nan\n",
    "    \n",
    "    df['Metric_Tons_Value'] = df['Metric_Tons_HP'].apply(lambda x: extract_numeric_range(x) if isinstance(x, str) else np.nan)\n",
    "    \n",
    "    # Step 4: Impute missing ProductSize values using Metric Tons\n",
    "    def impute_product_size(row):\n",
    "        if pd.isna(row['ProductSize']) and not pd.isna(row['Metric_Tons_Value']) and (row['Metric_Tons_Value']=='Metric Tons'):\n",
    "            if row['Metric_Tons_Value']  <= 5:\n",
    "                return 'Mini'\n",
    "            elif 5 < row['Metric_Tons_Value'] <= 75:\n",
    "                return 'Compact'\n",
    "            elif 20 < row['Metric_Tons_Value'] <= 50:\n",
    "                return 'Large / Medium'\n",
    "            elif 75 < row['Metric_Tons_Value'] <= 200:\n",
    "                return 'Medium'\n",
    "            elif row['Metric_Tons_Value'] > 200:\n",
    "                return 'Large'\n",
    "        return row['ProductSize']\n",
    "    \n",
    "    df['ProductSize_Imputed'] = df.apply(impute_product_size, axis=1)\n",
    "    df.drop(columns=['ProductSize','Metric_Tons_Value','Metric_Tons_HP','Unit_Type'],inplace=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    return df\n",
    "def Enclosure_fun(df):\n",
    "    list1=['EROPS','OROPS','EROPS AC']\n",
    "    list2=['NO ROPS','None or Unspecified']\n",
    "    df['Enclosure_cat']=0\n",
    "    df.loc[df['Enclosure'].isin(list1), 'Enclosure_cat'] = 1\n",
    "    df.loc[df['Enclosure'].isin(list2), 'Enclosure_cat'] = 0\n",
    "    df.loc[df['Enclosure']=='EROPS w AC','Enclosure_cat']=2\n",
    "    df.drop(columns='Enclosure',inplace=True)\n",
    "    return df\n",
    "# Define categories based on observed price trends\n",
    "def categorize_hydraulics(value):\n",
    "    if value in [\"Missing\", \"Auxiliary\", \"Standard\"]:\n",
    "        return \"Basic\"\n",
    "    elif value in [\"2 Valve\", \"None or Unspecified\", \"3 Valve\"]:\n",
    "        return \"Mid-Level\"\n",
    "    else:\n",
    "        return \"Advanced\"\n",
    "\n",
    "\n",
    "\n",
    "# Reload the dataset\n",
    "    \n",
    "\n",
    "#df=df[['SalesID','saledate',\"Enclosure\",\"YearMade\",\"ModelID\",\"MachineID\",\"Ripper\",\"ProductGroup\",\"ProductSize\",\"ProductGroupDesc\",\"Tire_Size\",\"MachineHoursCurrentMeter\",\"Blade_Type\",\"ProductGroupDesc\",\"Travel_Controls\",\"auctioneerID\",\"datasource\",\"Drive_System\",\"Pushblock\",\"Tire_Size\",\"Ride_Control\",\"UsageBand\",\"Coupler\",\"Engine_Horsepower_Imputed\"]].copy()\n",
    "def saledate(df):\n",
    "    # Convert 'saledate' to datetime and extract year, month, and day\n",
    "    df['saledate'] = pd.to_datetime(df['saledate'], errors='coerce')\n",
    "    df['sale_year'] = df['saledate'].dt.year\n",
    "    df['sale_month'] = df['saledate'].dt.month\n",
    "    df['sale_day'] = df['saledate'].dt.day\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_data(df,modelid_to_category, model_category_mapping,product_group_avg_price):\n",
    "    # Apply categorization\n",
    "    df[\"Hydraulics_Category\"] = df[\"Hydraulics\"].apply(categorize_hydraulics)\n",
    "    # Apply categorization\n",
    "    df[\"Steering_Controls_Category\"] = df[\"Steering_Controls\"].apply(categorize_steering_controls)\n",
    "    # Apply categorization\n",
    "    df[\"Transmission_Category\"] = df[\"Transmission\"].apply(categorize_transmission)\n",
    "    \n",
    "    # Apply categorization\n",
    "    df[\"Steering_Controls_Category\"] = df[\"Steering_Controls\"].apply(categorize_steering_controls_final)\n",
    "    # Apply categorization to test data\n",
    "    # Apply categorization to test dat\n",
    "    df[\"Predicted_Model_Category\"] = categorize_model_id(df, modelid_to_category, model_category_mapping) # need to upload the modelid_to_category, model_category_mapping in the test\n",
    "    df[\"Track_Type_Category\"] = df[\"Track_Type\"].apply(categorize_track_type)\n",
    "    # Map each ProductGroup to a price category\n",
    "    df[\"ProductGroup_Category\"] = df[\"ProductGroup\"].map(product_group_avg_price).apply(categorize_product_group) # need to upload the categorize_product_group in the test\n",
    "\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "# Define function to update 'YearMade' and create 'YearMade_Bucket'\n",
    "def update_YearMade(df):\n",
    "    print(\"update_YearMade START\")\n",
    "    \n",
    "    # Compute median YearMade for each ModelID\n",
    "    model_medians = df.loc[df['YearMade'] > 1000].groupby('ModelID')['YearMade'].median()\n",
    "    \n",
    "    # Update YearMade where it is 1000 using ModelID median\n",
    "    df.loc[df['YearMade'] == 1000, 'YearMade'] = df['ModelID'].map(model_medians)\n",
    "    \n",
    "    # Compute overall median YearMade excluding 1000 values\n",
    "    overall_median = df.loc[df['YearMade'] > 1000, 'YearMade'].median()\n",
    "    \n",
    "    # Replace any remaining 1000 values with overall median\n",
    "    df['YearMade'].fillna(overall_median, inplace=True)\n",
    "        \n",
    "    # Bucketize YearMade\n",
    "    bins = [0, 1980, 1995, 2005, 2010, 2025]\n",
    "    labels = [\"Before 1980\", \"1980-1995\", \"1996-2005\", \"2006-2010\", \"2011-2025\"]\n",
    "    df['YearMade_Bucket'] = pd.cut(df['YearMade'], bins=bins, labels=labels, right=True)\n",
    "\n",
    "    df_filtered[\"YearMade_Bucket\"] = df[\"YearMade_Bucket\"].astype(str).fillna(\"Unknown\")\n",
    "    \n",
    "    # Define category order and apply Ordinal Encoding\n",
    "    year_bucket_encoder = OrdinalEncoder(\n",
    "        categories=[[\"Before 1980\", \"1980-1995\", \"1996-2005\", \"2006-2010\", \"2011-2025\", \"Unknown\"]],\n",
    "        handle_unknown=\"use_encoded_value\",\n",
    "        unknown_value=-1\n",
    "    )\n",
    "    \n",
    "    df[\"YearMade_Bucket\"] = year_bucket_encoder.fit_transform(df[[\"YearMade_Bucket\"]])\n",
    "    \n",
    "    print(\"update_YearMade END\")\n",
    "    return df\n",
    "def pre_train(df):\n",
    "    model_avg_price = df.groupby(\"fiModelDesc\")[\"SalePrice\"].mean()\n",
    "    product_group_avg_price = df.groupby(\"ProductGroup\")[\"SalePrice\"].mean()\n",
    "\n",
    "    return model_avg_price,product_group_avg_price\n",
    "def Pre_train_test_model(df):\n",
    "    # Filter dataset to only include the last 5 years\n",
    "    recent_years = df['sale_year'].dropna().unique()\n",
    "    recent_years.sort()\n",
    "    #selected_years = recent_years[-5:]\n",
    "    #df_filtered = df[df['sale_year'].isin(selected_years)]\n",
    "    df_filtered=df.copy()\n",
    "    \n",
    "    # Drop irrelevant columns: 'SalesID', 'saledate'\n",
    "    high_cardinality_cols = [col for col in df_filtered.select_dtypes(include=['object']).columns if df_filtered[col].nunique() > 50]\n",
    "    df_filtered = df_filtered.drop(columns=['SalesID', 'saledate'] + high_cardinality_cols, errors='ignore')\n",
    "    \n",
    "    # Identify categorical columns\n",
    "    categorical_cols = df_filtered.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Identify high, moderate, and low cardinality categorical columns\n",
    "    high_cardinality_cols = [col for col in categorical_cols if df_filtered[col].nunique() >= 50]\n",
    "    moderate_cardinality_cols = [col for col in categorical_cols if 10 <= df_filtered[col].nunique() < 50]\n",
    "    low_cardinality_cols = [col for col in categorical_cols if df_filtered[col].nunique() < 10]\n",
    "    \n",
    "    # Fill missing categorical values with \"Unknown\"\n",
    "    for col in categorical_cols:\n",
    "        df_filtered[col] = df_filtered[col].fillna(\"Unknown\")\n",
    "    \n",
    "    # Encoding strategy\n",
    "    \n",
    "    # Apply One-Hot Encoding (OHE) to low-cardinality categories\n",
    "    df_encoded = pd.get_dummies(df_filtered, columns=low_cardinality_cols, drop_first=True)\n",
    "    \n",
    "    # Apply Ordinal Encoding to moderate-cardinality categories\n",
    "    if moderate_cardinality_cols:\n",
    "        ord_encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "        df_encoded[moderate_cardinality_cols] = ord_encoder.fit_transform(df_encoded[moderate_cardinality_cols])\n",
    "    \n",
    "    # Apply Frequency Encoding to high-cardinality categories\n",
    "    for col in high_cardinality_cols:\n",
    "        freq_encoding = df_encoded[col].value_counts().to_dict()\n",
    "        df_encoded[col] = df_encoded[col].map(freq_encoding)\n",
    "    \n",
    "    # Drop rows with missing SalePrice (target variable)\n",
    "    df_encoded = df_encoded.dropna(subset=['SalePrice'])\n",
    "\n",
    "    return df_encoded\n",
    "def train_run(df_encoded):\n",
    "\n",
    "    # Split into features and target\n",
    "    X = df_encoded.drop(columns=['SalePrice'])\n",
    "    y = df_encoded['SalePrice']\n",
    "    \n",
    "    # Split data into train and test sets\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train a Random Forest model\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    \n",
    "    rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "    \n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = mse ** 0.5\n",
    "    \n",
    "    # Display model performance\n",
    "    print(mae, rmse)\n",
    "    return rf_model\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3e2d67f1-e23c-4eff-9ab4-67d733d1df89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eitanb\\AppData\\Local\\Temp\\ipykernel_22792\\3751923447.py:4: DtypeWarning: Columns (13,39,40,41) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path_train)[['SalesID', 'SalePrice',  'ModelID',\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update_YearMade START\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eitanb\\AppData\\Local\\Temp\\ipykernel_22792\\908198221.py:287: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[1980. 1980. 1994. ... 2003. 2004. 2004.]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[df['YearMade'] == 1000, 'YearMade'] = df['ModelID'].map(model_medians)\n",
      "C:\\Users\\eitanb\\AppData\\Local\\Temp\\ipykernel_22792\\908198221.py:293: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['YearMade'].fillna(overall_median, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update_YearMade END\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eitanb\\AppData\\Local\\Temp\\ipykernel_22792\\908198221.py:112: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"Predicted_Model_Category\"].fillna(\"Unknown\", inplace=True)\n",
      "C:\\Users\\eitanb\\AppData\\Local\\Temp\\ipykernel_22792\\908198221.py:145: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  df['Engine_Horsepower_Imputed'] = df['Engine_Horsepower'].combine_first(df['Extracted_Horsepower'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4540.807883070412 7310.768671236769\n"
     ]
    }
   ],
   "source": [
    "#def main()\n",
    "ProductSize_path=r'C:\\Users\\eitanb\\Documents\\DS\\ML\\ML_project\\DATA/ProductSize_Null_fix_model.pkl'\n",
    "file_path_train = r\"C:\\Users\\eitanb\\Documents\\DS\\ML\\ML_project\\DATA/Train.csv\"\n",
    "df = pd.read_csv(file_path_train)[['SalesID', 'SalePrice',  'ModelID',\n",
    "        'YearMade', 'MachineHoursCurrentMeter', 'UsageBand',\n",
    "       'saledate', 'ProductSize',\n",
    "       'fiProductClassDesc', 'state', 'ProductGroup', \n",
    "       'Drive_System', 'Enclosure', \n",
    "        'Transmission', 'Turbocharged',  'Engine_Horsepower', 'Hydraulics',\n",
    "         'Tire_Size',\n",
    "       'Track_Type',\n",
    "       'Travel_Controls', 'Differential_Type', 'Steering_Controls','fiModelDesc']]\n",
    "\n",
    "# Apply YearMade update\n",
    "df = update_YearMade(df)\n",
    "df= saledate(df)\n",
    "\n",
    "model_avg_price,product_group_avg_price=pre_train(df)\n",
    "# Create mappings from training data only in saleprice \n",
    "modelid_to_category, model_category_mapping = create_model_category_mapping(df,model_avg_price) # need to upload the modelid_to_category, model_category_mapping in the test\n",
    "\n",
    "\n",
    "df=preprocess_data(df,modelid_to_category, model_category_mapping,product_group_avg_price)\n",
    "df=Enclosure_fun(df)\n",
    "df=extract_horsepower(df)\n",
    "\n",
    "# Map each ProductGroup to a price category\n",
    "df=preprocess_product_size(df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Compute the average price for each ProductGroup\n",
    "# Apply categorization\n",
    "\n",
    "\n",
    "df=Pre_train_test_model(df)\n",
    "\n",
    "\n",
    "#df.drop(columns=['fiProductClassDesc','Hydraulics','fiSecondaryDesc','auctioneerID','fiModelDesc','fiBaseModel','ProductGroupDesc','MachineID','ProductGroup'],inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "rf_model=train_run(df_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f1c13a9d-5824-49b5-ab94-698214fa9c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SalesID', 'SalePrice', 'ModelID', 'YearMade',\n",
       "       'MachineHoursCurrentMeter', 'UsageBand', 'saledate',\n",
       "       'fiProductClassDesc', 'state', 'ProductGroup', 'Drive_System',\n",
       "       'Transmission', 'Turbocharged', 'Hydraulics', 'Tire_Size', 'Track_Type',\n",
       "       'Travel_Controls', 'Differential_Type', 'Steering_Controls',\n",
       "       'fiModelDesc', 'YearMade_Bucket', 'Model_Category',\n",
       "       'Hydraulics_Category', 'Steering_Controls_Category',\n",
       "       'Transmission_Category', 'Predicted_Model_Category',\n",
       "       'Track_Type_Category', 'ProductGroup_Category', 'Enclosure_cat',\n",
       "       'Horsepower_Unit_Type', 'Extracted_Horsepower',\n",
       "       'Engine_Horsepower_Imputed', 'Product_Type', 'ProductSize_Imputed'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d382ba83-27ac-46c2-a88e-005e1a1cc9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Valid_df = pd.read_csv(file_path_train)[['SalesID',  'ModelID',\n",
    "        'YearMade', 'MachineHoursCurrentMeter', 'UsageBand',\n",
    "       'saledate', 'ProductSize',\n",
    "       'fiProductClassDesc', 'state', 'ProductGroup', \n",
    "       'Drive_System', 'Enclosure', \n",
    "        'Transmission', 'Turbocharged',  'Engine_Horsepower', 'Hydraulics',\n",
    "         'Tire_Size',\n",
    "       'Track_Type',\n",
    "       'Travel_Controls', 'Differential_Type', 'Steering_Controls','fiModelDesc']]\n",
    "\n",
    "# Apply YearMade update\n",
    "df = update_YearMade(df)\n",
    "df= saledate(df)\n",
    "\n",
    "model_avg_price,product_group_avg_price=pre_train(df)\n",
    "# Create mappings from training data only in saleprice \n",
    "modelid_to_category, model_category_mapping = create_model_category_mapping(df,model_avg_price) # need to upload the modelid_to_category, model_category_mapping in the test\n",
    "\n",
    "\n",
    "df=preprocess_data(df,modelid_to_category, model_category_mapping,product_group_avg_price)\n",
    "df=Enclosure_fun(df)\n",
    "df=extract_horsepower(df)\n",
    "\n",
    "# Map each ProductGroup to a price category\n",
    "df=preprocess_product_size(df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Compute the average price for each ProductGroup\n",
    "# Apply categorization\n",
    "\n",
    "\n",
    "df=Pre_train_test_model(df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Ensure predictions align with the original index\n",
    "predictions = rf_model.predict(df)\n",
    "\n",
    "# Verify matching row count\n",
    "if len(predictions) != len(df):\n",
    "    raise ValueError(\"Mismatch in row count between processed data and predictions!\")\n",
    "\n",
    "# Reconstruct results DataFrame, ensuring index alignment\n",
    "results = pd.DataFrame({\n",
    "    \"SalesID\": sales_id_col.iloc[:, 0],  # Ensure correct alignment\n",
    "    \"SalePrice\": »  # Reverse log transformation\n",
    "}, index=original_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "559d2bc3-7a45-436f-ad93-0689a2aee0f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feature_1_Predicted_Model_Category_Low Price Models_importance': 0.1859553473741895}\n",
      "{'feature_2_Predicted_Model_Category_Mid Price Models_importance': 0.14865644456054966}\n",
      "{'feature_3_Model_Category_Low Price Models_importance': 0.1345675563718487}\n",
      "{'feature_4_Model_Category_Mid Price Models_importance': 0.10344216393111744}\n",
      "{'feature_5_YearMade_importance': 0.07844089188209398}\n",
      "{'feature_6_ModelID_importance': 0.0734173951672887}\n",
      "{'feature_7_sale_year_importance': 0.07303127477363637}\n",
      "{'feature_8_Product_Type_importance': 0.02841903265202036}\n",
      "{'feature_9_sale_day_importance': 0.026005490469335507}\n",
      "{'feature_10_sale_month_importance': 0.022311927108702393}\n",
      "{'feature_11_Enclosure_cat_importance': 0.018173305572651913}\n",
      "{'feature_12_Engine_Horsepower_Imputed_importance': 0.018094654396660054}\n",
      "{'feature_13_Extracted_Horsepower_importance': 0.0180840214260408}\n",
      "{'feature_14_ProductSize_Imputed_Large / Medium_importance': 0.010947471226807249}\n",
      "{'feature_15_MachineHoursCurrentMeter_importance': 0.009306451539479128}\n",
      "{'feature_16_YearMade_Bucket_importance': 0.007868274094353585}\n",
      "{'feature_17_ProductSize_Imputed_Mini_importance': 0.004870649455385091}\n",
      "{'feature_18_ProductSize_Imputed_Small_importance': 0.004217493784309183}\n",
      "{'feature_19_Hydraulics_importance': 0.0034754913789566675}\n",
      "{'feature_20_Tire_Size_importance': 0.0029398219754241434}\n",
      "{'feature_21_Drive_System_Unknown_importance': 0.002159140148569304}\n",
      "{'feature_22_Drive_System_No_importance': 0.002133176912512211}\n",
      "{'feature_23_ProductGroup_Category_Low Price Group_importance': 0.0015937883301624307}\n",
      "{'feature_24_ProductGroup_SSL_importance': 0.0014247805350237155}\n",
      "{'feature_25_UsageBand_Low_importance': 0.0012710700615490489}\n",
      "{'feature_26_ProductSize_Imputed_Unknown_importance': 0.001139896306555562}\n",
      "{'feature_27_ProductSize_Imputed_Medium_importance': 0.0011251315324144475}\n",
      "{'feature_28_Travel_Controls_Differential Steer_importance': 0.001112768500107561}\n",
      "{'feature_29_ProductGroup_Category_Mid Price Group_importance': 0.0011057505065811934}\n",
      "{'feature_30_Transmission_Unknown_importance': 0.00106755738606137}\n",
      "{'feature_31_Transmission_Category_Unknown_importance': 0.0010055783902572933}\n",
      "{'feature_32_ProductGroup_MG_importance': 0.0009523773689303983}\n",
      "{'feature_33_UsageBand_Medium_importance': 0.0009268803150198221}\n",
      "{'feature_34_Travel_Controls_None or Unspecified_importance': 0.0008170026369878441}\n",
      "{'feature_35_UsageBand_Unknown_importance': 0.000777214070397875}\n",
      "{'feature_36_Transmission_None or Unspecified_importance': 0.0007487819103541778}\n",
      "{'feature_37_Drive_System_Four Wheel Drive_importance': 0.0006527623753017373}\n",
      "{'feature_38_Transmission_Standard_importance': 0.000603672252585613}\n",
      "{'feature_39_ProductSize_Imputed_Large_importance': 0.0005942817751027037}\n",
      "{'feature_40_Transmission_Category_Low Price Transmission_importance': 0.0005732654520178554}\n",
      "{'feature_41_Transmission_Powershift_importance': 0.00048647398757506574}\n",
      "{'feature_42_Drive_System_Two Wheel Drive_importance': 0.00044212615672007316}\n",
      "{'feature_43_ProductGroup_TTT_importance': 0.00043825448264375543}\n",
      "{'feature_44_Hydraulics_Category_Mid-Level_importance': 0.00042893345874139155}\n",
      "{'feature_45_Travel_Controls_Unknown_importance': 0.0004213843439288267}\n",
      "{'feature_46_Differential_Type_Standard_importance': 0.00041787025367137306}\n",
      "{'feature_47_Track_Type_Steel_importance': 0.00038046120607269083}\n",
      "{'feature_48_Track_Type_Category_Low Price Track_importance': 0.0002932897501643674}\n",
      "{'feature_49_Steering_Controls_Conventional_importance': 0.00027644953992531627}\n",
      "{'feature_50_Steering_Controls_Category_Mid Price Steering_importance': 0.0002510856062686638}\n",
      "{'feature_51_Travel_Controls_Finger Tip_importance': 0.00024078497622590064}\n",
      "{'feature_52_ProductGroup_WL_importance': 0.00023371533282103612}\n",
      "{'feature_53_Hydraulics_Category_Basic_importance': 0.00019351033502207327}\n",
      "{'feature_54_Steering_Controls_Unknown_importance': 0.00016353931082043404}\n",
      "{'feature_55_Differential_Type_Unknown_importance': 0.0001583378670514161}\n",
      "{'feature_56_Steering_Controls_Category_Unknown_importance': 0.00015735160266514993}\n",
      "{'feature_57_Track_Type_Unknown_importance': 0.00014955207541844382}\n",
      "{'feature_58_Track_Type_Category_Unknown_importance': 0.00014460336643140102}\n",
      "{'feature_59_ProductGroup_TEX_importance': 0.00012069921036814215}\n",
      "{'feature_60_Horsepower_Unit_Type_Unknown_importance': 9.896162799980483e-05}\n",
      "{'feature_61_Transmission_Hydrostatic_importance': 7.713598121783416e-05}\n",
      "{'feature_62_Transmission_Category_Mid Price Transmission_importance': 7.647260031095128e-05}\n",
      "{'feature_63_Travel_Controls_Lever_importance': 7.379935078532833e-05}\n",
      "{'feature_64_Travel_Controls_2 Pedal_importance': 6.642852244663165e-05}\n",
      "{'feature_65_Turbocharged_Yes_importance': 6.258708837287392e-05}\n",
      "{'feature_66_Transmission_Powershuttle_importance': 4.666321332572194e-05}\n",
      "{'feature_67_Transmission_Direct Drive_importance': 3.7501489548743136e-05}\n",
      "{'feature_68_Differential_Type_No Spin_importance': 2.4605406906376726e-05}\n",
      "{'feature_69_Turbocharged_Unknown_importance': 1.1777595976977793e-05}\n",
      "{'feature_70_Travel_Controls_Pedal_importance': 1.0364495460742081e-05}\n",
      "{'feature_71_Steering_Controls_Four Wheel Standard_importance': 3.813901633557317e-06}\n",
      "{'feature_72_Steering_Controls_Wheel_importance': 7.462721440218077e-07}\n",
      "{'feature_73_Transmission_Autoshift_importance': 6.376057463483205e-07}\n",
      "{'feature_74_Differential_Type_Locking_importance': 4.607824606119317e-08}\n",
      "{'feature_75_Steering_Controls_No_importance': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# Optional: Log feature importances\n",
    "importances = rf_model.feature_importances_\n",
    "\n",
    "# Create a zip of feature names and feature importances\n",
    "feature_zip = zip(rf_model.feature_names_in_, rf_model.feature_importances_)\n",
    "\n",
    "# Sort the zip by feature importance in descending order\n",
    "sorted_feature_zip = sorted(feature_zip, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted feature names, importances, and index\n",
    "for idx, (feat, importance) in enumerate(sorted_feature_zip, 1):\n",
    "  print({f\"feature_{idx}_{feat}_importance\" : importance})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7216ff6e-d934-4964-90a8-5b1625ca24e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "file_path_train = r\"C:\\Users\\eitanb\\Documents\\DS\\ML\\ML_project\\DATA/Train.csv\"\n",
    "\n",
    "def categorize_steering_controls(value):\n",
    "    if value in [\"Four Wheel Standard\"]:\n",
    "        return \"Low Price Steering\"\n",
    "    elif value in [\"Conventional\"]:\n",
    "        return \"Mid Price Steering\"\n",
    "    elif value in [\"Command Control\"]:\n",
    "        return \"High Price Steering\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "def categorize_transmission(value):\n",
    "    if value in [\"Powershuttle\", \"Standard\", \"Direct Drive\"]:\n",
    "        return \"Low Price Transmission\"\n",
    "    elif value in [\"Autoshift\", \"Hydrostatic\"]:\n",
    "        return \"Mid Price Transmission\"\n",
    "    elif value in [\"Powershift\", \"None or Unspecified\", \"AutoShift\"]:\n",
    "        return \"High Price Transmission\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "def categorize_track_type(value):\n",
    "    if value in [\"Rubber\"]:\n",
    "        return \"Low Price Track\"\n",
    "    elif value in [\"Steel\"]:\n",
    "        return \"High Price Track\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "def categorize_product_group(price):\n",
    "    if price < 20000:\n",
    "        return \"Low Price Group\"\n",
    "    elif 20000 <= price < 40000:\n",
    "        return \"Mid Price Group\"\n",
    "    else:\n",
    "        return \"High Price Group\"\n",
    "\n",
    "def categorize_hydraulics(value):\n",
    "    if value in [\"Missing\", \"Auxiliary\", \"Standard\"]:\n",
    "        return \"Basic\"\n",
    "    elif value in [\"2 Valve\", \"None or Unspecified\", \"3 Valve\"]:\n",
    "        return \"Mid-Level\"\n",
    "    else:\n",
    "        return \"Advanced\"\n",
    "\n",
    "def create_model_category_mapping(training_df,model_avg_price):\n",
    "    \n",
    "    \n",
    "    \n",
    "    def categorize_price(price):\n",
    "        if price < 20000:\n",
    "            return \"Low Price Models\"\n",
    "        elif 20000 <= price < 60000:\n",
    "            return \"Mid Price Models\"\n",
    "        else:\n",
    "            return \"High Price Models\"\n",
    "    \n",
    "    model_category_mapping = model_avg_price.apply(categorize_price).to_dict()\n",
    "    training_df[\"Model_Category\"] = training_df[\"fiModelDesc\"].map(model_category_mapping)\n",
    "    modelid_to_category = training_df.set_index(\"ModelID\")[\"Model_Category\"].to_dict()\n",
    "    \n",
    "    return modelid_to_category, model_category_mapping\n",
    "\n",
    "def categorize_model_id(df, modelid_to_category, model_category_mapping):\n",
    "    df[\"Predicted_Model_Category\"] = df[\"ModelID\"].map(modelid_to_category)\n",
    "    missing_mask = df[\"Predicted_Model_Category\"].isna()\n",
    "    df.loc[missing_mask, \"Predicted_Model_Category\"] = df.loc[missing_mask, \"fiModelDesc\"].map(model_category_mapping)\n",
    "    df[\"Predicted_Model_Category\"].fillna(\"Unknown\", inplace=True)\n",
    "    return df\n",
    "\n",
    "def preprocess_data(df,product_group_avg_price):\n",
    "    df['saledate'] = pd.to_datetime(df['saledate'], errors='coerce')\n",
    "    df['sale_year'] = df['saledate'].dt.year\n",
    "    df['sale_month'] = df['saledate'].dt.month\n",
    "    df['sale_day'] = df['saledate'].dt.day\n",
    "    \n",
    "    df[\"Hydraulics_Category\"] = df[\"Hydraulics\"].apply(categorize_hydraulics)\n",
    "    df[\"Steering_Controls_Category\"] = df[\"Steering_Controls\"].apply(categorize_steering_controls)\n",
    "    df[\"Transmission_Category\"] = df[\"Transmission\"].apply(categorize_transmission)\n",
    "    df[\"Track_Type_Category\"] = df[\"Track_Type\"].apply(categorize_track_type)\n",
    "    \n",
    "    \n",
    "    df[\"ProductGroup_Category\"] = df[\"ProductGroup\"].map(product_group_avg_price).apply(categorize_product_group)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def pre_train_only(file_path_train):\n",
    "    df = pd.read_csv(file_path_train)[['SalesID', 'SalePrice',  'ModelID',\n",
    "        'YearMade', 'MachineHoursCurrentMeter', 'UsageBand', 'saledate', 'ProductSize',\n",
    "       'fiProductClassDesc', 'state', 'ProductGroup', \n",
    "       'Drive_System', 'Enclosure', \n",
    "        'Transmission', 'Turbocharged',  'Engine_Horsepower', 'Hydraulics',\n",
    "         'Tire_Size',\n",
    "       'Track_Type',\n",
    "       'Travel_Controls', 'Differential_Type', 'Steering_Controls','fiModelDesc']]\n",
    "    product_group_avg_price = df.groupby(\"ProductGroup\")[\"SalePrice\"].mean()\n",
    "    model_avg_price = df.groupby(\"fiModelDesc\")[\"SalePrice\"].mean()\n",
    "    df = preprocess_data(df,product_group_avg_price)\n",
    "    modelid_to_category, model_category_mapping = create_model_category_mapping(df,model_avg_price)\n",
    "\n",
    "    df = categorize_model_id(df, modelid_to_category, model_category_mapping)\n",
    "    return df,product_group_avg_price,model_avg_price,modelid_to_category, model_category_mapping\n",
    "\n",
    "def test_only(df,product_group_avg_price,model_avg_price,modelid_to_category, model_category_mapping):\n",
    "    df = preprocess_data(df,product_group_avg_price)\n",
    "    modelid_to_category, model_category_mapping = create_model_category_mapping(df,model_avg_price)\n",
    "\n",
    "    df = categorize_model_id(df, modelid_to_category, model_category_mapping)\n",
    "    \n",
    "def pre_train_run_random_forest_model(df):\n",
    "    # Filter dataset to only include the last 5 years\n",
    "    recent_years = df['sale_year'].dropna().unique()\n",
    "    recent_years.sort()\n",
    "    #selected_years = recent_years[-5:]\n",
    "    #df_filtered = df[df['sale_year'].isin(selected_years)]\n",
    "    df_filtered=df.copy()\n",
    "    \n",
    "    # Drop irrelevant columns: 'SalesID', 'saledate'\n",
    "    high_cardinality_cols = [col for col in df_filtered.select_dtypes(include=['object']).columns if df_filtered[col].nunique() > 50]\n",
    "    df_filtered = df_filtered.drop(columns=['SalesID', 'saledate'] + high_cardinality_cols, errors='ignore')\n",
    "    categorical_cols = df_filtered.select_dtypes(include=['object']).columns\n",
    "\n",
    "\n",
    "    # Identify high, moderate, and low cardinality categorical columns\n",
    "    high_cardinality_cols = [col for col in categorical_cols if df_filtered[col].nunique() >= 50]\n",
    "    moderate_cardinality_cols = [col for col in categorical_cols if 10 <= df_filtered[col].nunique() < 50]\n",
    "    low_cardinality_cols = [col for col in categorical_cols if df_filtered[col].nunique() < 10]\n",
    "    \n",
    "    # Fill missing categorical values with \"Unknown\"\n",
    "    for col in categorical_cols:\n",
    "        df_filtered[col] = df_filtered[col].fillna(\"Unknown\")\n",
    "    \n",
    "    # Encoding strategy\n",
    "    \n",
    "    # Apply One-Hot Encoding (OHE) to low-cardinality categories\n",
    "    df_encoded = pd.get_dummies(df_filtered, columns=low_cardinality_cols, drop_first=True)\n",
    "    \n",
    "    # Apply Ordinal Encoding to moderate-cardinality categories\n",
    "    if moderate_cardinality_cols:\n",
    "        ord_encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "        df_encoded[moderate_cardinality_cols] = ord_encoder.fit_transform(df_encoded[moderate_cardinality_cols])\n",
    "    \n",
    "    # Apply Frequency Encoding to high-cardinality categories\n",
    "    for col in high_cardinality_cols:\n",
    "        freq_encoding = df_encoded[col].value_counts().to_dict()\n",
    "        df_encoded[col] = df_encoded[col].map(freq_encoding)\n",
    "    \n",
    "    # Drop rows with missing SalePrice (target variable)\n",
    "    df_encoded = df_encoded.dropna(subset=['SalePrice'])\n",
    "    return df_encoded\n",
    "\n",
    "def train_model(df_encoded):\n",
    "    X = df_encoded.drop(columns=['SalePrice'])\n",
    "    y = df_encoded['SalePrice']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    best_params = { 'n_estimators': 100}\n",
    "    \n",
    "    rf_model = RandomForestRegressor(**best_params, random_state=42, n_jobs=-1)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = mse ** 0.5\n",
    "    \n",
    "    # Display model performance\n",
    "    print(mae, rmse)\n",
    "        \n",
    "    return rf_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c0ed740d-4d73-4694-8c9d-89111fafb53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eitanb\\AppData\\Local\\Temp\\ipykernel_22792\\1263314632.py:96: DtypeWarning: Columns (13,39,40,41) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path_train)[['SalesID', 'SalePrice',  'ModelID',\n",
      "C:\\Users\\eitanb\\AppData\\Local\\Temp\\ipykernel_22792\\1263314632.py:76: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"Predicted_Model_Category\"].fillna(\"Unknown\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4540.807883070412 7310.768671236769\n"
     ]
    }
   ],
   "source": [
    "file_path_train = r\"C:\\Users\\eitanb\\Documents\\DS\\ML\\ML_project\\DATA/Train.csv\"\n",
    "df,product_group_avg_price,model_avg_price,modelid_to_category, model_category_mappingg=pre_train_only(file_path_train)\n",
    "df_encoded=pre_train_run_random_forest_model(df)\n",
    "rf_model=train_model(df_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c718d744-0227-4665-9935-f21f8975b73e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1d699166-dbe4-47df-8c1d-47c30d8e3a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eitanb\\AppData\\Local\\Temp\\ipykernel_22792\\1187768001.py:4: DtypeWarning: Columns (13,39,40,41) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Valid_df = pd.read_csv(file_path_train)[[\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update_YearMade START\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eitanb\\AppData\\Local\\Temp\\ipykernel_22792\\908198221.py:287: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[1980. 1980. 1994. ... 2003. 2004. 2004.]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[df['YearMade'] == 1000, 'YearMade'] = df['ModelID'].map(model_medians)\n",
      "C:\\Users\\eitanb\\AppData\\Local\\Temp\\ipykernel_22792\\908198221.py:293: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['YearMade'].fillna(overall_median, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update_YearMade END\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eitanb\\AppData\\Local\\Temp\\ipykernel_22792\\908198221.py:112: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"Predicted_Model_Category\"].fillna(\"Unknown\", inplace=True)\n",
      "C:\\Users\\eitanb\\AppData\\Local\\Temp\\ipykernel_22792\\908198221.py:145: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  df['Engine_Horsepower_Imputed'] = df['Engine_Horsepower'].combine_first(df['Extracted_Horsepower'])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m df \u001b[38;5;241m=\u001b[39m Enclosure_fun(df)\n\u001b[0;32m     27\u001b[0m df \u001b[38;5;241m=\u001b[39m extract_horsepower(df)\n\u001b[1;32m---> 28\u001b[0m df \u001b[38;5;241m=\u001b[39m preprocess_product_size(df)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Model pre-training process\u001b[39;00m\n\u001b[0;32m     31\u001b[0m df \u001b[38;5;241m=\u001b[39m Pre_train_test_model(df)\n",
      "Cell \u001b[1;32mIn[35], line 201\u001b[0m, in \u001b[0;36mpreprocess_product_size\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(numbers[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan\n\u001b[1;32m--> 201\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMetric_Tons_Value\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMetric_Tons_HP\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: extract_numeric_range(x) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan)\n\u001b[0;32m    203\u001b[0m \u001b[38;5;66;03m# Step 4: Impute missing ProductSize values using Metric Tons\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimpute_product_size\u001b[39m(row):\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4918\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4919\u001b[0m         func,\n\u001b[0;32m   4920\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[0;32m   4921\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[0;32m   4922\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   4923\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m-> 4924\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[0;32m   1508\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[0;32m   1509\u001b[0m )\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[35], line 201\u001b[0m, in \u001b[0;36mpreprocess_product_size.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(numbers[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan\n\u001b[1;32m--> 201\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMetric_Tons_Value\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMetric_Tons_HP\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: extract_numeric_range(x) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan)\n\u001b[0;32m    203\u001b[0m \u001b[38;5;66;03m# Step 4: Impute missing ProductSize values using Metric Tons\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimpute_product_size\u001b[39m(row):\n",
      "Cell \u001b[1;32mIn[35], line 196\u001b[0m, in \u001b[0;36mpreprocess_product_size.<locals>.extract_numeric_range\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m    194\u001b[0m numbers \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.]+\u001b[39m\u001b[38;5;124m'\u001b[39m, value)\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(numbers) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:  \u001b[38;5;66;03m# If range exists, take the average\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mfloat\u001b[39m(numbers[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mfloat\u001b[39m(numbers[\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(numbers) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# If only one number exists, use it\u001b[39;00m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(numbers[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load validation data\n",
    "Valid_df = pd.read_csv(file_path_train)[[\n",
    "    'SalesID', 'ModelID', 'YearMade', 'MachineHoursCurrentMeter', 'UsageBand',\n",
    "    'saledate', 'ProductSize', 'fiProductClassDesc', 'state', 'ProductGroup',\n",
    "    'Drive_System', 'Enclosure', 'Transmission', 'Turbocharged',\n",
    "    'Engine_Horsepower', 'Hydraulics', 'Tire_Size', 'Track_Type',\n",
    "    'Travel_Controls', 'Differential_Type', 'Steering_Controls', 'fiModelDesc'\n",
    "]]\n",
    "\n",
    "# Preserve SalesID before dropping it in preprocessing\n",
    "sales_id_col = Valid_df[['SalesID']].copy()\n",
    "\n",
    "# Apply preprocessing functions\n",
    "df = update_YearMade(Valid_df)\n",
    "df = saledate(df)\n",
    "\n",
    "#model_avg_price, product_group_avg_price = pre_train(df)\n",
    "\n",
    "# Create mappings\n",
    "modelid_to_category, model_category_mapping = create_model_category_mapping(df, model_avg_price)\n",
    "\n",
    "# Apply preprocessing\n",
    "df = preprocess_data(df, modelid_to_category, model_category_mapping, product_group_avg_price)\n",
    "df = Enclosure_fun(df)\n",
    "df = extract_horsepower(df)\n",
    "df = preprocess_product_size(df)\n",
    "\n",
    "# Model pre-training process\n",
    "df = Pre_train_test_model(df)\n",
    "\n",
    "# Ensure index alignment\n",
    "df.index = sales_id_col.index  # Keep index same as SalesID\n",
    "\n",
    "# Make predictions\n",
    "predictions = rf_model.predict(df)\n",
    "\n",
    "# Ensure matching row count\n",
    "if len(predictions) != len(df):\n",
    "    raise ValueError(\"Mismatch in row count between processed data and predictions!\")\n",
    "\n",
    "# Construct results DataFrame with correct alignment\n",
    "results = pd.DataFrame({\n",
    "    \"SalesID\": sales_id_col[\"SalesID\"].values,  # Retrieve original SalesID\n",
    "    \"SalePrice\": np.exp(predictions)  # Reverse log transformation if needed\n",
    "})\n",
    "\n",
    "# Save to Excel\n",
    "results.to_excel(\"predictions.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b747ef9-41ea-4e8e-b765-79a47e23c05e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
