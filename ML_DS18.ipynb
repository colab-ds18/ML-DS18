{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/colab-ds18/ML-DS18/blob/main/ML_DS18.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ofer functions\n",
        "'''\n",
        "General Funtions Author OA\n",
        "Date 29/01/2025\n",
        "'''\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split  # For splitting the dataset\n",
        "from sklearn.ensemble import RandomForestRegressor  # For the Random Forest regression model\n",
        "import shap\n",
        "import numpy as np\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.metrics import mean_squared_log_error\n",
        "\n",
        "def show_unique_values(df, column_name):\n",
        "  '''\n",
        "  Function to list all unique values in a given column of a DataFrame.\n",
        "  '''\n",
        "  if column_name not in df.columns:\n",
        "    raise ValueError(f\"Column '{column_name}' does not exist in the DataFrame.\")\n",
        "    unique_products = list_unique_values(df, column_name)\n",
        "    print(unique_products)\n",
        "    null_stats_per_column(df,column_name)\n",
        "\n",
        "\n",
        "def list_unique_values(df, column_name):\n",
        "    \"\"\"\n",
        "    Function to list all unique values in a given column of a DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    - df: pandas DataFrame\n",
        "    - column_name: Name of the column to find unique values\n",
        "\n",
        "    Returns:\n",
        "    - A list of unique values in the specified column.\n",
        "    \"\"\"\n",
        "    if column_name not in df.columns:\n",
        "        raise ValueError(f\"Column '{column_name}' does not exist in the DataFrame.\")\n",
        "\n",
        "    unique_values = df[column_name].unique()\n",
        "    return unique_values\n",
        "\n",
        "\n",
        "def update_NaN_To_None_or_Unspecified(df):\n",
        "    \"\"\"\n",
        "    Updates NaN values to 'None or Unspecified' in columns where the value\n",
        "    'None or Unspecified' is already present.\n",
        "    \"\"\"\n",
        "    print(\"update_NaN_To_None_or_Unspecified START\")\n",
        "\n",
        "    update_to = 'None or Unspecified'\n",
        "\n",
        "    for col in df.columns:\n",
        "        unique_values = list_unique_values(df, col)  # Get unique values for the column\n",
        "\n",
        "        # Check if 'None or Unspecified' exists and if there are NaN values\n",
        "        if update_to in unique_values and pd.isnull(unique_values).any():\n",
        "            df[col] = df[col].fillna(update_to)  # Fill NaN with the specified value\n",
        "            #print(f\"Updated '{col}' column successfully!\")\n",
        "        #else:\n",
        "        #    print(f\"Column '{col}' does not have the required conditions for update.\")\n",
        "\n",
        "    print(\"update_NaN_To_None_or_Unspecified END\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def columns_with_nulls(df,only_numeric_columns=True):\n",
        "  '''\n",
        "  Function to list all columns with null values in a given DataFrame.\n",
        "  '''\n",
        "  print(\"columns_with_nulls START/END\")\n",
        "  if only_numeric_columns:\n",
        "    # Get numeric columns only\n",
        "    numeric_columns = df.select_dtypes(include=['number'])\n",
        "\n",
        "    # Find numeric columns with null values\n",
        "    numeric_columns_with_null = numeric_columns.columns[numeric_columns.isnull().any()]\n",
        "    return numeric_columns_with_null.tolist()\n",
        "  else:\n",
        "    return df.columns[df.isnull().any()]\n",
        "\n",
        "\n",
        "def null_stats_per_column(df,chosen_column):\n",
        "  '''\n",
        "  Function to display the number of null rows in a given column of a DataFrame.\n",
        "  '''\n",
        "  print(\"null_stats_per_column START/END\")\n",
        "\n",
        "  # Count the number of null rows in the chosen column\n",
        "  null_count = df[chosen_column].isnull().sum()\n",
        "\n",
        "  # Calculate the percentage of nulls in the chosen column\n",
        "  null_percentage = (null_count / len(df)) * 100\n",
        "\n",
        "  # Display the result\n",
        "  print(f\"Column: {chosen_column}\")\n",
        "  print(f\"Null Count: {null_count}\")\n",
        "  print(f\"Null Percentage: {null_percentage:.2f}%\")\n",
        "\n",
        "\n",
        "def missing_values_histogram(df,columns):\n",
        "    '''\n",
        "    Function to plot a histogram of missing values in a given DataFrame.\n",
        "    '''\n",
        "    data=[{column:df[column].value_counts().sum()} for column in columns ]\n",
        "    # Convert the list of dictionaries to a DataFrame\n",
        "    formatted_data = pd.DataFrame([{name:count for single_data in data for name, count in single_data.items()}]).T.reset_index()\n",
        "    formatted_data.columns = ['Column', 'Count']\n",
        "    # Create a bar plot\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    ax = sns.barplot(x='Column', y='Count', data=formatted_data, hue='Column',legend=False)\n",
        "\n",
        "\n",
        "    # label count on top of each bar\n",
        "    for index, row in formatted_data.iterrows():\n",
        "        ax.text(index, row['Count'] + 10, str(row['Count']), color='black', ha=\"center\", fontsize=10)\n",
        "\n",
        "    # Add title and axis labels\n",
        "    plt.title(\"Column Counts in Dataframe\")\n",
        "    plt.xlabel(\"Column\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def encode_all_categories(df):\n",
        "  '''\n",
        "  Function to convert all string columns in a given DataFrame into categories.\n",
        "  '''\n",
        "  print(\"encode_all_categories START\")\n",
        "  for col in df.select_dtypes(['object']):\n",
        "    df[col] = df[col].astype('category')\n",
        "\n",
        "  for col in df.select_dtypes(['category']):\n",
        "    df[col] = df[col].cat.codes\n",
        "\n",
        "  print(\"encode_all_categories END\")\n",
        "\n",
        "  return df\n",
        "\n",
        "def drop_nulls(df):\n",
        "  '''\n",
        "  Function to drop all rows with null values in a given DataFrame.\n",
        "  '''\n",
        "  df = df.dropna()\n",
        "  return df\n",
        "\n",
        "def RMSLE(y_test, y_pred):\n",
        "    '''\n",
        "    RSMLE approximates the percent change\n",
        "    '''\n",
        "    return np.sqrt(np.mean((np.log(y_pred) - np.log(y_test))**2))\n",
        "\n",
        "def RMSE(y_, y_pred_):\n",
        "    '''\n",
        "    RSME\n",
        "    '''\n",
        "    return ((y_ - y_pred_) ** 2).mean() ** 0.5\n",
        "\n",
        "def train_model(df,column_to_predict,test_size_value=0.3,random_state_value=42):\n",
        "    '''\n",
        "    Function to train a Random Forest regression model on a given DataFrame.\n",
        "    '''\n",
        "    print(\"train_model START\")\n",
        "\n",
        "    X = df.drop(columns=[column_to_predict])\n",
        "    y = df[column_to_predict]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size_value, random_state=random_state_value)\n",
        "\n",
        "    # model = DecisionTreeRegressor(\n",
        "    # min_samples_leaf=16\n",
        "    # max_depth=3\n",
        "    # )\n",
        "    #model = LinearRegression()\n",
        "    model = RandomForestRegressor()\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    # 7. print the RMSE accuracy of the baseline (std dev)\n",
        "    print(\"RMSE Baseline accuracy:\", y_test.std())\n",
        "    print(\"Train RMSE:\", RMSE(y_train, y_train_pred))\n",
        "    print(\"Test RMSE:\", RMSE(y_test, y_test_pred))\n",
        "    print(\"Test RMSLE:\", RMSLE(y_test, y_test_pred))\n",
        "\n",
        "    print(\"train_model END\")\n",
        "\n",
        "    #display(pd.Series(model.feature_importances_, model.feature_names_in_).sort_values(ascending=False))\n",
        "    return model,(X_train, X_test, y_train, y_test)\n",
        "\n",
        "def feature_importances(model):\n",
        "  '''\n",
        "  Function to display the feature importances of a given Random Forest regression model.\n",
        "  '''\n",
        "  print(\"feature_importances START\")\n",
        "\n",
        "  dict(zip(model.feature_names_in_, model.feature_importances_))\n",
        "  fi = pd.Series(model.feature_importances_, index=model.feature_names_in_)\n",
        "  fi = fi.sort_values(ascending=False)\n",
        "  print(\"feature_importances END\")\n",
        "\n",
        "  return fi\n",
        "\n",
        "def explain_model_snap(model,X_test,water_fall_row=0):\n",
        "  '''\n",
        "  Function to explain the model's predictions using SHAP values.\n",
        "  '''\n",
        "  shap.initjs()\n",
        "  explainer = shap.Explainer(model)\n",
        "  explanation = explainer(X_test)  # New style\n",
        "  shap.summary_plot(explanation, X_test)\n",
        "\n",
        "  shap.plots.waterfall(explanation[water_fall_row])\n",
        "\n",
        "  shap.plots.waterfall(explanation[water_fall_row+2])\n",
        "\n",
        "  shap.plots.partial_dependence('fare', model.predict, X_test, feature_names=X_test.columns)\n",
        "\n",
        "  shap.plots.heatmap(explanation)\n",
        "\n",
        "  shap.plots.bar(explanation)\n",
        "\n",
        "\n",
        "def find_outliers(df, column_to_predict, test_size_value=0.3, random_state_value=42):\n",
        "    \"\"\"\n",
        "    Identify outliers using Isolation Forest and add anomaly-related columns to the DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    - df: Input DataFrame.\n",
        "    - column_to_predict: Column to exclude for training the Isolation Forest.\n",
        "    - test_size_value: Test size for train-test split.\n",
        "    - random_state_value: Random state for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "    - A new DataFrame with additional columns: 'anomaly_score', 'anomaly', and 'anomaly_label'.\n",
        "    \"\"\"\n",
        "    print(\"find_outliers START\")\n",
        "\n",
        "    # Create features (X) and target (y)\n",
        "    X = pd.get_dummies(df.drop(columns=[column_to_predict]), drop_first=True)\n",
        "    y = df[column_to_predict]\n",
        "\n",
        "    # Train Isolation Forest on the entire dataset (no splitting)\n",
        "    iso_forest = IsolationForest(n_estimators=100, contamination=0.01, random_state=random_state_value)\n",
        "    iso_forest.fit(X)\n",
        "\n",
        "    # Predict anomaly scores and labels\n",
        "    df = df.copy()  # Work on a copy of the DataFrame\n",
        "    df['anomaly_score'] = iso_forest.decision_function(X)  # Quantitative anomaly score\n",
        "    df['anomaly'] = iso_forest.predict(X)  # Binary anomaly label (1 for normal, -1 for anomaly)\n",
        "    df['anomaly_label'] = df['anomaly'].map({1: 'Normal', -1: 'Anomaly'})  # Map labels\n",
        "    print(\"find_outliers END\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "def convert_date_columns(df, date_column):\n",
        "    \"\"\"\n",
        "    Convert date column into multiple numeric features like year, month, day, etc.\n",
        "    Handles potential issues like missing date_column or invalid data.\n",
        "    \"\"\"\n",
        "    print(\"convert_date_columns START\")\n",
        "\n",
        "    # Ensure the date_column exists in the DataFrame\n",
        "    if date_column not in df.columns:\n",
        "        raise KeyError(f\"Column '{date_column}' does not exist in the DataFrame.\")\n",
        "\n",
        "    # Convert the column to datetime\n",
        "    df[date_column] = pd.to_datetime(df[date_column], errors='coerce')  # Handle invalid date strings\n",
        "\n",
        "    # Check if any date values are invalid after conversion\n",
        "    if df[date_column].isnull().all():\n",
        "        raise ValueError(f\"All values in '{date_column}' could not be converted to datetime.\")\n",
        "\n",
        "    # Extract date features\n",
        "    df['year'] = df[date_column].dt.year\n",
        "    df['month'] = df[date_column].dt.month\n",
        "    df['day'] = df[date_column].dt.day\n",
        "    df['day_of_week'] = df[date_column].dt.dayofweek\n",
        "    df['is_weekend'] = df['day_of_week'] >= 5  # Saturday=5, Sunday=6\n",
        "\n",
        "    # Drop the original date column\n",
        "    df = df.drop(columns=[date_column])\n",
        "\n",
        "    print(\"convert_date_columns END\")\n",
        "\n",
        "    return df\n",
        "\n",
        "#    model = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
        "#    xs=(X_train, X_test, y_train, y_test)\n",
        "def perm_importance_model(model, xs):\n",
        "    '''\n",
        "    Function to compute Permutation Importance of a given Random Forest regression model.\n",
        "    '''\n",
        "    print(\"perm_importance_model START\")\n",
        "\n",
        "    X_train, X_test, y_train, y_test = xs\n",
        "\n",
        "    # Compute Permutation Importance\n",
        "    perm_importance = permutation_importance(\n",
        "        model, X_test, y_test, n_repeats=5, random_state=42, n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # Create the results DataFrame\n",
        "    importance_model = pd.DataFrame({\n",
        "        \"Feature\": model.feature_names_in_,\n",
        "        \"Permutation Importance\": perm_importance.importances_mean,\n",
        "        \"Permutation Std Deviation\": perm_importance.importances_std,\n",
        "        \"Model Importance\": model.feature_importances_,\n",
        "    })\n",
        "\n",
        "    # Add ranking for permutation and model importances\n",
        "    importance_model[\"Permutation Rank\"] = importance_model[\"Permutation Importance\"].rank(ascending=False)\n",
        "    importance_model[\"Model Rank\"] = importance_model[\"Model Importance\"].rank(ascending=False)\n",
        "\n",
        "    # Sort by Permutation Importance for display\n",
        "    importance_model = importance_model.sort_values(by=\"Permutation Importance\", ascending=False)\n",
        "\n",
        "    # Display the DataFrame\n",
        "    display(importance_model)\n",
        "\n",
        "    print(\"perm_importance_model END\")\n",
        "\n",
        "    return importance_model\n",
        "\n",
        "def load_dataframe_from_drive():\n",
        "    \"\"\"\n",
        "    Function to load a DataFrame from a Google Drive file path.\n",
        "    \"\"\"\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Load the file from Google Drive\n",
        "    file_path = \"/content/drive/My Drive/Train.csv\"  # Replace with the saved file path\n",
        "    df = pd.read_csv(file_path)\n",
        "    return df\n",
        "\n",
        "def summarize_null_columns(dataframe):\n",
        "    \"\"\"\n",
        "    Function to find columns with null values and display detailed information\n",
        "    including column name, total null count, percentage of nulls, and data type.\n",
        "    \"\"\"\n",
        "    # Find total nulls and percentage of nulls for each column\n",
        "    columns_with_nulls = dataframe.isnull().sum()  # Total nulls per column\n",
        "    null_percentage = (columns_with_nulls / len(dataframe)) * 100  # Percentage of nulls\n",
        "\n",
        "    # Iterate through columns and display only those with null values\n",
        "    for col in dataframe.columns:\n",
        "        if columns_with_nulls[col] > 0:  # Check if the column has nulls\n",
        "            col_dtype = dataframe[col].dtype  # Get column data type\n",
        "            print(f\"Column: {col}\")\n",
        "            print(f\"  - Data Type: {col_dtype}\")\n",
        "            print(f\"  - Total Null Rows: {columns_with_nulls[col]}\")\n",
        "            print(f\"  - Percentage of Nulls: {null_percentage[col]:.2f}%\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "def drop_columns_with_high_nulls(dataframe, threshold=500):\n",
        "    \"\"\"\n",
        "    Function to drop columns from the DataFrame that have more than 'threshold' null values.\n",
        "\n",
        "    Parameters:\n",
        "    dataframe (pd.DataFrame): The input DataFrame.\n",
        "    threshold (int): The maximum allowable null rows for a column. Columns with nulls > threshold will be dropped.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: A new DataFrame with columns dropped based on the condition.\n",
        "    \"\"\"\n",
        "    print(\"drop_columns_with_high_nulls START\")\n",
        "\n",
        "    # Identify columns with null counts greater than the threshold\n",
        "    columns_to_drop = dataframe.columns[dataframe.isnull().sum() > threshold]\n",
        "\n",
        "    # Drop these columns from the DataFrame\n",
        "    cleaned_dataframe = dataframe.drop(columns=columns_to_drop)\n",
        "\n",
        "    print(\"drop_columns_with_high_nulls END\")\n",
        "\n",
        "    #print(f\"Dropped columns: {list(columns_to_drop)}\")\n",
        "    return cleaned_dataframe\n",
        "\n",
        "def update_MachineHoursCurrentMeter(df):\n",
        "  print(\"update_MachineHoursCurrentMeter START\")\n",
        "\n",
        "  # Update the 'MachineHoursCurrentMeter' column: replace NaN or 0 with 0\n",
        "  column_name = 'MachineHoursCurrentMeter'\n",
        "\n",
        "  # Check if the column exists\n",
        "  if column_name in df.columns:\n",
        "      df[column_name] = df[column_name].fillna(0)  # Replace NaN with 0\n",
        "      df[column_name] = df[column_name].replace(0, 0)  # Ensure 0 stays 0\n",
        "      #print(f\"Updated '{column_name}' column successfully!\")\n",
        "  #else:\n",
        "      #print(f\"Column '{column_name}' does not exist in the DataFrame.\")\n",
        "\n",
        "  print(\"update_MachineHoursCurrentMeter END\")\n",
        "\n",
        "  return df\n",
        "\n",
        "def update_auctioneerID(df):\n",
        "  print(\"update_auctioneerID START\")\n",
        "\n",
        "  # Assuming 'df' is your DataFrame\n",
        "  # Fill null values in the 'auctioneerID' column with 100.0\n",
        "  if 'auctioneerID' in df.columns:\n",
        "      df['auctioneerID'] = df['auctioneerID'].fillna(100.0)\n",
        "      #print(\"'auctioneerID' null values filled with 100.0 successfully!\")\n",
        "  #else:\n",
        "  #    print(\"Column 'auctioneerID' does not exist in the DataFrame.\")\n",
        "  print(\"update_auctioneerID END\")\n",
        "\n",
        "  return df\n",
        "\n",
        "def update_Enclosure(df):\n",
        "  print(\"update_Enclosure START\")\n",
        "\n",
        "  # Replace null values in the 'Enclosure' column with 'N/A'\n",
        "  if 'Enclosure' in df.columns:\n",
        "      df['Enclosure'] = df['Enclosure'].fillna('N/A')\n",
        "      #print(\"Replaced null values in the 'Enclosure' column with 'N/A'.\")\n",
        "  #else:\n",
        "      #print(\"Column 'Enclosure' does not exist in the DataFrame.\")\n",
        "\n",
        "  print(\"update_Enclosure END\")\n",
        "\n",
        "  return df\n",
        "'''\n",
        "# Function to calculate YearMade based on ModelID\n",
        "def calc_YearMade(df,model_id):\n",
        "    valid_years = df.loc[(df['ModelID'] == model_id) & (df['YearMade'] > 1000), 'YearMade']\n",
        "    return int(valid_years.mean()) if not valid_years.empty else 1000  # Default if no valid years exist\n",
        "'''\n",
        "# Function to calculate YearMade based on ModelID\n",
        "def calc_YearMade(df, model_id):\n",
        "    valid_years = df.loc[(df['ModelID'] == model_id) & (df['YearMade'] > 1000), 'YearMade']\n",
        "    return int(valid_years.mean()) if not valid_years.empty else 1000  # Default if no valid years exist\n",
        "\n",
        "\n",
        "def count_total_rows_per_column(df,column_name):\n",
        "  return df[df[column_name] == 1000].shape[0]\n",
        "\n",
        "\n",
        "#First run OA\n",
        "def first_run_25012025():\n",
        "  df=load_dataframe_from_drive()\n",
        "\n",
        "  missing_values_histogram(df,df.columns)\n",
        "\n",
        "  #df.info()\n",
        "\n",
        "  #df.describe()\n",
        "\n",
        "  df=update_MachineHoursCurrentMeter(df)\n",
        "  df=update_auctioneerID(df)\n",
        "\n",
        "  #print(\"columns with null values:\", columns_with_nulls(df,False))\n",
        "  #summarize_null_columns(df)\n",
        "\n",
        "  df = drop_columns_with_high_nulls(df, threshold=500)\n",
        "  df=update_Enclosure(df)\n",
        "  df=convert_date_columns(df,'saledate')\n",
        "  df=encode_all_categories(df)\n",
        "  model,xs=train_model(df=df,column_to_predict='SalePrice')#took 8 min runtime\n",
        "  return model,xs\n",
        "\n",
        "#Second run OA remove cols and outliners\n",
        "def Second_run_26012025():\n",
        "  df=load_dataframe_from_drive()\n",
        "  df=update_MachineHoursCurrentMeter(df)\n",
        "  df=update_auctioneerID(df)\n",
        "  df = drop_columns_with_high_nulls(df, threshold=500)\n",
        "  df=update_Enclosure(df)\n",
        "  df=convert_date_columns(df,'saledate')\n",
        "  df=encode_all_categories(df)\n",
        "\n",
        "  sampled_df = df.sample(n=25000, random_state=42)  # Example: 10,000 rows\n",
        "\n",
        "  model,xs=train_model(df=sampled_df,column_to_predict='SalePrice')\n",
        "\n",
        "  feature_importances(model)\n",
        "\n",
        "  sampled_df = sampled_df.drop(columns=['is_weekend','datasource','MachineHoursCurrentMeter','day_of_week','auctioneerID'], errors='ignore')  # 'errors=\"ignore\"' prevents errors if columns don't exist\n",
        "\n",
        "  model,xs=train_model(df=sampled_df,column_to_predict='SalePrice')\n",
        "\n",
        "  sampled_df=find_outliers(df=sampled_df,column_to_predict='SalePrice')\n",
        "\n",
        "  # Assuming df_with_anomalies is the DataFrame containing anomaly labels\n",
        "  anomalies = sampled_df[sampled_df['anomaly_label'] == 'Anomaly']\n",
        "\n",
        "  # Display rows where 'anomaly_label' is 'Anomaly'\n",
        "  print(anomalies)\n",
        "\n",
        "  # Optionally, display the count of anomalies\n",
        "  print(f\"Number of anomalies detected: {len(anomalies)}\")\n",
        "\n",
        "  # Remove all rows where 'anomaly_label' is 'Anomaly'\n",
        "  sampled_df = sampled_df[sampled_df['anomaly_label'] != 'Anomaly']\n",
        "\n",
        "  # Drop the columns 'anomaly_label', 'anomaly_score', and 'anomaly'\n",
        "  columns_to_drop = ['anomaly_label', 'anomaly_score', 'anomaly']\n",
        "  sampled_df = sampled_df.drop(columns=columns_to_drop, errors='ignore')\n",
        "\n",
        "  model,xs=train_model(df=sampled_df,column_to_predict='SalePrice')\n",
        "\n",
        "  perm_importance_model(model,xs)\n",
        "\n",
        "  sampled_df = sampled_df.drop(columns=['day','month','state','MachineID','SalesID','ProductGroupDesc'], errors='ignore')  # 'errors=\"ignore\"' prevents errors if columns don't exist\n",
        "\n",
        "  model,xs=train_model(df=sampled_df,column_to_predict='SalePrice')\n",
        "\n",
        "  perm_importance_model(model,xs)\n",
        "\n",
        "  return model,xs\n",
        "\n",
        "#First run OA 27012025\n",
        "def First_run_27012025():\n",
        "  df=load_dataframe_from_drive()\n",
        "\n",
        "  df=update_NaN_To_None_or_Unspecified(df)\n",
        "\n",
        "  df=update_MachineHoursCurrentMeter(df)\n",
        "  df=update_auctioneerID(df)\n",
        "  df = drop_columns_with_high_nulls(df, threshold=500)\n",
        "  df=update_Enclosure(df)\n",
        "  df=convert_date_columns(df,'saledate')\n",
        "  df=encode_all_categories(df)\n",
        "\n",
        "  sampled_df = df.sample(n=25000, random_state=42)  # Example: 10,000 rows\n",
        "\n",
        "  model,xs=train_model(df=sampled_df,column_to_predict='SalePrice')\n",
        "\n",
        "  feature_importances(model)\n",
        "\n",
        "  sampled_df = sampled_df.drop(columns=['is_weekend','datasource','MachineHoursCurrentMeter','day_of_week','auctioneerID'], errors='ignore')  # 'errors=\"ignore\"' prevents errors if columns don't exist\n",
        "\n",
        "  model,xs=train_model(df=sampled_df,column_to_predict='SalePrice')\n",
        "\n",
        "  sampled_df=find_outliers(df=sampled_df,column_to_predict='SalePrice')\n",
        "\n",
        "  # Assuming df_with_anomalies is the DataFrame containing anomaly labels\n",
        "  anomalies = sampled_df[sampled_df['anomaly_label'] == 'Anomaly']\n",
        "\n",
        "  # Display rows where 'anomaly_label' is 'Anomaly'\n",
        "  print(anomalies)\n",
        "\n",
        "  # Optionally, display the count of anomalies\n",
        "  print(f\"Number of anomalies detected: {len(anomalies)}\")\n",
        "\n",
        "  # Remove all rows where 'anomaly_label' is 'Anomaly'\n",
        "  sampled_df = sampled_df[sampled_df['anomaly_label'] != 'Anomaly']\n",
        "\n",
        "  # Drop the columns 'anomaly_label', 'anomaly_score', and 'anomaly'\n",
        "  columns_to_drop = ['anomaly_label', 'anomaly_score', 'anomaly']\n",
        "  sampled_df = sampled_df.drop(columns=columns_to_drop, errors='ignore')\n",
        "\n",
        "  model,xs=train_model(df=sampled_df,column_to_predict='SalePrice')\n",
        "\n",
        "  perm_importance_model(model,xs)\n",
        "\n",
        "  sampled_df = sampled_df.drop(columns=['day','month','state','MachineID','SalesID','ProductGroupDesc'], errors='ignore')  # 'errors=\"ignore\"' prevents errors if columns don't exist\n",
        "\n",
        "  model,xs=train_model(df=sampled_df,column_to_predict='SalePrice')\n",
        "\n",
        "  importance_model=perm_importance_model(model,xs)\n",
        "\n",
        "  # Drop columns where 'Permutation Importance' is less than 0.02\n",
        "  importance_model_filtered = importance_model[importance_model['Permutation Importance'] >= 0.02]\n",
        "\n",
        "  # Display the filtered DataFrame\n",
        "  print(importance_model_filtered)\n",
        "\n",
        "  # Optionally, list the dropped columns\n",
        "  dropped_columns = importance_model[importance_model['Permutation Importance'] < 0.02]['Feature']\n",
        "  print(\"Dropped columns:\", dropped_columns.tolist())\n",
        "  sampled_df.drop(columns=dropped_columns, inplace=True)\n",
        "\n",
        "  model,xs=train_model(df=sampled_df,column_to_predict='SalePrice')\n",
        "\n",
        "  return model,xs\n",
        "\n",
        "\n",
        "#First run OA 27012025\n",
        "def First_run_02022025():\n",
        "  pd.set_option('display.expand_frame_repr', False)\n",
        "\n",
        "  df=load_dataframe_from_drive()\n",
        "  print(count_total_rows_per_column(df,'YearMade'))\n",
        "  # Update YearMade where it is 1000\n",
        "  df.loc[df['YearMade'] == 1000, 'YearMade'] = df.loc[df['YearMade'] == 1000, 'ModelID'].apply(calc_YearMade)\n",
        "  print(count_total_rows_per_column(df,'YearMade'))\n",
        "\n",
        "  df=update_NaN_To_None_or_Unspecified(df)\n",
        "\n",
        "  df=update_MachineHoursCurrentMeter(df)\n",
        "  df=update_auctioneerID(df)\n",
        "  df = drop_columns_with_high_nulls(df, threshold=500)\n",
        "  df=update_Enclosure(df)\n",
        "  df=convert_date_columns(df,'saledate')\n",
        "  df=encode_all_categories(df)\n",
        "\n",
        "  sampled_df = df.sample(n=25000, random_state=42)  # Example: 10,000 rows\n",
        "\n",
        "  model,xs=train_model(df=sampled_df,column_to_predict='SalePrice')\n",
        "\n",
        "  feature_importances(model)\n",
        "\n",
        "  sampled_df = sampled_df.drop(columns=['is_weekend','datasource','MachineHoursCurrentMeter','day_of_week','auctioneerID'], errors='ignore')  # 'errors=\"ignore\"' prevents errors if columns don't exist\n",
        "\n",
        "  model,xs=train_model(df=sampled_df,column_to_predict='SalePrice')\n",
        "\n",
        "  sampled_df=find_outliers(df=sampled_df,column_to_predict='SalePrice')\n",
        "\n",
        "  # Assuming df_with_anomalies is the DataFrame containing anomaly labels\n",
        "  anomalies = sampled_df[sampled_df['anomaly_label'] == 'Anomaly']\n",
        "\n",
        "  # Display rows where 'anomaly_label' is 'Anomaly'\n",
        "  print(anomalies)\n",
        "\n",
        "  # Optionally, display the count of anomalies\n",
        "  print(f\"Number of anomalies detected: {len(anomalies)}\")\n",
        "\n",
        "  # Remove all rows where 'anomaly_label' is 'Anomaly'\n",
        "  sampled_df = sampled_df[sampled_df['anomaly_label'] != 'Anomaly']\n",
        "\n",
        "  # Drop the columns 'anomaly_label', 'anomaly_score', and 'anomaly'\n",
        "  columns_to_drop = ['anomaly_label', 'anomaly_score', 'anomaly']\n",
        "  sampled_df = sampled_df.drop(columns=columns_to_drop, errors='ignore')\n",
        "\n",
        "  model,xs=train_model(df=sampled_df,column_to_predict='SalePrice')\n",
        "\n",
        "  perm_importance_model(model,xs)\n",
        "\n",
        "  sampled_df = sampled_df.drop(columns=['day','month','state','MachineID','SalesID','ProductGroupDesc'], errors='ignore')  # 'errors=\"ignore\"' prevents errors if columns don't exist\n",
        "\n",
        "  model,xs=train_model(df=sampled_df,column_to_predict='SalePrice')\n",
        "\n",
        "  importance_model=perm_importance_model(model,xs)\n",
        "\n",
        "  # Drop columns where 'Permutation Importance' is less than 0.02\n",
        "  importance_model_filtered = importance_model[importance_model['Permutation Importance'] >= 0.02]\n",
        "\n",
        "  # Display the filtered DataFrame\n",
        "  print(importance_model_filtered)\n",
        "\n",
        "  # Optionally, list the dropped columns\n",
        "  dropped_columns = importance_model[importance_model['Permutation Importance'] < 0.02]['Feature']\n",
        "  print(\"Dropped columns:\", dropped_columns.tolist())\n",
        "  sampled_df.drop(columns=dropped_columns, inplace=True)\n",
        "\n",
        "  model,xs=train_model(df=sampled_df,column_to_predict='SalePrice')\n",
        "\n",
        "  return model,xs\n",
        "\n",
        "def prepare_dataframe(df_pre:pd.DataFrame):\n",
        "  # Update YearMade where it is 1000\n",
        "  df_pre.loc[df_pre['YearMade'] == 1000, 'YearMade'] = df_pre.loc[df_pre['YearMade'] == 1000, 'ModelID'].apply(lambda model_id: calc_YearMade(df_pre, model_id))\n",
        "  # Compute the mean excluding rows where YearMade == 1000\n",
        "  mean_yearmade = df_pre.loc[df_pre['YearMade'] != 1000, 'YearMade'].mean()\n",
        "\n",
        "  # Update all rows where YearMade == 1000 with the calculated mean\n",
        "  df_pre.loc[df_pre['YearMade'] == 1000, 'YearMade'] = mean_yearmade\n",
        "\n",
        "  df_pre=update_NaN_To_None_or_Unspecified(df_pre)\n",
        "  df_pre=encode_all_categories(df_pre)\n",
        "  df_pre['SalePrice']=np.log(df_pre['SalePrice'])\n",
        "  '''\n",
        "  df_pre=update_MachineHoursCurrentMeter(df_pre)\n",
        "  df_pre=update_auctioneerID(df_pre)\n",
        "  df_pre = drop_columns_with_high_nulls(df_pre, threshold=500)\n",
        "  df_pre=update_Enclosure(df_pre)\n",
        "  df_pre=convert_date_columns(df_pre,'saledate')\n",
        "  df_pre=encode_all_categories(df_pre)\n",
        "  df_pre = df_pre.drop(columns=['is_weekend','datasource','MachineHoursCurrentMeter','day_of_week','auctioneerID'], errors='ignore')  # 'errors=\"ignore\"' prevents errors if columns don't exist\n",
        "  '''\n",
        "  model,xs=train_model(df=df_pre,column_to_predict='SalePrice')\n",
        "\n",
        "\n",
        "  #df_pre=find_outliers(df=df_pre,column_to_predict='SalePrice')\n",
        "\n",
        "  # Assuming df_with_anomalies is the DataFrame containing anomaly labels\n",
        "  #anomalies = df_pre[df_pre['anomaly_label'] == 'Anomaly']\n",
        "  # Remove all rows where 'anomaly_label' is 'Anomaly'\n",
        "  #df_pre = df_pre[df_pre['anomaly_label'] != 'Anomaly']\n",
        "\n",
        "  # Drop the columns 'anomaly_label', 'anomaly_score', and 'anomaly'\n",
        "  #columns_to_drop = ['anomaly_label', 'anomaly_score', 'anomaly']\n",
        "  #df_pre = df_pre.drop(columns=columns_to_drop, errors='ignore')\n",
        "\n",
        "\n",
        "  df_pre = df_pre.drop(columns=['day','month','state','MachineID','SalesID','ProductGroupDesc'], errors='ignore')  # 'errors=\"ignore\"' prevents errors if columns don't exist\n",
        "  #dropped_columns = ['UsageBand', 'fiSecondaryDesc', 'fiModelSeries', 'fiModelDescriptor', 'ProductSize', 'Drive_System', 'Stick', 'Engine_Horsepower', 'Track_Type', 'Grouser_Type', 'Differential_Type', 'Steering_Controls']\n",
        "  #df_pre.drop(columns=dropped_columns, inplace=True)\n",
        "\n",
        "  return df_pre\n",
        "\n",
        "def test_model(model):\n",
        "  pd.set_option('display.expand_frame_repr', False)\n",
        "\n",
        "  file_path = \"/content/drive/My Drive/Valid.csv\"  # Replace with the saved file path\n",
        "  df_valid = pd.read_csv(file_path)\n",
        "  df_valid['SalePrice']=None\n",
        "  print(df_valid.columns)\n",
        "  '''\n",
        "  df_valid=prepare_dataframe(df_valid)\n",
        "  print(df_valid.columns)\n",
        "\n",
        "  X = df_valid.drop(columns=['SalePrice'])\n",
        "  y = df_valid['SalePrice']\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "  y_test_pred = model.predict(X_test)\n",
        "  # 7. print the RMSE accuracy of the baseline (std dev)\n",
        "  print(\"Test RMSE:\", RMSE(y_test, y_test_pred))\n",
        "  df_valid.columns\n",
        "  df_valid[['SalesID', 'SalePrice']].to_csv(\"/path/to/output.csv\", index=False)\n",
        "  '''\n",
        "\n",
        "#TEST run OA 02022025\n",
        "def TEST_run_02022025():\n",
        "  pd.set_option('display.expand_frame_repr', False)\n",
        "  df=load_dataframe_from_drive()\n",
        "  df = df.sample(n=10000, random_state=42)\n",
        "  df=prepare_dataframe(df)\n",
        "  model,xs=train_model(df=df,column_to_predict='SalePrice')\n",
        "\n",
        "  return model,xs\n",
        "\n",
        "\n",
        "'''\n",
        "import pandas as pd\n",
        "\n",
        "# Compute correlation matrix\n",
        "corr_matrix = df.corr()\n",
        "\n",
        "# Find highly correlated features (e.g., correlation > 0.85)\n",
        "high_corr_features = set()\n",
        "threshold = 0.85\n",
        "\n",
        "for i in range(len(corr_matrix.columns)):\n",
        "    for j in range(i):\n",
        "        if abs(corr_matrix.iloc[i, j]) > threshold:\n",
        "            colname = corr_matrix.columns[i]\n",
        "            high_corr_features.add(colname)\n",
        "\n",
        "print(\"Highly correlated features:\", high_corr_features)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "corr_matrix = df.corr()\n",
        "threshold = 0.60\n",
        "\n",
        "# Apply threshold filter to only show correlations above 0.5\n",
        "filtered_corr = corr_matrix[(abs(corr_matrix) > threshold) & (abs(corr_matrix) < 1)]\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(filtered_corr, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "plt.title(\"Example Correlation Heatmap\")\n",
        "plt.show()\n",
        "'''"
      ],
      "metadata": {
        "id": "GetwQ5yXBMFX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "outputId": "4c772e34-6a30-42cb-f9cf-b94027871eaa",
        "collapsed": true
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport pandas as pd\\n\\n# Compute correlation matrix\\ncorr_matrix = df.corr()\\n\\n# Find highly correlated features (e.g., correlation > 0.85)\\nhigh_corr_features = set()\\nthreshold = 0.85\\n\\nfor i in range(len(corr_matrix.columns)):\\n    for j in range(i):\\n        if abs(corr_matrix.iloc[i, j]) > threshold:\\n            colname = corr_matrix.columns[i]\\n            high_corr_features.add(colname)\\n\\nprint(\"Highly correlated features:\", high_corr_features)\\n\\nimport pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\ncorr_matrix = df.corr()\\nthreshold = 0.60\\n\\n# Apply threshold filter to only show correlations above 0.5\\nfiltered_corr = corr_matrix[(abs(corr_matrix) > threshold) & (abs(corr_matrix) < 1)]\\n\\n# Plot heatmap\\nplt.figure(figsize=(10, 6))\\nsns.heatmap(filtered_corr, annot=True, cmap=\\'coolwarm\\', fmt=\".2f\", linewidths=0.5)\\nplt.title(\"Example Correlation Heatmap\")\\nplt.show()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "\n",
        "#erez functions1\n",
        "\n",
        "def add_age_machine(df):\n",
        "    # Ensure 'saledate' is in datetime format\n",
        "    df['saledate'] = pd.to_datetime(df['saledate'], errors='coerce')\n",
        "\n",
        "    # Extract the year from 'saledate'\n",
        "    df['saledate_year'] = df['saledate'].dt.year\n",
        "\n",
        "    # Calculate the machine's age\n",
        "    df['age_machine'] = df['saledate_year'] - df['YearMade']\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply the function\n",
        "df = add_age_machine(df)\n",
        "\n",
        "# Display the DataFrame with the new 'age_machine' column\n",
        "df.info()\n",
        "print(df[['YearMade', 'saledate', 'age_machine']])\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "ElpLtGhC3zSk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "fe3ccc15-6b31-45c5-ce0b-7ff58ddf4e1c",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-5fa7547edc22>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Apply the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_age_machine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Display the DataFrame with the new 'age_machine' column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "#erez functions2\n",
        "for col in df.select_dtypes(include=['object']).columns:\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "# Replace NaN with 0\n",
        "df[col] = df[col].fillna(0).astype(int)\n",
        "\n",
        "# Or, drop rows with NaN values\n",
        "df.dropna(subset=[col], inplace=True)\n",
        "\n",
        "df[col] = df[col].replace({'\\$': '', ',': ''}, regex=True).astype(float)\n",
        "\n",
        "for col in df.select_dtypes(include=['object']).columns:\n",
        "    if df[col].str.isnumeric().all():  # Check if all values are numeric\n",
        "        df[col] = df[col].astype(int)\n",
        "    else:\n",
        "        print(f\"Cannot convert column {col} to int\")\n",
        "\n",
        "# Convert all object columns that can be converted to integers\n",
        "for col in df.select_dtypes(include=['object']).columns:\n",
        "    try:\n",
        "        df[col] = df[col].astype(int)\n",
        "    except ValueError:\n",
        "        print(f\"Cannot convert column {col} to int\")\n",
        "\n",
        "def robust_convert(df):\n",
        "    for col in df.select_dtypes(include=['object']).columns:\n",
        "        try:\n",
        "            # Step 1: Remove non-numeric symbols (if any, e.g., '$')\n",
        "            df[col] = df[col].replace({'\\$': '', ',': '', '%': ''}, regex=True)\n",
        "\n",
        "            # Step 2: Convert to numeric, coerce errors to NaN\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "            # Step 3: Handle NaN values (e.g., replace with 0 or mean of column)\n",
        "            df[col] = df[col].fillna(0).astype(int)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to convert column {col}: {e}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply the robust conversion\n",
        "df = robust_convert(df)\n",
        "df.info()\n",
        "\n",
        "# Separate numerical columns and non-numerical columns\n",
        "numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "non_numerical_cols = df.select_dtypes(exclude=['float64', 'int64']).columns\n",
        "\n",
        "# Optionally, convert datetime columns to numerical values\n",
        "if 'saledate' in non_numerical_cols:\n",
        "    df['saledate'] = (df['saledate'] - df['saledate'].min()) / pd.Timedelta(days=1)\n",
        "\n",
        "# Now apply KNNImputer only on numerical columns\n",
        "imputer = KNNImputer(n_neighbors=5)\n",
        "\n",
        "# Make sure that you're passing the correct DataFrame subset\n",
        "numerical_data = df[numerical_cols].copy()  # Avoid modifying the original DataFrame directly\n",
        "\n",
        "# Perform imputation\n",
        "imputed_data = imputer.fit_transform(numerical_data)\n",
        "\n",
        "# Convert imputed data back to DataFrame and ensure column names match\n",
        "df_imputed = pd.DataFrame(imputed_data, columns=numerical_cols)\n",
        "\n",
        "# Replace the original columns with the imputed ones\n",
        "df[numerical_cols] = df_imputed\n",
        "\n",
        "# For categorical columns, you can impute using the mode\n",
        "for col in non_numerical_cols:\n",
        "    if df[col].dtype == 'object':\n",
        "        df[col].fillna(df[col].mode()[0], inplace=True)\n",
        "\n",
        "# If you converted datetime columns, convert them back to datetime format\n",
        "df['saledate'] = pd.to_datetime(df['saledate'], unit='D', origin=pd.Timestamp('2020-01-01'))\n",
        "\n",
        "# Check for any remaining null values\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tZ9_sbuorTTI",
        "outputId": "5c7cd740-5fed-46ed-df50-7d2f07366f5a",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 401125 entries, 0 to 401124\n",
            "Data columns (total 53 columns):\n",
            " #   Column                    Non-Null Count   Dtype  \n",
            "---  ------                    --------------   -----  \n",
            " 0   SalesID                   401125 non-null  int64  \n",
            " 1   SalePrice                 401125 non-null  int64  \n",
            " 2   MachineID                 401125 non-null  int64  \n",
            " 3   ModelID                   401125 non-null  int64  \n",
            " 4   datasource                401125 non-null  int64  \n",
            " 5   auctioneerID              380989 non-null  float64\n",
            " 6   YearMade                  401125 non-null  int64  \n",
            " 7   MachineHoursCurrentMeter  142765 non-null  float64\n",
            " 8   UsageBand                 0 non-null       float64\n",
            " 9   saledate                  401125 non-null  int32  \n",
            " 10  fiModelDesc               61647 non-null   float64\n",
            " 11  fiBaseModel               252195 non-null  float64\n",
            " 12  fiSecondaryDesc           292 non-null     float64\n",
            " 13  fiModelSeries             25528 non-null   float64\n",
            " 14  fiModelDescriptor         8879 non-null    float64\n",
            " 15  ProductSize               0 non-null       float64\n",
            " 16  fiProductClassDesc        0 non-null       float64\n",
            " 17  state                     0 non-null       float64\n",
            " 18  ProductGroup              0 non-null       float64\n",
            " 19  ProductGroupDesc          0 non-null       float64\n",
            " 20  Drive_System              0 non-null       float64\n",
            " 21  Enclosure                 0 non-null       float64\n",
            " 22  Forks                     0 non-null       float64\n",
            " 23  Pad_Type                  0 non-null       float64\n",
            " 24  Ride_Control              0 non-null       float64\n",
            " 25  Stick                     0 non-null       float64\n",
            " 26  Transmission              0 non-null       float64\n",
            " 27  Turbocharged              0 non-null       float64\n",
            " 28  Blade_Extension           0 non-null       float64\n",
            " 29  Blade_Width               0 non-null       float64\n",
            " 30  Enclosure_Type            0 non-null       float64\n",
            " 31  Engine_Horsepower         0 non-null       float64\n",
            " 32  Hydraulics                0 non-null       float64\n",
            " 33  Pushblock                 0 non-null       float64\n",
            " 34  Ripper                    0 non-null       float64\n",
            " 35  Scarifier                 0 non-null       float64\n",
            " 36  Tip_Control               0 non-null       float64\n",
            " 37  Tire_Size                 35447 non-null   float64\n",
            " 38  Coupler                   0 non-null       float64\n",
            " 39  Coupler_System            0 non-null       float64\n",
            " 40  Grouser_Tracks            0 non-null       float64\n",
            " 41  Hydraulics_Flow           0 non-null       float64\n",
            " 42  Track_Type                0 non-null       float64\n",
            " 43  Undercarriage_Pad_Width   0 non-null       float64\n",
            " 44  Stick_Length              0 non-null       float64\n",
            " 45  Thumb                     0 non-null       float64\n",
            " 46  Pattern_Changer           0 non-null       float64\n",
            " 47  Grouser_Type              0 non-null       float64\n",
            " 48  Backhoe_Mounting          0 non-null       float64\n",
            " 49  Blade_Type                0 non-null       float64\n",
            " 50  Travel_Controls           0 non-null       float64\n",
            " 51  Differential_Type         0 non-null       float64\n",
            " 52  Steering_Controls         401125 non-null  float64\n",
            "dtypes: float64(46), int32(1), int64(6)\n",
            "memory usage: 160.7 MB\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UFuncTypeError",
          "evalue": "ufunc 'divide' cannot use operands with types dtype('int32') and dtype('<m8[ns]')",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-f263f99d8c8a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# Optionally, convert datetime columns to numerical values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'saledate'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnon_numerical_cols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'saledate'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'saledate'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'saledate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdays\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# Now apply KNNImputer only on numerical columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/ops/common.py\u001b[0m in \u001b[0;36mnew_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py\u001b[0m in \u001b[0;36m__truediv__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__truediv__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__truediv__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_arith_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruediv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__rtruediv__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_arith_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6134\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_align_for_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6135\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndexOpsMixin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_arith_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_align_for_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign_asobject\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   1380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marithmetic_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36marithmetic_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0;31m# Timedelta/Timestamp and other custom scalars are included in the check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;31m# because numexpr will fail on it, see GH#31457\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;31m# TODO we should handle EAs consistently and move this check before the if/else\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mtimedeltas.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.timedeltas.Timedelta.__rtruediv__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mUFuncTypeError\u001b[0m: ufunc 'divide' cannot use operands with types dtype('int32') and dtype('<m8[ns]')"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "#eeitan function\n",
        "def calculate_machine_age(df):\n",
        "    # Ensure that 'SaleDate' is in datetime format\n",
        "    df['saledate'] = pd.to_datetime(df['saledate'])\n",
        "    # Extract the year from 'SaleDate'\n",
        "    df['saledate'] = df['saledate'].dt.year\n",
        "\n",
        "    # Calculate MachineAge by subtracting 'YearMade' from the 'SaleYear'\n",
        "    df['machineage'] = df['saleyear'] - df['yearmade']\n",
        "\n",
        "    # Drop the 'SaleYear' column if you don't need it anymore\n",
        "    df.drop(columns=['saleyear'], inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "df=load_dataframe_from_drive()\n",
        "df['saledate']\n",
        "# Example usage:\n",
        "# Assuming your dataframe is named `df`\n",
        "df = calculate_machine_age(df)\n",
        "\n",
        "# Display the dataframe with the new 'MachineAge' column\n",
        "print(df[['YearMade', 'SaleDate', 'MachineAge']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        },
        "collapsed": true,
        "id": "r6_vim-k4By6",
        "outputId": "54a592ee-edb7-47f2-e3be-83c1396f0296",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-c3887af4472d>:314: DtypeWarning: Columns (13,39,40,41) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'saleyear'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'saleyear'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-40962fc4db68>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Example usage:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Assuming your dataframe is named `df`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_machine_age\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Display the dataframe with the new 'MachineAge' column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-40962fc4db68>\u001b[0m in \u001b[0;36mcalculate_machine_age\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Calculate MachineAge by subtracting 'YearMade' from the 'SaleYear'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'machineage'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'saleyear'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'yearmade'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Drop the 'SaleYear' column if you don't need it anymore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'saleyear'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sorel functions"
      ],
      "metadata": {
        "id": "1EojaDZG4LoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model,xs=train_model(df=df,column_to_predict='SalePrice')#took 8 min runtime"
      ],
      "metadata": {
        "id": "rjC7CgKHEDBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RMSE Baseline accuracy: 22932.4005340408 Train RMSE: 2864.123815046 Test RMSE: 7633.339248575835 0 Enclosure 0.240869 YearMade 0.147308 fiProductClassDesc 0.117304 fiModelDesc 0.099289 ModelID 0.093105 year 0.080002 SalesID 0.043689 fiBaseModel 0.033948 ProductGroup 0.032850 ProductGroupDesc 0.029777 MachineID 0.023082 day 0.013131 month 0.013108 state 0.012552 auctioneerID 0.006424 day_of_week 0.006194 MachineHoursCurrentMeter 0.005897 datasource 0.000885 is_weekend 0.000586"
      ],
      "metadata": {
        "id": "9X3AVb8dEH69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ofer runtime\n",
        "#model,xs=first_run_25012025()\n",
        "#model,xs=Second_run_26012025()\n",
        "#model,xs=First_run_27012025()\n",
        "#model,xs=First_run_02022025()\n",
        "#model,xs=TEST_run_02022025()\n",
        "print('END TRAIN')\n",
        "test_model(None)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YWjBrHg34pvE",
        "outputId": "2ca6397a-a5d2-41ae-be21-fe238d8bcc64"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-90-fe1f36c97aa4>:341: DtypeWarning: Columns (13,39,40,41) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path)\n",
            "<ipython-input-90-fe1f36c97aa4>:667: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1993.4933172811538' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  df_pre.loc[df_pre['YearMade'] == 1000, 'YearMade'] = mean_yearmade\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "update_NaN_To_None_or_Unspecified START\n",
            "update_NaN_To_None_or_Unspecified END\n",
            "encode_all_categories START\n",
            "encode_all_categories END\n",
            "train_model START\n",
            "RMSE Baseline accuracy: 0.6937168026277001\n",
            "Train RMSE: 0.09816112898278556\n",
            "Test RMSE: 0.2618562930094179\n",
            "Test RMSLE: 0.025977565453372514\n",
            "train_model END\n",
            "train_model START\n",
            "RMSE Baseline accuracy: 0.6937168026277001\n",
            "Train RMSE: 0.1013715759598807\n",
            "Test RMSE: 0.2650753148503571\n",
            "Test RMSLE: 0.02630527015691445\n",
            "train_model END\n",
            "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
            "Index(['SalesID', 'MachineID', 'ModelID', 'datasource', 'auctioneerID',\n",
            "       'YearMade', 'MachineHoursCurrentMeter', 'UsageBand', 'saledate',\n",
            "       'fiModelDesc', 'fiBaseModel', 'fiSecondaryDesc', 'fiModelSeries',\n",
            "       'fiModelDescriptor', 'ProductSize', 'fiProductClassDesc', 'state',\n",
            "       'ProductGroup', 'ProductGroupDesc', 'Drive_System', 'Enclosure',\n",
            "       'Forks', 'Pad_Type', 'Ride_Control', 'Stick', 'Transmission',\n",
            "       'Turbocharged', 'Blade_Extension', 'Blade_Width', 'Enclosure_Type',\n",
            "       'Engine_Horsepower', 'Hydraulics', 'Pushblock', 'Ripper', 'Scarifier',\n",
            "       'Tip_Control', 'Tire_Size', 'Coupler', 'Coupler_System',\n",
            "       'Grouser_Tracks', 'Hydraulics_Flow', 'Track_Type',\n",
            "       'Undercarriage_Pad_Width', 'Stick_Length', 'Thumb', 'Pattern_Changer',\n",
            "       'Grouser_Type', 'Backhoe_Mounting', 'Blade_Type', 'Travel_Controls',\n",
            "       'Differential_Type', 'Steering_Controls', 'SalePrice'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-90-fe1f36c97aa4>:667: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1998.938827677391' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  df_pre.loc[df_pre['YearMade'] == 1000, 'YearMade'] = mean_yearmade\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "update_NaN_To_None_or_Unspecified START\n",
            "update_NaN_To_None_or_Unspecified END\n",
            "encode_all_categories START\n",
            "encode_all_categories END\n",
            "train_model START\n",
            "RMSE Baseline accuracy: 2.664919059643828e-13\n",
            "Train RMSE: 2.575717417130363e-13\n",
            "Test RMSE: 2.575717417130363e-13\n",
            "Test RMSLE: 5.595524044110789e-14\n",
            "train_model END\n",
            "Index(['ModelID', 'datasource', 'auctioneerID', 'YearMade',\n",
            "       'MachineHoursCurrentMeter', 'UsageBand', 'saledate', 'fiModelDesc',\n",
            "       'fiBaseModel', 'fiSecondaryDesc', 'fiModelSeries', 'fiModelDescriptor',\n",
            "       'ProductSize', 'fiProductClassDesc', 'ProductGroup', 'Drive_System',\n",
            "       'Enclosure', 'Forks', 'Pad_Type', 'Ride_Control', 'Stick',\n",
            "       'Transmission', 'Turbocharged', 'Blade_Extension', 'Blade_Width',\n",
            "       'Enclosure_Type', 'Engine_Horsepower', 'Hydraulics', 'Pushblock',\n",
            "       'Ripper', 'Scarifier', 'Tip_Control', 'Tire_Size', 'Coupler',\n",
            "       'Coupler_System', 'Grouser_Tracks', 'Hydraulics_Flow', 'Track_Type',\n",
            "       'Undercarriage_Pad_Width', 'Stick_Length', 'Thumb', 'Pattern_Changer',\n",
            "       'Grouser_Type', 'Backhoe_Mounting', 'Blade_Type', 'Travel_Controls',\n",
            "       'Differential_Type', 'Steering_Controls', 'SalePrice'],\n",
            "      dtype='object')\n",
            "Test RMSE: 5.454063527052506\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['SalesID'] not in index\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-91-e5fbb1abe757>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTEST_run_02022025\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-90-fe1f36c97aa4>\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    718\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test RMSE:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRMSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m   \u001b[0mdf_valid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m   \u001b[0mdf_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SalesID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SalePrice'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/path/to/output.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4107\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4108\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4110\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6198\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6251\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6252\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6254\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['SalesID'] not in index\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split  # For splitting the dataset\n",
        "from sklearn.ensemble import RandomForestRegressor  # For the Random Forest regression model\n",
        "import shap\n",
        "import numpy as np\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.metrics import mean_squared_log_error\n",
        "\n",
        "\n",
        "file_path = \"/content/drive/My Drive/Valid.csv\"  # Replace with the saved file path\n",
        "df_valid = pd.read_csv(file_path)\n",
        "df_valid['SalePrice']=None\n",
        "X = df_valid.drop(columns=['SalePrice'])\n",
        "y = df_valid['SalePrice']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "y_test_pred = model.predict(X_test)\n",
        "# 7. print the RMSE accuracy of the baseline (std dev)\n",
        "print(\"RMSE Baseline accuracy:\", y_test.std())\n",
        "print(\"Train RMSE:\", RMSE(y_train, y_train_pred))\n",
        "print(\"Test RMSE:\", RMSE(y_test, y_test_pred))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "collapsed": true,
        "cellView": "form",
        "id": "SUeiDWWZfs9v",
        "outputId": "327fbaef-1a9e-45e5-e6c2-56c40f989d78"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- Differential_Type\n- Drive_System\n- Engine_Horsepower\n- Grouser_Type\n- Hydraulics\n- ...\nFeature names seen at fit time, yet now missing:\n- year\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-3afceae954af>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0my_test_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;31m# 7. print the RMSE accuracy of the baseline (std dev)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RMSE Baseline accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1064\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m         \u001b[0;31m# Check data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1066\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1067\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0;31m# Assign chunk of trees to jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0mensure_all_finite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         X = validate_data(\n\u001b[0m\u001b[1;32m    639\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2917\u001b[0m         \u001b[0mvalidated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2918\u001b[0m     \"\"\"\n\u001b[0;32m-> 2919\u001b[0;31m     \u001b[0m_check_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_estimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2920\u001b[0m     \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2921\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_tags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequired\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_check_feature_names\u001b[0;34m(estimator, X, reset)\u001b[0m\n\u001b[1;32m   2775\u001b[0m             \u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Feature names must be in the same order as they were in fit.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- Differential_Type\n- Drive_System\n- Engine_Horsepower\n- Grouser_Type\n- Hydraulics\n- ...\nFeature names seen at fit time, yet now missing:\n- year\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#erez runtime"
      ],
      "metadata": {
        "id": "FjxhNAey39au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#eitan runtime"
      ],
      "metadata": {
        "id": "LqMwx8dz4Uhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sorel runtime"
      ],
      "metadata": {
        "id": "HCO_KaS14X16"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}