{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/colab-ds18/ML-DS18/blob/main/ML_DS18.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ofer functions\n",
        "'''\n",
        "General Funtions Author OA\n",
        "Date 29/01/2025\n",
        "'''\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split  # For splitting the dataset\n",
        "from sklearn.ensemble import RandomForestRegressor  # For the Random Forest regression model\n",
        "import shap\n",
        "import numpy as np\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.metrics import mean_squared_log_error\n",
        "\n",
        "def show_unique_values(df, column_name):\n",
        "  '''\n",
        "  Function to list all unique values in a given column of a DataFrame.\n",
        "  '''\n",
        "  if column_name not in df.columns:\n",
        "    raise ValueError(f\"Column '{column_name}' does not exist in the DataFrame.\")\n",
        "    unique_products = list_unique_values(df, column_name)\n",
        "    print(unique_products)\n",
        "    null_stats_per_column(df,column_name)\n",
        "\n",
        "\n",
        "def list_unique_values(df, column_name):\n",
        "    \"\"\"\n",
        "    Function to list all unique values in a given column of a DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    - df: pandas DataFrame\n",
        "    - column_name: Name of the column to find unique values\n",
        "\n",
        "    Returns:\n",
        "    - A list of unique values in the specified column.\n",
        "    \"\"\"\n",
        "    if column_name not in df.columns:\n",
        "        raise ValueError(f\"Column '{column_name}' does not exist in the DataFrame.\")\n",
        "\n",
        "    unique_values = df[column_name].unique()\n",
        "    return unique_values\n",
        "\n",
        "\n",
        "def update_NaN_To_None_or_Unspecified(df):\n",
        "    \"\"\"\n",
        "    Updates NaN values to 'None or Unspecified' in columns where the value\n",
        "    'None or Unspecified' is already present.\n",
        "    \"\"\"\n",
        "    update_to = 'None or Unspecified'\n",
        "\n",
        "    for col in df.columns:\n",
        "        unique_values = list_unique_values(df, col)  # Get unique values for the column\n",
        "\n",
        "        # Check if 'None or Unspecified' exists and if there are NaN values\n",
        "        if update_to in unique_values and pd.isnull(unique_values).any():\n",
        "            df[col] = df[col].fillna(update_to)  # Fill NaN with the specified value\n",
        "            print(f\"Updated '{col}' column successfully!\")\n",
        "        else:\n",
        "            print(f\"Column '{col}' does not have the required conditions for update.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def columns_with_nulls(df,only_numeric_columns=True):\n",
        "  '''\n",
        "  Function to list all columns with null values in a given DataFrame.\n",
        "  '''\n",
        "  if only_numeric_columns:\n",
        "    # Get numeric columns only\n",
        "    numeric_columns = df.select_dtypes(include=['number'])\n",
        "\n",
        "    # Find numeric columns with null values\n",
        "    numeric_columns_with_null = numeric_columns.columns[numeric_columns.isnull().any()]\n",
        "    return numeric_columns_with_null.tolist()\n",
        "  else:\n",
        "    return df.columns[df.isnull().any()]\n",
        "\n",
        "\n",
        "def null_stats_per_column(df,chosen_column):\n",
        "  '''\n",
        "  Function to display the number of null rows in a given column of a DataFrame.\n",
        "  '''\n",
        "  # Count the number of null rows in the chosen column\n",
        "  null_count = df[chosen_column].isnull().sum()\n",
        "\n",
        "  # Calculate the percentage of nulls in the chosen column\n",
        "  null_percentage = (null_count / len(df)) * 100\n",
        "\n",
        "  # Display the result\n",
        "  print(f\"Column: {chosen_column}\")\n",
        "  print(f\"Null Count: {null_count}\")\n",
        "  print(f\"Null Percentage: {null_percentage:.2f}%\")\n",
        "\n",
        "\n",
        "def missing_values_histogram(df,columns):\n",
        "    '''\n",
        "    Function to plot a histogram of missing values in a given DataFrame.\n",
        "    '''\n",
        "    data=[{column:df[column].value_counts().sum()} for column in columns ]\n",
        "    # Convert the list of dictionaries to a DataFrame\n",
        "    formatted_data = pd.DataFrame([{name:count for single_data in data for name, count in single_data.items()}]).T.reset_index()\n",
        "    formatted_data.columns = ['Column', 'Count']\n",
        "    # Create a bar plot\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    ax = sns.barplot(x='Column', y='Count', data=formatted_data, hue='Column',legend=False)\n",
        "\n",
        "\n",
        "    # label count on top of each bar\n",
        "    for index, row in formatted_data.iterrows():\n",
        "        ax.text(index, row['Count'] + 10, str(row['Count']), color='black', ha=\"center\", fontsize=10)\n",
        "\n",
        "    # Add title and axis labels\n",
        "    plt.title(\"Column Counts in Dataframe\")\n",
        "    plt.xlabel(\"Column\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def encode_all_categories(df):\n",
        "  '''\n",
        "  Function to convert all string columns in a given DataFrame into categories.\n",
        "  '''\n",
        "  for col in df.select_dtypes(['object']):\n",
        "    df[col] = df[col].astype('category')\n",
        "\n",
        "  for col in df.select_dtypes(['category']):\n",
        "    df[col] = df[col].cat.codes\n",
        "\n",
        "  return df\n",
        "\n",
        "def drop_nulls(df):\n",
        "  '''\n",
        "  Function to drop all rows with null values in a given DataFrame.\n",
        "  '''\n",
        "  df = df.dropna()\n",
        "  return df\n",
        "\n",
        "def RMSLE(y_test, y_pred):\n",
        "    '''\n",
        "    RSMLE approximates the percent change\n",
        "    '''\n",
        "    return np.sqrt(np.mean((np.log(y_pred) - np.log(y_test))**2))\n",
        "\n",
        "def RMSE(y_, y_pred_):\n",
        "    '''\n",
        "    RSME\n",
        "    '''\n",
        "    return ((y_ - y_pred_) ** 2).mean() ** 0.5\n",
        "\n",
        "def train_model(df,column_to_predict,test_size_value=0.3,random_state_value=42):\n",
        "    '''\n",
        "    Function to train a Random Forest regression model on a given DataFrame.\n",
        "    '''\n",
        "    X = df.drop(columns=[column_to_predict])\n",
        "    y = df[column_to_predict]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size_value, random_state=random_state_value)\n",
        "\n",
        "    # model = DecisionTreeRegressor(\n",
        "    # min_samples_leaf=16\n",
        "    # max_depth=3\n",
        "    # )\n",
        "    #model = LinearRegression()\n",
        "    model = RandomForestRegressor()\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    # 7. print the RMSE accuracy of the baseline (std dev)\n",
        "    print(\"RMSE Baseline accuracy:\", y_test.std())\n",
        "    print(\"Train RMSE:\", RMSE(y_train, y_train_pred))\n",
        "    print(\"Test RMSE:\", RMSE(y_test, y_test_pred))\n",
        "    print(\"Test RMSLE:\", RMSLE(y_test, y_test_pred))\n",
        "\n",
        "    #display(pd.Series(model.feature_importances_, model.feature_names_in_).sort_values(ascending=False))\n",
        "    return model,(X_train, X_test, y_train, y_test)\n",
        "\n",
        "def feature_importances(model):\n",
        "  '''\n",
        "  Function to display the feature importances of a given Random Forest regression model.\n",
        "  '''\n",
        "  dict(zip(model.feature_names_in_, model.feature_importances_))\n",
        "  fi = pd.Series(model.feature_importances_, index=model.feature_names_in_)\n",
        "  fi = fi.sort_values(ascending=False)\n",
        "  print(fi)\n",
        "\n",
        "  return fi\n",
        "\n",
        "def explain_model_snap(model,X_test,water_fall_row=0):\n",
        "  '''\n",
        "  Function to explain the model's predictions using SHAP values.\n",
        "  '''\n",
        "  shap.initjs()\n",
        "  explainer = shap.Explainer(model)\n",
        "  explanation = explainer(X_test)  # New style\n",
        "  shap.summary_plot(explanation, X_test)\n",
        "\n",
        "  shap.plots.waterfall(explanation[water_fall_row])\n",
        "\n",
        "  shap.plots.waterfall(explanation[water_fall_row+2])\n",
        "\n",
        "  shap.plots.partial_dependence('fare', model.predict, X_test, feature_names=X_test.columns)\n",
        "\n",
        "  shap.plots.heatmap(explanation)\n",
        "\n",
        "  shap.plots.bar(explanation)\n",
        "\n",
        "\n",
        "def find_outliers(df, column_to_predict, test_size_value=0.3, random_state_value=42):\n",
        "    \"\"\"\n",
        "    Identify outliers using Isolation Forest and add anomaly-related columns to the DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    - df: Input DataFrame.\n",
        "    - column_to_predict: Column to exclude for training the Isolation Forest.\n",
        "    - test_size_value: Test size for train-test split.\n",
        "    - random_state_value: Random state for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "    - A new DataFrame with additional columns: 'anomaly_score', 'anomaly', and 'anomaly_label'.\n",
        "    \"\"\"\n",
        "    # Create features (X) and target (y)\n",
        "    X = pd.get_dummies(df.drop(columns=[column_to_predict]), drop_first=True)\n",
        "    y = df[column_to_predict]\n",
        "\n",
        "    # Train Isolation Forest on the entire dataset (no splitting)\n",
        "    iso_forest = IsolationForest(n_estimators=100, contamination=0.01, random_state=random_state_value)\n",
        "    iso_forest.fit(X)\n",
        "\n",
        "    # Predict anomaly scores and labels\n",
        "    df = df.copy()  # Work on a copy of the DataFrame\n",
        "    df['anomaly_score'] = iso_forest.decision_function(X)  # Quantitative anomaly score\n",
        "    df['anomaly'] = iso_forest.predict(X)  # Binary anomaly label (1 for normal, -1 for anomaly)\n",
        "    df['anomaly_label'] = df['anomaly'].map({1: 'Normal', -1: 'Anomaly'})  # Map labels\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "def convert_date_columns(df, date_column):\n",
        "    \"\"\"\n",
        "    Convert date column into multiple numeric features like year, month, day, etc.\n",
        "    Handles potential issues like missing date_column or invalid data.\n",
        "    \"\"\"\n",
        "    # Ensure the date_column exists in the DataFrame\n",
        "    if date_column not in df.columns:\n",
        "        raise KeyError(f\"Column '{date_column}' does not exist in the DataFrame.\")\n",
        "\n",
        "    # Convert the column to datetime\n",
        "    df[date_column] = pd.to_datetime(df[date_column], errors='coerce')  # Handle invalid date strings\n",
        "\n",
        "    # Check if any date values are invalid after conversion\n",
        "    if df[date_column].isnull().all():\n",
        "        raise ValueError(f\"All values in '{date_column}' could not be converted to datetime.\")\n",
        "\n",
        "    # Extract date features\n",
        "    df['year'] = df[date_column].dt.year\n",
        "    df['month'] = df[date_column].dt.month\n",
        "    df['day'] = df[date_column].dt.day\n",
        "    df['day_of_week'] = df[date_column].dt.dayofweek\n",
        "    df['is_weekend'] = df['day_of_week'] >= 5  # Saturday=5, Sunday=6\n",
        "\n",
        "    # Drop the original date column\n",
        "    df = df.drop(columns=[date_column])\n",
        "\n",
        "    return df\n",
        "\n",
        "#    model = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
        "#    xs=(X_train, X_test, y_train, y_test)\n",
        "def perm_importance_model(model, xs):\n",
        "    '''\n",
        "    Function to compute Permutation Importance of a given Random Forest regression model.\n",
        "    '''\n",
        "    X_train, X_test, y_train, y_test = xs\n",
        "\n",
        "    # Compute Permutation Importance\n",
        "    perm_importance = permutation_importance(\n",
        "        model, X_test, y_test, n_repeats=5, random_state=42, n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # Create the results DataFrame\n",
        "    importance_model = pd.DataFrame({\n",
        "        \"Feature\": model.feature_names_in_,\n",
        "        \"Permutation Importance\": perm_importance.importances_mean,\n",
        "        \"Permutation Std Deviation\": perm_importance.importances_std,\n",
        "        \"Model Importance\": model.feature_importances_,\n",
        "    })\n",
        "\n",
        "    # Add ranking for permutation and model importances\n",
        "    importance_model[\"Permutation Rank\"] = importance_model[\"Permutation Importance\"].rank(ascending=False)\n",
        "    importance_model[\"Model Rank\"] = importance_model[\"Model Importance\"].rank(ascending=False)\n",
        "\n",
        "    # Sort by Permutation Importance for display\n",
        "    importance_model = importance_model.sort_values(by=\"Permutation Importance\", ascending=False)\n",
        "\n",
        "    # Display the DataFrame\n",
        "    display(importance_model)\n",
        "\n",
        "    return importance_model\n",
        "\n",
        "def load_dataframe_from_drive():\n",
        "    \"\"\"\n",
        "    Function to load a DataFrame from a Google Drive file path.\n",
        "    \"\"\"\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Load the file from Google Drive\n",
        "    file_path = \"/content/drive/My Drive/Train.csv\"  # Replace with the saved file path\n",
        "    df = pd.read_csv(file_path)\n",
        "    return df\n",
        "\n",
        "def summarize_null_columns(dataframe):\n",
        "    \"\"\"\n",
        "    Function to find columns with null values and display detailed information\n",
        "    including column name, total null count, percentage of nulls, and data type.\n",
        "    \"\"\"\n",
        "    # Find total nulls and percentage of nulls for each column\n",
        "    columns_with_nulls = dataframe.isnull().sum()  # Total nulls per column\n",
        "    null_percentage = (columns_with_nulls / len(dataframe)) * 100  # Percentage of nulls\n",
        "\n",
        "    # Iterate through columns and display only those with null values\n",
        "    for col in dataframe.columns:\n",
        "        if columns_with_nulls[col] > 0:  # Check if the column has nulls\n",
        "            col_dtype = dataframe[col].dtype  # Get column data type\n",
        "            print(f\"Column: {col}\")\n",
        "            print(f\"  - Data Type: {col_dtype}\")\n",
        "            print(f\"  - Total Null Rows: {columns_with_nulls[col]}\")\n",
        "            print(f\"  - Percentage of Nulls: {null_percentage[col]:.2f}%\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "def drop_columns_with_high_nulls(dataframe, threshold=500):\n",
        "    \"\"\"\n",
        "    Function to drop columns from the DataFrame that have more than 'threshold' null values.\n",
        "\n",
        "    Parameters:\n",
        "    dataframe (pd.DataFrame): The input DataFrame.\n",
        "    threshold (int): The maximum allowable null rows for a column. Columns with nulls > threshold will be dropped.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: A new DataFrame with columns dropped based on the condition.\n",
        "    \"\"\"\n",
        "    # Identify columns with null counts greater than the threshold\n",
        "    columns_to_drop = dataframe.columns[dataframe.isnull().sum() > threshold]\n",
        "\n",
        "    # Drop these columns from the DataFrame\n",
        "    cleaned_dataframe = dataframe.drop(columns=columns_to_drop)\n",
        "\n",
        "    print(f\"Dropped columns: {list(columns_to_drop)}\")\n",
        "    return cleaned_dataframe\n",
        "\n",
        "def update_MachineHoursCurrentMeter(df):\n",
        "  # Update the 'MachineHoursCurrentMeter' column: replace NaN or 0 with 0\n",
        "  column_name = 'MachineHoursCurrentMeter'\n",
        "\n",
        "  # Check if the column exists\n",
        "  if column_name in df.columns:\n",
        "      df[column_name] = df[column_name].fillna(0)  # Replace NaN with 0\n",
        "      df[column_name] = df[column_name].replace(0, 0)  # Ensure 0 stays 0\n",
        "      print(f\"Updated '{column_name}' column successfully!\")\n",
        "  else:\n",
        "      print(f\"Column '{column_name}' does not exist in the DataFrame.\")\n",
        "\n",
        "  return df\n",
        "\n",
        "def update_auctioneerID(df):\n",
        "  # Assuming 'df' is your DataFrame\n",
        "  # Fill null values in the 'auctioneerID' column with 100.0\n",
        "  if 'auctioneerID' in df.columns:\n",
        "      df['auctioneerID'] = df['auctioneerID'].fillna(100.0)\n",
        "      print(\"'auctioneerID' null values filled with 100.0 successfully!\")\n",
        "  else:\n",
        "      print(\"Column 'auctioneerID' does not exist in the DataFrame.\")\n",
        "\n",
        "  return df\n",
        "\n",
        "def update_Enclosure(df):\n",
        "  # Replace null values in the 'Enclosure' column with 'N/A'\n",
        "  if 'Enclosure' in df.columns:\n",
        "      df['Enclosure'] = df['Enclosure'].fillna('N/A')\n",
        "      print(\"Replaced null values in the 'Enclosure' column with 'N/A'.\")\n",
        "  else:\n",
        "      print(\"Column 'Enclosure' does not exist in the DataFrame.\")\n",
        "\n",
        "  return df\n",
        "\n",
        "#First run OA\n",
        "def first_run_25012025():\n",
        "  df=load_dataframe_from_drive()\n",
        "\n",
        "  missing_values_histogram(df,df.columns)\n",
        "\n",
        "  #df.info()\n",
        "\n",
        "  #df.describe()\n",
        "\n",
        "  df=update_MachineHoursCurrentMeter(df)\n",
        "  df=update_auctioneerID(df)\n",
        "\n",
        "  #print(\"columns with null values:\", columns_with_nulls(df,False))\n",
        "  #summarize_null_columns(df)\n",
        "\n",
        "  df = drop_columns_with_high_nulls(df, threshold=500)\n",
        "  df=update_Enclosure(df)\n",
        "  df=convert_date_columns(df,'saledate')\n",
        "  df=encode_all_categories(df)\n",
        "  model,xs=train_model(df=df,column_to_predict='SalePrice')#took 8 min runtime\n",
        "  return model,xs\n",
        "\n",
        "#Second run OA remove cols and outliners\n",
        "def Second_run_26012025():\n",
        "  df=load_dataframe_from_drive()\n",
        "  df=update_MachineHoursCurrentMeter(df)\n",
        "  df=update_auctioneerID(df)\n",
        "  df = drop_columns_with_high_nulls(df, threshold=500)\n",
        "  df=update_Enclosure(df)\n",
        "  df=convert_date_columns(df,'saledate')\n",
        "  df=encode_all_categories(df)\n",
        "\n",
        "  sampled_df = df.sample(n=25000, random_state=42)  # Example: 10,000 rows\n",
        "\n",
        "  model,xs=train_model(df=sampled_df,column_to_predict='SalePrice')\n",
        "\n",
        "  feature_importances(model)\n",
        "\n",
        "  sampled_df = sampled_df.drop(columns=['is_weekend','datasource','MachineHoursCurrentMeter','day_of_week','auctioneerID'], errors='ignore')  # 'errors=\"ignore\"' prevents errors if columns don't exist\n",
        "\n",
        "  model,xs=train_model(df=sampled_df,column_to_predict='SalePrice')\n",
        "\n",
        "  sampled_df=find_outliers(df=sampled_df,column_to_predict='SalePrice')\n",
        "\n",
        "  # Assuming df_with_anomalies is the DataFrame containing anomaly labels\n",
        "  anomalies = sampled_df[sampled_df['anomaly_label'] == 'Anomaly']\n",
        "\n",
        "  # Display rows where 'anomaly_label' is 'Anomaly'\n",
        "  print(anomalies)\n",
        "\n",
        "  # Optionally, display the count of anomalies\n",
        "  print(f\"Number of anomalies detected: {len(anomalies)}\")\n",
        "\n",
        "  # Remove all rows where 'anomaly_label' is 'Anomaly'\n",
        "  sampled_df = sampled_df[sampled_df['anomaly_label'] != 'Anomaly']\n",
        "\n",
        "  # Drop the columns 'anomaly_label', 'anomaly_score', and 'anomaly'\n",
        "  columns_to_drop = ['anomaly_label', 'anomaly_score', 'anomaly']\n",
        "  sampled_df = sampled_df.drop(columns=columns_to_drop, errors='ignore')\n",
        "\n",
        "  model,xs=train_model(df=sampled_df,column_to_predict='SalePrice')\n",
        "\n",
        "  perm_importance_model(model,xs)\n",
        "\n",
        "  sampled_df = sampled_df.drop(columns=['day','month','state','MachineID','SalesID','ProductGroupDesc'], errors='ignore')  # 'errors=\"ignore\"' prevents errors if columns don't exist\n",
        "\n",
        "  model,xs=train_model(df=sampled_df,column_to_predict='SalePrice')\n",
        "\n",
        "  perm_importance_model(model,xs)\n",
        "\n",
        "  return model,xs\n",
        "\n",
        "#First run OA 27012025\n",
        "def First_run_27012025():\n",
        "  df=load_dataframe_from_drive()\n",
        "\n",
        "  update_NaN_To_None_or_Unspecified(df)\n",
        "\n",
        "  df=update_MachineHoursCurrentMeter(df)\n",
        "  df=update_auctioneerID(df)\n",
        "  df = drop_columns_with_high_nulls(df, threshold=500)\n",
        "  df=update_Enclosure(df)\n",
        "  df=convert_date_columns(df,'saledate')\n",
        "  df=encode_all_categories(df)\n",
        "\n",
        "  sampled_df = df.sample(n=25000, random_state=42)  # Example: 10,000 rows\n",
        "\n",
        "  model,xs=train_model(df=sampled_df,column_to_predict='SalePrice')\n",
        "\n",
        "  feature_importances(model)\n",
        "\n",
        "  sampled_df = sampled_df.drop(columns=['is_weekend','datasource','MachineHoursCurrentMeter','day_of_week','auctioneerID'], errors='ignore')  # 'errors=\"ignore\"' prevents errors if columns don't exist\n",
        "\n",
        "  model,xs=train_model(df=sampled_df,column_to_predict='SalePrice')\n",
        "\n",
        "  sampled_df=find_outliers(df=sampled_df,column_to_predict='SalePrice')\n",
        "\n",
        "  # Assuming df_with_anomalies is the DataFrame containing anomaly labels\n",
        "  anomalies = sampled_df[sampled_df['anomaly_label'] == 'Anomaly']\n",
        "\n",
        "  # Display rows where 'anomaly_label' is 'Anomaly'\n",
        "  print(anomalies)\n",
        "\n",
        "  # Optionally, display the count of anomalies\n",
        "  print(f\"Number of anomalies detected: {len(anomalies)}\")\n",
        "\n",
        "  # Remove all rows where 'anomaly_label' is 'Anomaly'\n",
        "  sampled_df = sampled_df[sampled_df['anomaly_label'] != 'Anomaly']\n",
        "\n",
        "  # Drop the columns 'anomaly_label', 'anomaly_score', and 'anomaly'\n",
        "  columns_to_drop = ['anomaly_label', 'anomaly_score', 'anomaly']\n",
        "  sampled_df = sampled_df.drop(columns=columns_to_drop, errors='ignore')\n",
        "\n",
        "  model,xs=train_model(df=sampled_df,column_to_predict='SalePrice')\n",
        "\n",
        "  perm_importance_model(model,xs)\n",
        "\n",
        "  sampled_df = sampled_df.drop(columns=['day','month','state','MachineID','SalesID','ProductGroupDesc'], errors='ignore')  # 'errors=\"ignore\"' prevents errors if columns don't exist\n",
        "\n",
        "  model,xs=train_model(df=sampled_df,column_to_predict='SalePrice')\n",
        "\n",
        "  importance_model=perm_importance_model(model,xs)\n",
        "\n",
        "  # Drop columns where 'Permutation Importance' is less than 0.02\n",
        "  importance_model_filtered = importance_model[importance_model['Permutation Importance'] >= 0.02]\n",
        "\n",
        "  # Display the filtered DataFrame\n",
        "  print(importance_model_filtered)\n",
        "\n",
        "  # Optionally, list the dropped columns\n",
        "  dropped_columns = importance_model[importance_model['Permutation Importance'] < 0.02]['Feature']\n",
        "  print(\"Dropped columns:\", dropped_columns.tolist())\n",
        "  sampled_df.drop(columns=dropped_columns, inplace=True)\n",
        "\n",
        "  model,xs=train_model(df=sampled_df,column_to_predict='SalePrice')\n",
        "\n",
        "  return model,xs\n",
        "\n",
        "\n",
        "'''\n",
        "import pandas as pd\n",
        "\n",
        "# Compute correlation matrix\n",
        "corr_matrix = df.corr()\n",
        "\n",
        "# Find highly correlated features (e.g., correlation > 0.85)\n",
        "high_corr_features = set()\n",
        "threshold = 0.85\n",
        "\n",
        "for i in range(len(corr_matrix.columns)):\n",
        "    for j in range(i):\n",
        "        if abs(corr_matrix.iloc[i, j]) > threshold:\n",
        "            colname = corr_matrix.columns[i]\n",
        "            high_corr_features.add(colname)\n",
        "\n",
        "print(\"Highly correlated features:\", high_corr_features)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "corr_matrix = df.corr()\n",
        "threshold = 0.60\n",
        "\n",
        "# Apply threshold filter to only show correlations above 0.5\n",
        "filtered_corr = corr_matrix[(abs(corr_matrix) > threshold) & (abs(corr_matrix) < 1)]\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(filtered_corr, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "plt.title(\"Example Correlation Heatmap\")\n",
        "plt.show()\n",
        "'''"
      ],
      "metadata": {
        "id": "GetwQ5yXBMFX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "outputId": "5618259f-ec53-4fa4-ad13-7c2e722f8c4c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport pandas as pd\\n\\n# Compute correlation matrix\\ncorr_matrix = df.corr()\\n\\n# Find highly correlated features (e.g., correlation > 0.85)\\nhigh_corr_features = set()\\nthreshold = 0.85\\n\\nfor i in range(len(corr_matrix.columns)):\\n    for j in range(i):\\n        if abs(corr_matrix.iloc[i, j]) > threshold:\\n            colname = corr_matrix.columns[i]\\n            high_corr_features.add(colname)\\n\\nprint(\"Highly correlated features:\", high_corr_features)\\n\\nimport pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\ncorr_matrix = df.corr()\\nthreshold = 0.60\\n\\n# Apply threshold filter to only show correlations above 0.5\\nfiltered_corr = corr_matrix[(abs(corr_matrix) > threshold) & (abs(corr_matrix) < 1)]\\n\\n# Plot heatmap\\nplt.figure(figsize=(10, 6))\\nsns.heatmap(filtered_corr, annot=True, cmap=\\'coolwarm\\', fmt=\".2f\", linewidths=0.5)\\nplt.title(\"Example Correlation Heatmap\")\\nplt.show()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#erez functions"
      ],
      "metadata": {
        "id": "ElpLtGhC3zSk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#eeitan function\n",
        "def calculate_machine_age(df):\n",
        "    # Ensure that 'SaleDate' is in datetime format\n",
        "    df['saledate'] = pd.to_datetime(df['saledate'])\n",
        "    # Extract the year from 'SaleDate'\n",
        "    df['saledate'] = df['saledate'].dt.year\n",
        "\n",
        "    # Calculate MachineAge by subtracting 'YearMade' from the 'SaleYear'\n",
        "    df['machineage'] = df['saleyear'] - df['yearmade']\n",
        "\n",
        "    # Drop the 'SaleYear' column if you don't need it anymore\n",
        "    df.drop(columns=['saleyear'], inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "df=load_dataframe_from_drive()\n",
        "df['saledate']\n",
        "# Example usage:\n",
        "# Assuming your dataframe is named `df`\n",
        "df = calculate_machine_age(df)\n",
        "\n",
        "# Display the dataframe with the new 'MachineAge' column\n",
        "print(df[['YearMade', 'SaleDate', 'MachineAge']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "collapsed": true,
        "id": "r6_vim-k4By6",
        "outputId": "d75a5612-ac4d-478f-b3b9-c63c4304e154"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-eb3e13734bff>:314: DtypeWarning: Columns (13,39,40,41) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1989-01-17 00:00:00\n",
            "2011-12-30 00:00:00\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'saleyear'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'saleyear'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-be69d7b82365>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Example usage:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Assuming your dataframe is named `df`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_machine_age\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Display the dataframe with the new 'MachineAge' column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-be69d7b82365>\u001b[0m in \u001b[0;36mcalculate_machine_age\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Calculate MachineAge by subtracting 'YearMade' from the 'SaleYear'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'machineage'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'saleyear'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'yearmade'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Drop the 'SaleYear' column if you don't need it anymore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'saleyear'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sorel functions"
      ],
      "metadata": {
        "id": "1EojaDZG4LoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model,xs=train_model(df=df,column_to_predict='SalePrice')#took 8 min runtime"
      ],
      "metadata": {
        "id": "rjC7CgKHEDBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RMSE Baseline accuracy: 22932.4005340408 Train RMSE: 2864.123815046 Test RMSE: 7633.339248575835 0 Enclosure 0.240869 YearMade 0.147308 fiProductClassDesc 0.117304 fiModelDesc 0.099289 ModelID 0.093105 year 0.080002 SalesID 0.043689 fiBaseModel 0.033948 ProductGroup 0.032850 ProductGroupDesc 0.029777 MachineID 0.023082 day 0.013131 month 0.013108 state 0.012552 auctioneerID 0.006424 day_of_week 0.006194 MachineHoursCurrentMeter 0.005897 datasource 0.000885 is_weekend 0.000586"
      ],
      "metadata": {
        "id": "9X3AVb8dEH69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ofer runtime\n",
        "#model,xs=first_run_25012025()\n",
        "#model,xs=Second_run_26012025()\n",
        "#model,xs=First_run_27012025()\n",
        "df=load_dataframe_from_drive()\n",
        "print(df['YearMade'].min())\n",
        "print(df['YearMade'].max())\n",
        "df.columns\n",
        "#sum_of_rows = df[df['yearmade'] == 1000].sum(numeric_only=True)\n",
        "pd.set_option('display.expand_frame_repr', False)\n",
        "def calc_YearMade(ModelID):\n",
        "  return int(df[(df['ModelID']==ModelID) & (df['YearMade']>1000)]['YearMade'].mean())\n",
        "df_modelid=df[df['YearMade']==1000]['ModelID']\n",
        "print(df_modelid.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWjBrHg34pvE",
        "outputId": "83bb134e-621f-4b38-dab2-9b9cf3bd928c"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-cc1b73a54d3d>:314: DtypeWarning: Columns (13,39,40,41) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n",
            "2013\n",
            "9     3883\n",
            "21    3350\n",
            "33    1918\n",
            "35     112\n",
            "36    7110\n",
            "Name: ModelID, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#erez runtime"
      ],
      "metadata": {
        "id": "FjxhNAey39au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#eitan runtime"
      ],
      "metadata": {
        "id": "LqMwx8dz4Uhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sorel runtime"
      ],
      "metadata": {
        "id": "HCO_KaS14X16"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}