{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/colab-ds18/ML-DS18/blob/main/ML_DS18.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ofer functions\n",
        "'''\n",
        "General Funtions Author OA\n",
        "Date 29/01/2025\n",
        "'''\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split  # For splitting the dataset\n",
        "from sklearn.ensemble import RandomForestRegressor  # For the Random Forest regression model\n",
        "import shap\n",
        "import numpy as np\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.metrics import mean_squared_log_error\n",
        "\n",
        "def show_unique_values(df, column_name):\n",
        "  '''\n",
        "  Function to list all unique values in a given column of a DataFrame.\n",
        "  '''\n",
        "  if column_name not in df.columns:\n",
        "    raise ValueError(f\"Column '{column_name}' does not exist in the DataFrame.\")\n",
        "    unique_products = list_unique_values(df, column_name)\n",
        "    print(unique_products)\n",
        "    null_stats_per_column(df,column_name)\n",
        "\n",
        "\n",
        "def list_unique_values(df, column_name):\n",
        "    \"\"\"\n",
        "    Function to list all unique values in a given column of a DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    - df: pandas DataFrame\n",
        "    - column_name: Name of the column to find unique values\n",
        "\n",
        "    Returns:\n",
        "    - A list of unique values in the specified column.\n",
        "    \"\"\"\n",
        "    if column_name not in df.columns:\n",
        "        raise ValueError(f\"Column '{column_name}' does not exist in the DataFrame.\")\n",
        "\n",
        "    unique_values = df[column_name].unique()\n",
        "    return unique_values\n",
        "\n",
        "\n",
        "def update_NaN_To_None_or_Unspecified(df):\n",
        "    \"\"\"\n",
        "    Updates NaN values to 'None or Unspecified' in columns where the value\n",
        "    'None or Unspecified' is already present.\n",
        "    \"\"\"\n",
        "    print(\"update_NaN_To_None_or_Unspecified START\")\n",
        "\n",
        "    update_to = 'None or Unspecified'\n",
        "\n",
        "    for col in df.columns:\n",
        "        unique_values = list_unique_values(df, col)  # Get unique values for the column\n",
        "\n",
        "        # Check if 'None or Unspecified' exists and if there are NaN values\n",
        "        if update_to in unique_values and pd.isnull(unique_values).any():\n",
        "            df[col] = df[col].fillna(update_to)  # Fill NaN with the specified value\n",
        "            #print(f\"Updated '{col}' column successfully!\")\n",
        "        #else:\n",
        "        #    print(f\"Column '{col}' does not have the required conditions for update.\")\n",
        "\n",
        "    print(\"update_NaN_To_None_or_Unspecified END\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def columns_with_nulls(df,only_numeric_columns=True):\n",
        "  '''\n",
        "  Function to list all columns with null values in a given DataFrame.\n",
        "  '''\n",
        "  print(\"columns_with_nulls START/END\")\n",
        "  if only_numeric_columns:\n",
        "    # Get numeric columns only\n",
        "    numeric_columns = df.select_dtypes(include=['number'])\n",
        "\n",
        "    # Find numeric columns with null values\n",
        "    numeric_columns_with_null = numeric_columns.columns[numeric_columns.isnull().any()]\n",
        "    return numeric_columns_with_null.tolist()\n",
        "  else:\n",
        "    return df.columns[df.isnull().any()]\n",
        "\n",
        "\n",
        "def null_stats_per_column(df,chosen_column):\n",
        "  '''\n",
        "  Function to display the number of null rows in a given column of a DataFrame.\n",
        "  '''\n",
        "  print(\"null_stats_per_column START/END\")\n",
        "\n",
        "  # Count the number of null rows in the chosen column\n",
        "  null_count = df[chosen_column].isnull().sum()\n",
        "\n",
        "  # Calculate the percentage of nulls in the chosen column\n",
        "  null_percentage = (null_count / len(df)) * 100\n",
        "\n",
        "  # Display the result\n",
        "  print(f\"Column: {chosen_column}\")\n",
        "  print(f\"Null Count: {null_count}\")\n",
        "  print(f\"Null Percentage: {null_percentage:.2f}%\")\n",
        "\n",
        "\n",
        "def missing_values_histogram(df,columns):\n",
        "    '''\n",
        "    Function to plot a histogram of missing values in a given DataFrame.\n",
        "    '''\n",
        "    data=[{column:df[column].value_counts().sum()} for column in columns ]\n",
        "    # Convert the list of dictionaries to a DataFrame\n",
        "    formatted_data = pd.DataFrame([{name:count for single_data in data for name, count in single_data.items()}]).T.reset_index()\n",
        "    formatted_data.columns = ['Column', 'Count']\n",
        "    # Create a bar plot\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    ax = sns.barplot(x='Column', y='Count', data=formatted_data, hue='Column',legend=False)\n",
        "\n",
        "\n",
        "    # label count on top of each bar\n",
        "    for index, row in formatted_data.iterrows():\n",
        "        ax.text(index, row['Count'] + 10, str(row['Count']), color='black', ha=\"center\", fontsize=10)\n",
        "\n",
        "    # Add title and axis labels\n",
        "    plt.title(\"Column Counts in Dataframe\")\n",
        "    plt.xlabel(\"Column\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def encode_all_categories(df):\n",
        "  '''\n",
        "  Function to convert all string columns in a given DataFrame into categories.\n",
        "  '''\n",
        "  print(\"encode_all_categories START\")\n",
        "  for col in df.select_dtypes(['object']):\n",
        "    df[col] = df[col].astype('category')\n",
        "\n",
        "  for col in df.select_dtypes(['category']):\n",
        "    df[col] = df[col].cat.codes\n",
        "\n",
        "  print(\"encode_all_categories END\")\n",
        "\n",
        "  return df\n",
        "\n",
        "def drop_nulls(df):\n",
        "  '''\n",
        "  Function to drop all rows with null values in a given DataFrame.\n",
        "  '''\n",
        "  df = df.dropna()\n",
        "  return df\n",
        "\n",
        "def RMSLE(y_test, y_pred):\n",
        "    '''\n",
        "    RSMLE approximates the percent change\n",
        "    '''\n",
        "    return np.sqrt(np.mean((np.log(y_pred) - np.log(y_test))**2))\n",
        "\n",
        "def RMSE(y_, y_pred_):\n",
        "    '''\n",
        "    RSME\n",
        "    '''\n",
        "    return ((y_ - y_pred_) ** 2).mean() ** 0.5\n",
        "\n",
        "def train_model(df,column_to_predict,test_size_value=0.3,random_state_value=42):\n",
        "    '''\n",
        "    Function to train a Random Forest regression model on a given DataFrame.\n",
        "    '''\n",
        "    print(\"train_model START\")\n",
        "\n",
        "    X = df.drop(columns=[column_to_predict])\n",
        "    y = df[column_to_predict]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size_value, random_state=random_state_value)\n",
        "\n",
        "    # model = DecisionTreeRegressor(\n",
        "    # min_samples_leaf=16\n",
        "    # max_depth=3\n",
        "    # )\n",
        "    #model = LinearRegression()\n",
        "    model = RandomForestRegressor()\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    # 7. print the RMSE accuracy of the baseline (std dev)\n",
        "    print(\"RMSE Baseline accuracy:\", y_test.std())\n",
        "    print(\"Train RMSE:\", RMSE(y_train, y_train_pred))\n",
        "    print(\"Test RMSE:\", RMSE(y_test, y_test_pred))\n",
        "    print(\"Test RMSLE:\", RMSLE(y_test, y_test_pred))\n",
        "\n",
        "    print(\"train_model END\")\n",
        "\n",
        "    #display(pd.Series(model.feature_importances_, model.feature_names_in_).sort_values(ascending=False))\n",
        "    return model,(X_train, X_test, y_train, y_test)\n",
        "\n",
        "def feature_importances(model):\n",
        "  '''\n",
        "  Function to display the feature importances of a given Random Forest regression model.\n",
        "  '''\n",
        "  print(\"feature_importances START\")\n",
        "\n",
        "  dict(zip(model.feature_names_in_, model.feature_importances_))\n",
        "  fi = pd.Series(model.feature_importances_, index=model.feature_names_in_)\n",
        "  fi = fi.sort_values(ascending=False)\n",
        "  print(\"feature_importances END\")\n",
        "\n",
        "  return fi\n",
        "\n",
        "def explain_model_snap(model,X_test,water_fall_row=0):\n",
        "  '''\n",
        "  Function to explain the model's predictions using SHAP values.\n",
        "  '''\n",
        "  shap.initjs()\n",
        "  explainer = shap.Explainer(model)\n",
        "  explanation = explainer(X_test)  # New style\n",
        "  shap.summary_plot(explanation, X_test)\n",
        "\n",
        "  shap.plots.waterfall(explanation[water_fall_row])\n",
        "\n",
        "  shap.plots.waterfall(explanation[water_fall_row+2])\n",
        "\n",
        "  shap.plots.partial_dependence('fare', model.predict, X_test, feature_names=X_test.columns)\n",
        "\n",
        "  shap.plots.heatmap(explanation)\n",
        "\n",
        "  shap.plots.bar(explanation)\n",
        "\n",
        "\n",
        "def find_outliers(df, column_to_predict, test_size_value=0.3, random_state_value=42):\n",
        "    \"\"\"\n",
        "    Identify outliers using Isolation Forest and add anomaly-related columns to the DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    - df: Input DataFrame.\n",
        "    - column_to_predict: Column to exclude for training the Isolation Forest.\n",
        "    - test_size_value: Test size for train-test split.\n",
        "    - random_state_value: Random state for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "    - A new DataFrame with additional columns: 'anomaly_score', 'anomaly', and 'anomaly_label'.\n",
        "    \"\"\"\n",
        "    print(\"find_outliers START\")\n",
        "\n",
        "    # Create features (X) and target (y)\n",
        "    X = pd.get_dummies(df.drop(columns=[column_to_predict]), drop_first=True)\n",
        "    y = df[column_to_predict]\n",
        "\n",
        "    # Train Isolation Forest on the entire dataset (no splitting)\n",
        "    iso_forest = IsolationForest(n_estimators=100, contamination=0.01, random_state=random_state_value)\n",
        "    iso_forest.fit(X)\n",
        "\n",
        "    # Predict anomaly scores and labels\n",
        "    df = df.copy()  # Work on a copy of the DataFrame\n",
        "    df['anomaly_score'] = iso_forest.decision_function(X)  # Quantitative anomaly score\n",
        "    df['anomaly'] = iso_forest.predict(X)  # Binary anomaly label (1 for normal, -1 for anomaly)\n",
        "    df['anomaly_label'] = df['anomaly'].map({1: 'Normal', -1: 'Anomaly'})  # Map labels\n",
        "    print(\"find_outliers END\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "def convert_date_columns(df, date_column):\n",
        "    \"\"\"\n",
        "    Convert date column into multiple numeric features like year, month, day, etc.\n",
        "    Handles potential issues like missing date_column or invalid data.\n",
        "    \"\"\"\n",
        "    print(\"convert_date_columns START\")\n",
        "\n",
        "    # Ensure the date_column exists in the DataFrame\n",
        "    if date_column not in df.columns:\n",
        "        raise KeyError(f\"Column '{date_column}' does not exist in the DataFrame.\")\n",
        "\n",
        "    # Convert the column to datetime\n",
        "    df[date_column] = pd.to_datetime(df[date_column], errors='coerce')  # Handle invalid date strings\n",
        "\n",
        "    # Check if any date values are invalid after conversion\n",
        "    if df[date_column].isnull().all():\n",
        "        raise ValueError(f\"All values in '{date_column}' could not be converted to datetime.\")\n",
        "\n",
        "    # Extract date features\n",
        "    df['year'] = df[date_column].dt.year\n",
        "    df['month'] = df[date_column].dt.month\n",
        "    df['day'] = df[date_column].dt.day\n",
        "    df['day_of_week'] = df[date_column].dt.dayofweek\n",
        "    df['is_weekend'] = df['day_of_week'] >= 5  # Saturday=5, Sunday=6\n",
        "\n",
        "    # Drop the original date column\n",
        "    df = df.drop(columns=[date_column])\n",
        "\n",
        "    print(\"convert_date_columns END\")\n",
        "\n",
        "    return df\n",
        "\n",
        "#    model = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
        "#    xs=(X_train, X_test, y_train, y_test)\n",
        "def perm_importance_model(model, xs):\n",
        "    '''\n",
        "    Function to compute Permutation Importance of a given Random Forest regression model.\n",
        "    '''\n",
        "    print(\"perm_importance_model START\")\n",
        "\n",
        "    X_train, X_test, y_train, y_test = xs\n",
        "\n",
        "    # Compute Permutation Importance\n",
        "    perm_importance = permutation_importance(\n",
        "        model, X_test, y_test, n_repeats=5, random_state=42, n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # Create the results DataFrame\n",
        "    importance_model = pd.DataFrame({\n",
        "        \"Feature\": model.feature_names_in_,\n",
        "        \"Permutation Importance\": perm_importance.importances_mean,\n",
        "        \"Permutation Std Deviation\": perm_importance.importances_std,\n",
        "        \"Model Importance\": model.feature_importances_,\n",
        "    })\n",
        "\n",
        "    # Add ranking for permutation and model importances\n",
        "    importance_model[\"Permutation Rank\"] = importance_model[\"Permutation Importance\"].rank(ascending=False)\n",
        "    importance_model[\"Model Rank\"] = importance_model[\"Model Importance\"].rank(ascending=False)\n",
        "\n",
        "    # Sort by Permutation Importance for display\n",
        "    importance_model = importance_model.sort_values(by=\"Permutation Importance\", ascending=False)\n",
        "\n",
        "    # Display the DataFrame\n",
        "    display(importance_model)\n",
        "\n",
        "    print(\"perm_importance_model END\")\n",
        "\n",
        "    return importance_model\n",
        "\n",
        "def load_dataframe_from_drive():\n",
        "    \"\"\"\n",
        "    Function to load a DataFrame from a Google Drive file path.\n",
        "    \"\"\"\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Load the file from Google Drive\n",
        "    file_path = \"/content/drive/My Drive/Train.csv\"  # Replace with the saved file path\n",
        "    df = pd.read_csv(file_path)\n",
        "    return df\n",
        "\n",
        "def summarize_null_columns(dataframe):\n",
        "    \"\"\"\n",
        "    Function to find columns with null values and display detailed information\n",
        "    including column name, total null count, percentage of nulls, and data type.\n",
        "    \"\"\"\n",
        "    # Find total nulls and percentage of nulls for each column\n",
        "    columns_with_nulls = dataframe.isnull().sum()  # Total nulls per column\n",
        "    null_percentage = (columns_with_nulls / len(dataframe)) * 100  # Percentage of nulls\n",
        "\n",
        "    # Iterate through columns and display only those with null values\n",
        "    for col in dataframe.columns:\n",
        "        if columns_with_nulls[col] > 0:  # Check if the column has nulls\n",
        "            col_dtype = dataframe[col].dtype  # Get column data type\n",
        "            print(f\"Column: {col}\")\n",
        "            print(f\"  - Data Type: {col_dtype}\")\n",
        "            print(f\"  - Total Null Rows: {columns_with_nulls[col]}\")\n",
        "            print(f\"  - Percentage of Nulls: {null_percentage[col]:.2f}%\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "def drop_columns_with_high_nulls(dataframe, threshold=500):\n",
        "    \"\"\"\n",
        "    Function to drop columns from the DataFrame that have more than 'threshold' null values.\n",
        "\n",
        "    Parameters:\n",
        "    dataframe (pd.DataFrame): The input DataFrame.\n",
        "    threshold (int): The maximum allowable null rows for a column. Columns with nulls > threshold will be dropped.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: A new DataFrame with columns dropped based on the condition.\n",
        "    \"\"\"\n",
        "    print(\"drop_columns_with_high_nulls START\")\n",
        "\n",
        "    # Identify columns with null counts greater than the threshold\n",
        "    columns_to_drop = dataframe.columns[dataframe.isnull().sum() > threshold]\n",
        "\n",
        "    # Drop these columns from the DataFrame\n",
        "    cleaned_dataframe = dataframe.drop(columns=columns_to_drop)\n",
        "\n",
        "    print(\"drop_columns_with_high_nulls END\")\n",
        "\n",
        "    #print(f\"Dropped columns: {list(columns_to_drop)}\")\n",
        "    return cleaned_dataframe\n",
        "\n",
        "def update_MachineHoursCurrentMeter(df):\n",
        "  print(\"update_MachineHoursCurrentMeter START\")\n",
        "\n",
        "  # Update the 'MachineHoursCurrentMeter' column: replace NaN or 0 with 0\n",
        "  column_name = 'MachineHoursCurrentMeter'\n",
        "\n",
        "  # Check if the column exists\n",
        "  if column_name in df.columns:\n",
        "      df[column_name] = df[column_name].fillna(0)  # Replace NaN with 0\n",
        "      df[column_name] = df[column_name].replace(0, 0)  # Ensure 0 stays 0\n",
        "      #print(f\"Updated '{column_name}' column successfully!\")\n",
        "  #else:\n",
        "      #print(f\"Column '{column_name}' does not exist in the DataFrame.\")\n",
        "\n",
        "  print(\"update_MachineHoursCurrentMeter END\")\n",
        "\n",
        "  return df\n",
        "\n",
        "def update_auctioneerID(df):\n",
        "  print(\"update_auctioneerID START\")\n",
        "\n",
        "  # Assuming 'df' is your DataFrame\n",
        "  # Fill null values in the 'auctioneerID' column with 100.0\n",
        "  if 'auctioneerID' in df.columns:\n",
        "      df['auctioneerID'] = df['auctioneerID'].fillna(100.0)\n",
        "      #print(\"'auctioneerID' null values filled with 100.0 successfully!\")\n",
        "  #else:\n",
        "  #    print(\"Column 'auctioneerID' does not exist in the DataFrame.\")\n",
        "  print(\"update_auctioneerID END\")\n",
        "\n",
        "  return df\n",
        "\n",
        "def update_Enclosure(df):\n",
        "  print(\"update_Enclosure START\")\n",
        "\n",
        "  # Replace null values in the 'Enclosure' column with 'N/A'\n",
        "  if 'Enclosure' in df.columns:\n",
        "      df['Enclosure'] = df['Enclosure'].fillna('N/A')\n",
        "      #print(\"Replaced null values in the 'Enclosure' column with 'N/A'.\")\n",
        "  #else:\n",
        "      #print(\"Column 'Enclosure' does not exist in the DataFrame.\")\n",
        "\n",
        "  print(\"update_Enclosure END\")\n",
        "\n",
        "  return df\n",
        "'''\n",
        "# Function to calculate YearMade based on ModelID\n",
        "def calc_YearMade(df,model_id):\n",
        "    valid_years = df.loc[(df['ModelID'] == model_id) & (df['YearMade'] > 1000), 'YearMade']\n",
        "    return int(valid_years.mean()) if not valid_years.empty else 1000  # Default if no valid years exist\n",
        "'''\n",
        "# Function to calculate YearMade based on ModelID\n",
        "def calc_YearMade(df, model_id):\n",
        "    valid_years = df.loc[(df['ModelID'] == model_id) & (df['YearMade'] > 1000), 'YearMade']\n",
        "    return int(valid_years.mean()) if not valid_years.empty else 1000  # Default if no valid years exist\n",
        "\n",
        "\n",
        "def count_total_rows_per_column(df,column_name):\n",
        "  return df[df[column_name] == 1000].shape[0]\n",
        "\n",
        "\n",
        "#First run OA\n",
        "def first_run_25012025():\n",
        "  df=load_dataframe_from_drive()\n",
        "\n",
        "  missing_values_histogram(df,df.columns)\n",
        "\n",
        "  #df.info()\n",
        "\n",
        "  #df.describe()\n",
        "\n",
        "  df=update_MachineHoursCurrentMeter(df)\n",
        "  df=update_auctioneerID(df)\n",
        "\n",
        "  #print(\"columns with null values:\", columns_with_nulls(df,False))\n",
        "  #summarize_null_columns(df)\n",
        "\n",
        "  df = drop_columns_with_high_nulls(df, threshold=500)\n",
        "  df=update_Enclosure(df)\n",
        "  df=convert_date_columns(df,'saledate')\n",
        "  df=encode_all_categories(df)\n",
        "  model,xs=train_model(df=df,column_to_predict='SalePrice')#took 8 min runtime\n",
        "  return model,xs\n",
        "\n",
        "#Second run OA remove cols and outliners\n",
        "def Second_run_26012025():\n",
        "  df=load_dataframe_from_drive()\n",
        "  df=update_MachineHoursCurrentMeter(df)\n",
        "  df=update_auctioneerID(df)\n",
        "  df = drop_columns_with_high_nulls(df, threshold=500)\n",
        "  df=update_Enclosure(df)\n",
        "  df=convert_date_columns(df,'saledate')\n",
        "  df=encode_all_categories(df)\n",
        "\n",
        "  sampled_df = df.sample(n=25000, random_state=42)  # Example: 10,000 rows\n",
        "\n",
        "  model,xs=train_model(df=sampled_df,column_to_predict='SalePrice')\n",
        "\n",
        "  feature_importances(model)\n",
        "\n",
        "  sampled_df = sampled_df.drop(columns=['is_weekend','datasource','MachineHoursCurrentMeter','day_of_week','auctioneerID'], errors='ignore')  # 'errors=\"ignore\"' prevents errors if columns don't exist\n",
        "\n",
        "  model,xs=train_model(df=sampled_df,column_to_predict='SalePrice')\n",
        "\n",
        "  sampled_df=find_outliers(df=sampled_df,column_to_predict='SalePrice')\n",
        "\n",
        "  # Assuming df_with_anomalies is the DataFrame containing anomaly labels\n",
        "  anomalies = sampled_df[sampled_df['anomaly_label'] == 'Anomaly']\n",
        "\n",
        "  # Display rows where 'anomaly_label' is 'Anomaly'\n",
        "  print(anomalies)\n",
        "\n",
        "  # Optionally, display the count of anomalies\n",
        "  print(f\"Number of anomalies detected: {len(anomalies)}\")\n",
        "\n",
        "  # Remove all rows where 'anomaly_label' is 'Anomaly'\n",
        "  sampled_df = sampled_df[sampled_df['anomaly_label'] != 'Anomaly']\n",
        "\n",
        "  # Drop the columns 'anomaly_label', 'anomaly_score', and 'anomaly'\n",
        "  columns_to_drop = ['anomaly_label', 'anomaly_score', 'anomaly']\n",
        "  sampled_df = sampled_df.drop(columns=columns_to_drop, errors='ignore')\n",
        "\n",
        "  model,xs=train_model(df=sampled_df,column_to_predict='SalePrice')\n",
        "\n",
        "  perm_importance_model(model,xs)\n",
        "\n",
        "  sampled_df = sampled_df.drop(columns=['day','month','state','MachineID','SalesID','ProductGroupDesc'], errors='ignore')  # 'errors=\"ignore\"' prevents errors if columns don't exist\n",
        "\n",
        "  model,xs=train_model(df=sampled_df,column_to_predict='SalePrice')\n",
        "\n",
        "  perm_importance_model(model,xs)\n",
        "\n",
        "  return model,xs\n",
        "\n",
        "#First run OA 27012025\n",
        "def First_run_27012025():\n",
        "  df=load_dataframe_from_drive()\n",
        "\n",
        "  df=update_NaN_To_None_or_Unspecified(df)\n",
        "\n",
        "  df=update_MachineHoursCurrentMeter(df)\n",
        "  df=update_auctioneerID(df)\n",
        "  df = drop_columns_with_high_nulls(df, threshold=500)\n",
        "  df=update_Enclosure(df)\n",
        "  df=convert_date_columns(df,'saledate')\n",
        "  df=encode_all_categories(df)\n",
        "\n",
        "  sampled_df = df.sample(n=25000, random_state=42)  # Example: 10,000 rows\n",
        "\n",
        "  model,xs=train_model(df=sampled_df,column_to_predict='SalePrice')\n",
        "\n",
        "  feature_importances(model)\n",
        "\n",
        "  sampled_df = sampled_df.drop(columns=['is_weekend','datasource','MachineHoursCurrentMeter','day_of_week','auctioneerID'], errors='ignore')  # 'errors=\"ignore\"' prevents errors if columns don't exist\n",
        "\n",
        "  model,xs=train_model(df=sampled_df,column_to_predict='SalePrice')\n",
        "\n",
        "  sampled_df=find_outliers(df=sampled_df,column_to_predict='SalePrice')\n",
        "\n",
        "  # Assuming df_with_anomalies is the DataFrame containing anomaly labels\n",
        "  anomalies = sampled_df[sampled_df['anomaly_label'] == 'Anomaly']\n",
        "\n",
        "  # Display rows where 'anomaly_label' is 'Anomaly'\n",
        "  print(anomalies)\n",
        "\n",
        "  # Optionally, display the count of anomalies\n",
        "  print(f\"Number of anomalies detected: {len(anomalies)}\")\n",
        "\n",
        "  # Remove all rows where 'anomaly_label' is 'Anomaly'\n",
        "  sampled_df = sampled_df[sampled_df['anomaly_label'] != 'Anomaly']\n",
        "\n",
        "  # Drop the columns 'anomaly_label', 'anomaly_score', and 'anomaly'\n",
        "  columns_to_drop = ['anomaly_label', 'anomaly_score', 'anomaly']\n",
        "  sampled_df = sampled_df.drop(columns=columns_to_drop, errors='ignore')\n",
        "\n",
        "  model,xs=train_model(df=sampled_df,column_to_predict='SalePrice')\n",
        "\n",
        "  perm_importance_model(model,xs)\n",
        "\n",
        "  sampled_df = sampled_df.drop(columns=['day','month','state','MachineID','SalesID','ProductGroupDesc'], errors='ignore')  # 'errors=\"ignore\"' prevents errors if columns don't exist\n",
        "\n",
        "  model,xs=train_model(df=sampled_df,column_to_predict='SalePrice')\n",
        "\n",
        "  importance_model=perm_importance_model(model,xs)\n",
        "\n",
        "  # Drop columns where 'Permutation Importance' is less than 0.02\n",
        "  importance_model_filtered = importance_model[importance_model['Permutation Importance'] >= 0.02]\n",
        "\n",
        "  # Display the filtered DataFrame\n",
        "  print(importance_model_filtered)\n",
        "\n",
        "  # Optionally, list the dropped columns\n",
        "  dropped_columns = importance_model[importance_model['Permutation Importance'] < 0.02]['Feature']\n",
        "  print(\"Dropped columns:\", dropped_columns.tolist())\n",
        "  sampled_df.drop(columns=dropped_columns, inplace=True)\n",
        "\n",
        "  model,xs=train_model(df=sampled_df,column_to_predict='SalePrice')\n",
        "\n",
        "  return model,xs\n",
        "\n",
        "\n",
        "#First run OA 27012025\n",
        "def First_run_02022025():\n",
        "  pd.set_option('display.expand_frame_repr', False)\n",
        "\n",
        "  df=load_dataframe_from_drive()\n",
        "  print(count_total_rows_per_column(df,'YearMade'))\n",
        "  # Update YearMade where it is 1000\n",
        "  df.loc[df['YearMade'] == 1000, 'YearMade'] = df.loc[df['YearMade'] == 1000, 'ModelID'].apply(calc_YearMade)\n",
        "  print(count_total_rows_per_column(df,'YearMade'))\n",
        "\n",
        "  df=update_NaN_To_None_or_Unspecified(df)\n",
        "\n",
        "  df=update_MachineHoursCurrentMeter(df)\n",
        "  df=update_auctioneerID(df)\n",
        "  df = drop_columns_with_high_nulls(df, threshold=500)\n",
        "  df=update_Enclosure(df)\n",
        "  df=convert_date_columns(df,'saledate')\n",
        "  df=encode_all_categories(df)\n",
        "\n",
        "  sampled_df = df.sample(n=25000, random_state=42)  # Example: 10,000 rows\n",
        "\n",
        "  model,xs=train_model(df=sampled_df,column_to_predict='SalePrice')\n",
        "\n",
        "  feature_importances(model)\n",
        "\n",
        "  sampled_df = sampled_df.drop(columns=['is_weekend','datasource','MachineHoursCurrentMeter','day_of_week','auctioneerID'], errors='ignore')  # 'errors=\"ignore\"' prevents errors if columns don't exist\n",
        "\n",
        "  model,xs=train_model(df=sampled_df,column_to_predict='SalePrice')\n",
        "\n",
        "  sampled_df=find_outliers(df=sampled_df,column_to_predict='SalePrice')\n",
        "\n",
        "  # Assuming df_with_anomalies is the DataFrame containing anomaly labels\n",
        "  anomalies = sampled_df[sampled_df['anomaly_label'] == 'Anomaly']\n",
        "\n",
        "  # Display rows where 'anomaly_label' is 'Anomaly'\n",
        "  print(anomalies)\n",
        "\n",
        "  # Optionally, display the count of anomalies\n",
        "  print(f\"Number of anomalies detected: {len(anomalies)}\")\n",
        "\n",
        "  # Remove all rows where 'anomaly_label' is 'Anomaly'\n",
        "  sampled_df = sampled_df[sampled_df['anomaly_label'] != 'Anomaly']\n",
        "\n",
        "  # Drop the columns 'anomaly_label', 'anomaly_score', and 'anomaly'\n",
        "  columns_to_drop = ['anomaly_label', 'anomaly_score', 'anomaly']\n",
        "  sampled_df = sampled_df.drop(columns=columns_to_drop, errors='ignore')\n",
        "\n",
        "  model,xs=train_model(df=sampled_df,column_to_predict='SalePrice')\n",
        "\n",
        "  perm_importance_model(model,xs)\n",
        "\n",
        "  sampled_df = sampled_df.drop(columns=['day','month','state','MachineID','SalesID','ProductGroupDesc'], errors='ignore')  # 'errors=\"ignore\"' prevents errors if columns don't exist\n",
        "\n",
        "  model,xs=train_model(df=sampled_df,column_to_predict='SalePrice')\n",
        "\n",
        "  importance_model=perm_importance_model(model,xs)\n",
        "\n",
        "  # Drop columns where 'Permutation Importance' is less than 0.02\n",
        "  importance_model_filtered = importance_model[importance_model['Permutation Importance'] >= 0.02]\n",
        "\n",
        "  # Display the filtered DataFrame\n",
        "  print(importance_model_filtered)\n",
        "\n",
        "  # Optionally, list the dropped columns\n",
        "  dropped_columns = importance_model[importance_model['Permutation Importance'] < 0.02]['Feature']\n",
        "  print(\"Dropped columns:\", dropped_columns.tolist())\n",
        "  sampled_df.drop(columns=dropped_columns, inplace=True)\n",
        "\n",
        "  model,xs=train_model(df=sampled_df,column_to_predict='SalePrice')\n",
        "\n",
        "  return model,xs\n",
        "\n",
        "def prepare_dataframe(df_pre:pd.DataFrame):\n",
        "  # Update YearMade where it is 1000\n",
        "  df_pre.loc[df_pre['YearMade'] == 1000, 'YearMade'] = df_pre.loc[df_pre['YearMade'] == 1000, 'ModelID'].apply(lambda model_id: calc_YearMade(df_pre, model_id))\n",
        "  df_pre=update_NaN_To_None_or_Unspecified(df_pre)\n",
        "  df_pre=update_MachineHoursCurrentMeter(df_pre)\n",
        "  df_pre=update_auctioneerID(df_pre)\n",
        "  df_pre = drop_columns_with_high_nulls(df_pre, threshold=500)\n",
        "  df_pre=update_Enclosure(df_pre)\n",
        "  df_pre=convert_date_columns(df_pre,'saledate')\n",
        "  df_pre=encode_all_categories(df_pre)\n",
        "  df_pre = df_pre.drop(columns=['is_weekend','datasource','MachineHoursCurrentMeter','day_of_week','auctioneerID'], errors='ignore')  # 'errors=\"ignore\"' prevents errors if columns don't exist\n",
        "  model,xs=train_model(df=df_pre,column_to_predict='SalePrice')\n",
        "\n",
        "\n",
        "  #df_pre=find_outliers(df=df_pre,column_to_predict='SalePrice')\n",
        "\n",
        "  # Assuming df_with_anomalies is the DataFrame containing anomaly labels\n",
        "  #anomalies = df_pre[df_pre['anomaly_label'] == 'Anomaly']\n",
        "  # Remove all rows where 'anomaly_label' is 'Anomaly'\n",
        "  #df_pre = df_pre[df_pre['anomaly_label'] != 'Anomaly']\n",
        "\n",
        "  # Drop the columns 'anomaly_label', 'anomaly_score', and 'anomaly'\n",
        "  #columns_to_drop = ['anomaly_label', 'anomaly_score', 'anomaly']\n",
        "  #df_pre = df_pre.drop(columns=columns_to_drop, errors='ignore')\n",
        "\n",
        "\n",
        "  df_pre = df_pre.drop(columns=['day','month','state','MachineID','SalesID','ProductGroupDesc'], errors='ignore')  # 'errors=\"ignore\"' prevents errors if columns don't exist\n",
        "  #dropped_columns = ['UsageBand', 'fiSecondaryDesc', 'fiModelSeries', 'fiModelDescriptor', 'ProductSize', 'Drive_System', 'Stick', 'Engine_Horsepower', 'Track_Type', 'Grouser_Type', 'Differential_Type', 'Steering_Controls']\n",
        "  #df_pre.drop(columns=dropped_columns, inplace=True)\n",
        "\n",
        "  return df_pre\n",
        "\n",
        "def test_model(model):\n",
        "  pd.set_option('display.expand_frame_repr', False)\n",
        "\n",
        "  file_path = \"/content/drive/My Drive/Valid.csv\"  # Replace with the saved file path\n",
        "  df_valid = pd.read_csv(file_path)\n",
        "  df_valid['SalePrice']=99\n",
        "  print(df_valid.head(5))\n",
        "  df_valid=prepare_dataframe(df_valid)\n",
        "  print(df_valid.head(5))\n",
        "\n",
        "  X = df_valid.drop(columns=['SalePrice'])\n",
        "  y = df_valid['SalePrice']\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "  y_test_pred = model.predict(X_test)\n",
        "  # 7. print the RMSE accuracy of the baseline (std dev)\n",
        "  print(\"Test RMSE:\", RMSE(y_test, y_test_pred))\n",
        "  df_valid.columns\n",
        "  df_valid[['SalesID', 'SalePrice']].to_csv(\"/path/to/output.csv\", index=False)\n",
        "\n",
        "\n",
        "#TEST run OA 02022025\n",
        "def TEST_run_02022025():\n",
        "  df=load_dataframe_from_drive()\n",
        "  df=prepare_dataframe(df)\n",
        "  model,xs=train_model(df=df,column_to_predict='SalePrice')\n",
        "\n",
        "  return model,xs\n",
        "\n",
        "\n",
        "'''\n",
        "import pandas as pd\n",
        "\n",
        "# Compute correlation matrix\n",
        "corr_matrix = df.corr()\n",
        "\n",
        "# Find highly correlated features (e.g., correlation > 0.85)\n",
        "high_corr_features = set()\n",
        "threshold = 0.85\n",
        "\n",
        "for i in range(len(corr_matrix.columns)):\n",
        "    for j in range(i):\n",
        "        if abs(corr_matrix.iloc[i, j]) > threshold:\n",
        "            colname = corr_matrix.columns[i]\n",
        "            high_corr_features.add(colname)\n",
        "\n",
        "print(\"Highly correlated features:\", high_corr_features)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "corr_matrix = df.corr()\n",
        "threshold = 0.60\n",
        "\n",
        "# Apply threshold filter to only show correlations above 0.5\n",
        "filtered_corr = corr_matrix[(abs(corr_matrix) > threshold) & (abs(corr_matrix) < 1)]\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(filtered_corr, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "plt.title(\"Example Correlation Heatmap\")\n",
        "plt.show()\n",
        "'''"
      ],
      "metadata": {
        "id": "GetwQ5yXBMFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#erez functions1\n",
        "\n",
        "def add_age_machine(df):\n",
        "    # Ensure 'saledate' is in datetime format\n",
        "    df['saledate'] = pd.to_datetime(df['saledate'], errors='coerce')\n",
        "\n",
        "    # Extract the year from 'saledate'\n",
        "    df['saledate_year'] = df['saledate'].dt.year\n",
        "\n",
        "    # Calculate the machine's age\n",
        "    df['age_machine'] = df['saledate_year'] - df['YearMade']\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply the function\n",
        "df = add_age_machine(df)\n",
        "\n",
        "# Display the DataFrame with the new 'age_machine' column\n",
        "df.info()\n",
        "print(df[['YearMade', 'saledate', 'age_machine']])\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "ElpLtGhC3zSk",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#erez functions2\n",
        "for col in df.select_dtypes(include=['object']).columns:\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "# Replace NaN with 0\n",
        "df[col] = df[col].fillna(0).astype(int)\n",
        "\n",
        "# Or, drop rows with NaN values\n",
        "df.dropna(subset=[col], inplace=True)\n",
        "\n",
        "df[col] = df[col].replace({'\\$': '', ',': ''}, regex=True).astype(float)\n",
        "\n",
        "for col in df.select_dtypes(include=['object']).columns:\n",
        "    if df[col].str.isnumeric().all():  # Check if all values are numeric\n",
        "        df[col] = df[col].astype(int)\n",
        "    else:\n",
        "        print(f\"Cannot convert column {col} to int\")\n",
        "\n",
        "# Convert all object columns that can be converted to integers\n",
        "for col in df.select_dtypes(include=['object']).columns:\n",
        "    try:\n",
        "        df[col] = df[col].astype(int)\n",
        "    except ValueError:\n",
        "        print(f\"Cannot convert column {col} to int\")\n",
        "\n",
        "def robust_convert(df):\n",
        "    for col in df.select_dtypes(include=['object']).columns:\n",
        "        try:\n",
        "            # Step 1: Remove non-numeric symbols (if any, e.g., '$')\n",
        "            df[col] = df[col].replace({'\\$': '', ',': '', '%': ''}, regex=True)\n",
        "\n",
        "            # Step 2: Convert to numeric, coerce errors to NaN\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "            # Step 3: Handle NaN values (e.g., replace with 0 or mean of column)\n",
        "            df[col] = df[col].fillna(0).astype(int)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to convert column {col}: {e}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply the robust conversion\n",
        "df = robust_convert(df)\n",
        "df.info()\n",
        "\n",
        "# Separate numerical columns and non-numerical columns\n",
        "numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "non_numerical_cols = df.select_dtypes(exclude=['float64', 'int64']).columns\n",
        "\n",
        "# Optionally, convert datetime columns to numerical values\n",
        "if 'saledate' in non_numerical_cols:\n",
        "    df['saledate'] = (df['saledate'] - df['saledate'].min()) / pd.Timedelta(days=1)\n",
        "\n",
        "# Now apply KNNImputer only on numerical columns\n",
        "imputer = KNNImputer(n_neighbors=5)\n",
        "\n",
        "# Make sure that you're passing the correct DataFrame subset\n",
        "numerical_data = df[numerical_cols].copy()  # Avoid modifying the original DataFrame directly\n",
        "\n",
        "# Perform imputation\n",
        "imputed_data = imputer.fit_transform(numerical_data)\n",
        "\n",
        "# Convert imputed data back to DataFrame and ensure column names match\n",
        "df_imputed = pd.DataFrame(imputed_data, columns=numerical_cols)\n",
        "\n",
        "# Replace the original columns with the imputed ones\n",
        "df[numerical_cols] = df_imputed\n",
        "\n",
        "# For categorical columns, you can impute using the mode\n",
        "for col in non_numerical_cols:\n",
        "    if df[col].dtype == 'object':\n",
        "        df[col].fillna(df[col].mode()[0], inplace=True)\n",
        "\n",
        "# If you converted datetime columns, convert them back to datetime format\n",
        "df['saledate'] = pd.to_datetime(df['saledate'], unit='D', origin=pd.Timestamp('2020-01-01'))\n",
        "\n",
        "# Check for any remaining null values\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "tZ9_sbuorTTI",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#erez function3\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def handle_nulls(df):\n",
        "    for column in df.columns:\n",
        "        # Calculate the percentage of nulls in the column\n",
        "        null_percentage = df[column].isnull().mean() * 100\n",
        "\n",
        "        if null_percentage > 80:\n",
        "            # Drop the column if more than 80% values are null\n",
        "            df.drop(column, axis=1, inplace=True)\n",
        "        else:\n",
        "            # Fill the null values with the mean of the column\n",
        "            df[column].fillna(df[column].mean(), inplace=True)\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "p31GcYVyIgi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#eeitan function\n",
        "def calculate_machine_age(df):\n",
        "    # Ensure that 'SaleDate' is in datetime format\n",
        "    df['saledate'] = pd.to_datetime(df['saledate'])\n",
        "    # Extract the year from 'SaleDate'\n",
        "    df['saledate'] = df['saledate'].dt.year\n",
        "\n",
        "    # Calculate MachineAge by subtracting 'YearMade' from the 'SaleYear'\n",
        "    df['machineage'] = df['saleyear'] - df['yearmade']\n",
        "\n",
        "    # Drop the 'SaleYear' column if you don't need it anymore\n",
        "    df.drop(columns=['saleyear'], inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "df=load_dataframe_from_drive()\n",
        "df['saledate']\n",
        "# Example usage:\n",
        "# Assuming your dataframe is named `df`\n",
        "df = calculate_machine_age(df)\n",
        "\n",
        "# Display the dataframe with the new 'MachineAge' column\n",
        "print(df[['YearMade', 'SaleDate', 'MachineAge']].head())\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "r6_vim-k4By6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sorel functions"
      ],
      "metadata": {
        "id": "1EojaDZG4LoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model,xs=train_model(df=df,column_to_predict='SalePrice')#took 8 min runtime"
      ],
      "metadata": {
        "id": "rjC7CgKHEDBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RMSE Baseline accuracy: 22932.4005340408 Train RMSE: 2864.123815046 Test RMSE: 7633.339248575835 0 Enclosure 0.240869 YearMade 0.147308 fiProductClassDesc 0.117304 fiModelDesc 0.099289 ModelID 0.093105 year 0.080002 SalesID 0.043689 fiBaseModel 0.033948 ProductGroup 0.032850 ProductGroupDesc 0.029777 MachineID 0.023082 day 0.013131 month 0.013108 state 0.012552 auctioneerID 0.006424 day_of_week 0.006194 MachineHoursCurrentMeter 0.005897 datasource 0.000885 is_weekend 0.000586"
      ],
      "metadata": {
        "id": "9X3AVb8dEH69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ofer runtime\n",
        "#model,xs=first_run_25012025()\n",
        "#model,xs=Second_run_26012025()\n",
        "#model,xs=First_run_27012025()\n",
        "#model,xs=First_run_02022025()\n",
        "model,xs=TEST_run_02022025()\n",
        "print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
        "test_model(model)\n"
      ],
      "metadata": {
        "id": "YWjBrHg34pvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split  # For splitting the dataset\n",
        "from sklearn.ensemble import RandomForestRegressor  # For the Random Forest regression model\n",
        "import shap\n",
        "import numpy as np\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.metrics import mean_squared_log_error\n",
        "\n",
        "\n",
        "file_path = \"/content/drive/My Drive/Valid.csv\"  # Replace with the saved file path\n",
        "df_valid = pd.read_csv(file_path)\n",
        "df_valid['SalePrice']=None\n",
        "X = df_valid.drop(columns=['SalePrice'])\n",
        "y = df_valid['SalePrice']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "y_test_pred = model.predict(X_test)\n",
        "# 7. print the RMSE accuracy of the baseline (std dev)\n",
        "print(\"RMSE Baseline accuracy:\", y_test.std())\n",
        "print(\"Train RMSE:\", RMSE(y_train, y_train_pred))\n",
        "print(\"Test RMSE:\", RMSE(y_test, y_test_pred))\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "SUeiDWWZfs9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#erez runtime"
      ],
      "metadata": {
        "id": "FjxhNAey39au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#eitan runtime"
      ],
      "metadata": {
        "id": "LqMwx8dz4Uhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sorel runtime"
      ],
      "metadata": {
        "id": "HCO_KaS14X16"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}