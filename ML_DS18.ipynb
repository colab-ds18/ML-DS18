{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/colab-ds18/ML-DS18/blob/main/ML_DS18.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ofer functions\n",
        "'''\n",
        "\n",
        "General Funtions Author OA\n",
        "Date 29/01/2025\n",
        "'''\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split  # For splitting the dataset\n",
        "from sklearn.ensemble import RandomForestRegressor  # For the Random Forest regression model\n",
        "import shap\n",
        "import numpy as np\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.metrics import mean_squared_log_error\n",
        "import re\n",
        "\n",
        "'''\n",
        "def fill_missing_values(df):\n",
        "    for col in df.select_dtypes(include=['number']):\n",
        "        df[col] = df[col].fillna(df[col].median())  # Use assignment instead of inplace=True\n",
        "    for col in df.select_dtypes(include=['object']):\n",
        "        df[col] = df[col].fillna('None or Unspecified')  # Use assignment instead of inplace=True\n",
        "    return df  # Return modified DataFrame\n",
        "'''\n",
        "\n",
        "def save_dataframe(df,file_name):\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  file_path = \"/content/drive/My Drive/\"+file_name+\".csv\"\n",
        "  df.to_csv(file_path, index=False)\n",
        "  print(f\"File saved successfully at: {file_path}\")\n",
        "\n",
        "def load_dataframe_from_drive(file_name='Train'):\n",
        "    \"\"\"\n",
        "    Function to load a DataFrame from a Google Drive file path.\n",
        "    \"\"\"\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Load the file from Google Drive\n",
        "    file_path = \"/content/drive/My Drive/\"+file_name+\".csv\"  # Replace with the saved file path\n",
        "    df = pd.read_csv(file_path)\n",
        "    return df\n",
        "\n",
        "def save_model(model,model_name):\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  import joblib\n",
        "\n",
        "  # Assuming 'rf_model' is your trained RandomForestRegressor\n",
        "  model_path = '/content/drive/My Drive/'+model_name+'.pkl'\n",
        "  joblib.dump(model, model_path)\n",
        "\n",
        "  print(f\"Model saved at {model_path}\")\n",
        "\n",
        "def load_model(model_name):\n",
        "  import joblib\n",
        "\n",
        "  model = joblib.load('/content/drive/My Drive/'+model_name+'.pkl')\n",
        "  print(\"Model loaded successfully!\")\n",
        "  return model\n",
        "\n",
        "def generate_X_Y_params(df,column_to_predict,test_size_value=0.3,random_state_value=42):\n",
        "    X = df.drop(columns=[column_to_predict])\n",
        "    y = df[column_to_predict]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size_value, random_state=random_state_value)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "'''\n",
        "def show_unique_values(df, column_name):\n",
        "  if column_name not in df.columns:\n",
        "    raise ValueError(f\"Column '{column_name}' does not exist in the DataFrame.\")\n",
        "    unique_products = list_unique_values(df, column_name)\n",
        "    print(unique_products)\n",
        "    null_stats_per_column(df,column_name)\n",
        "\n",
        "'''\n",
        "\n",
        "def list_unique_values(df, column_name):\n",
        "    \"\"\"\n",
        "    Function to list all unique values in a given column of a DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    - df: pandas DataFrame\n",
        "    - column_name: Name of the column to find unique values\n",
        "\n",
        "    Returns:\n",
        "    - A list of unique values in the specified column.\n",
        "    \"\"\"\n",
        "    if column_name not in df.columns:\n",
        "        raise ValueError(f\"Column '{column_name}' does not exist in the DataFrame.\")\n",
        "\n",
        "    unique_values = df[column_name].unique()\n",
        "    return unique_values\n",
        "\n",
        "\n",
        "def update_NaN_To_None_or_Unspecified(df):\n",
        "    \"\"\"\n",
        "    Updates NaN values to 'None or Unspecified' in columns where the value\n",
        "    'None or Unspecified' is already present.\n",
        "    \"\"\"\n",
        "    print(\"update_NaN_To_None_or_Unspecified START\")\n",
        "\n",
        "    update_to = 'None or Unspecified'\n",
        "\n",
        "    for col in df.columns:\n",
        "        unique_values = list_unique_values(df, col)  # Get unique values for the column\n",
        "\n",
        "        # Check if 'None or Unspecified' exists and if there are NaN values\n",
        "        if update_to in unique_values and pd.isnull(unique_values).any():\n",
        "            df[col] = df[col].fillna(update_to)  # Fill NaN with the specified value\n",
        "\n",
        "    print(\"update_NaN_To_None_or_Unspecified END\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def columns_with_nulls(df,only_numeric_columns=True):\n",
        "  '''\n",
        "  Function to list all columns with null values in a given DataFrame.\n",
        "  '''\n",
        "  print(\"columns_with_nulls START/END\")\n",
        "  if only_numeric_columns:\n",
        "    # Get numeric columns only\n",
        "    numeric_columns = df.select_dtypes(include=['number'])\n",
        "\n",
        "    # Find numeric columns with null values\n",
        "    numeric_columns_with_null = numeric_columns.columns[numeric_columns.isnull().any()]\n",
        "    return numeric_columns_with_null.tolist()\n",
        "  else:\n",
        "    return df.columns[df.isnull().any()]\n",
        "\n",
        "\n",
        "def null_stats_per_column(df,chosen_column):\n",
        "  '''\n",
        "  Function to display the number of null rows in a given column of a DataFrame.\n",
        "  '''\n",
        "  print(\"null_stats_per_column START/END\")\n",
        "\n",
        "  # Count the number of null rows in the chosen column\n",
        "  null_count = df[chosen_column].isnull().sum()\n",
        "\n",
        "  # Calculate the percentage of nulls in the chosen column\n",
        "  null_percentage = (null_count / len(df)) * 100\n",
        "\n",
        "  # Display the result\n",
        "  print(f\"Column: {chosen_column}\")\n",
        "  print(f\"Null Count: {null_count}\")\n",
        "  print(f\"Null Percentage: {null_percentage:.2f}%\")\n",
        "\n",
        "\n",
        "def missing_values_histogram(df,columns):\n",
        "    '''\n",
        "    Function to plot a histogram of missing values in a given DataFrame.\n",
        "    '''\n",
        "    data=[{column:df[column].value_counts().sum()} for column in columns ]\n",
        "    # Convert the list of dictionaries to a DataFrame\n",
        "    formatted_data = pd.DataFrame([{name:count for single_data in data for name, count in single_data.items()}]).T.reset_index()\n",
        "    formatted_data.columns = ['Column', 'Count']\n",
        "    # Create a bar plot\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    ax = sns.barplot(x='Column', y='Count', data=formatted_data, hue='Column',legend=False)\n",
        "\n",
        "\n",
        "    # label count on top of each bar\n",
        "    for index, row in formatted_data.iterrows():\n",
        "        ax.text(index, row['Count'] + 10, str(row['Count']), color='black', ha=\"center\", fontsize=10)\n",
        "\n",
        "    # Add title and axis labels\n",
        "    plt.title(\"Column Counts in Dataframe\")\n",
        "    plt.xlabel(\"Column\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def encode_all_categories(df):\n",
        "  '''\n",
        "  Function to convert all string columns in a given DataFrame into categories.\n",
        "  '''\n",
        "  print(\"encode_all_categories START\")\n",
        "  for col in df.select_dtypes(['object']):\n",
        "    df[col] = df[col].astype('category')\n",
        "\n",
        "  for col in df.select_dtypes(['category']):\n",
        "    df[col] = df[col].cat.codes\n",
        "\n",
        "  print(\"encode_all_categories END\")\n",
        "\n",
        "  return df\n",
        "\n",
        "def drop_nulls(df):\n",
        "  '''\n",
        "  Function to drop all rows with null values in a given DataFrame.\n",
        "  '''\n",
        "  df = df.dropna()\n",
        "  return df\n",
        "\n",
        "def RMSLE(y_test, y_pred):\n",
        "    '''\n",
        "    RSMLE approximates the percent change\n",
        "    '''\n",
        "    return np.sqrt(np.mean((np.log(y_pred) - np.log(y_test))**2))\n",
        "\n",
        "def RMSE(y_, y_pred_):\n",
        "    '''\n",
        "    RSME\n",
        "    '''\n",
        "    return ((y_ - y_pred_) ** 2).mean() ** 0.5\n",
        "\n",
        "def train_model(df,column_to_predict,test_size_value=0.3,random_state_value=42):\n",
        "    '''\n",
        "    Function to train a Random Forest regression model on a given DataFrame.\n",
        "    '''\n",
        "    print(\"train_model START\")\n",
        "\n",
        "    X = df.drop(columns=[column_to_predict])\n",
        "    y = df[column_to_predict]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size_value, random_state=random_state_value)\n",
        "\n",
        "    # model = DecisionTreeRegressor(\n",
        "    # min_samples_leaf=16\n",
        "    # max_depth=3\n",
        "    # )\n",
        "    #model = LinearRegression()\n",
        "    model = RandomForestRegressor()\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    # 7. print the RMSE accuracy of the baseline (std dev)\n",
        "    print(\"RMSE Baseline accuracy:\", y_test.std())\n",
        "    print(\"Train RMSE:\", RMSE(y_train, y_train_pred))\n",
        "    print(\"Test RMSE:\", RMSE(y_test, y_test_pred))\n",
        "    #print(\"Test RMSLE:\", RMSLE(y_test, y_test_pred))\n",
        "\n",
        "    print(\"train_model END\")\n",
        "\n",
        "    #display(pd.Series(model.feature_importances_, model.feature_names_in_).sort_values(ascending=False))\n",
        "    return model,(X_train, X_test, y_train, y_test)\n",
        "\n",
        "def feature_importances(model):\n",
        "  '''\n",
        "  Function to display the feature importances of a given Random Forest regression model.\n",
        "  '''\n",
        "  print(\"feature_importances START\")\n",
        "\n",
        "  dict(zip(model.feature_names_in_, model.feature_importances_))\n",
        "  fi = pd.Series(model.feature_importances_, index=model.feature_names_in_)\n",
        "  fi = fi.sort_values(ascending=False)\n",
        "  print(\"feature_importances END\")\n",
        "\n",
        "  return fi\n",
        "\n",
        "def explain_model_snap(model,X_test,water_fall_row=0):\n",
        "  '''\n",
        "  Function to explain the model's predictions using SHAP values.\n",
        "  '''\n",
        "  shap.initjs()\n",
        "  explainer = shap.Explainer(model)\n",
        "  explanation = explainer(X_test)  # New style\n",
        "  shap.summary_plot(explanation, X_test)\n",
        "\n",
        "  shap.plots.waterfall(explanation[water_fall_row])\n",
        "\n",
        "  shap.plots.waterfall(explanation[water_fall_row+2])\n",
        "\n",
        "  shap.plots.partial_dependence('fare', model.predict, X_test, feature_names=X_test.columns)\n",
        "\n",
        "  shap.plots.heatmap(explanation)\n",
        "\n",
        "  shap.plots.bar(explanation)\n",
        "\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import pandas as pd\n",
        "\n",
        "def detect_and_filter_anomalies(df, contamination=0.05, random_state=42):\n",
        "    \"\"\"\n",
        "    Detects anomalies in a DataFrame using IsolationForest and returns only normal data.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): The input DataFrame.\n",
        "        contamination (float): The proportion of anomalies (default: 5%).\n",
        "        random_state (int): Random state for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        normal_df (pd.DataFrame): DataFrame containing only normal rows, without anomaly columns.\n",
        "    \"\"\"\n",
        "    df_outliers = df.copy()  # Copy original DataFrame\n",
        "\n",
        "    # Select only numerical columns for anomaly detection\n",
        "    X = df_outliers.select_dtypes(include=['number']).copy()\n",
        "\n",
        "    # Handle missing values (important to uncomment if NaNs exist)\n",
        "    X.fillna(X.median(), inplace=True)\n",
        "\n",
        "    # Initialize IsolationForest\n",
        "    iso_forest = IsolationForest(contamination=contamination, random_state=random_state)\n",
        "    iso_forest.fit(X)\n",
        "\n",
        "    # Add anomaly scores and labels\n",
        "    df_outliers['anomaly_score'] = iso_forest.decision_function(X)  # Quantitative weirdness\n",
        "    df_outliers['anomaly'] = iso_forest.predict(X)  # Binary anomaly label\n",
        "    df_outliers['anomaly_label'] = df_outliers['anomaly'].map({1: 'Normal', -1: 'Anomaly'})\n",
        "\n",
        "    # Keep only normal data\n",
        "    normal_df = df_outliers[df_outliers['anomaly'] == 1].drop(columns=['anomaly_score', 'anomaly', 'anomaly_label'])\n",
        "\n",
        "    return normal_df\n",
        "\n",
        "def convert_date_columns(df, date_column):\n",
        "    \"\"\"\n",
        "    Convert date column into multiple numeric features like year, month, day, etc.\n",
        "    Handles potential issues like missing date_column or invalid data.\n",
        "    \"\"\"\n",
        "    print(\"convert_date_columns START\")\n",
        "\n",
        "    # Ensure the date_column exists in the DataFrame\n",
        "    if date_column not in df.columns:\n",
        "        raise KeyError(f\"Column '{date_column}' does not exist in the DataFrame.\")\n",
        "\n",
        "    # Convert the column to datetime\n",
        "    df[date_column] = pd.to_datetime(df[date_column], errors='coerce')  # Handle invalid date strings\n",
        "\n",
        "    # Check if any date values are invalid after conversion\n",
        "    if df[date_column].isnull().all():\n",
        "        raise ValueError(f\"All values in '{date_column}' could not be converted to datetime.\")\n",
        "\n",
        "    # Extract date features\n",
        "    df['year'] = df[date_column].dt.year\n",
        "    df['month'] = df[date_column].dt.month\n",
        "    df['day'] = df[date_column].dt.day\n",
        "    df['day_of_week'] = df[date_column].dt.dayofweek\n",
        "    df['is_weekend'] = df['day_of_week'] >= 5  # Saturday=5, Sunday=6\n",
        "\n",
        "    # Drop the original date column\n",
        "    df = df.drop(columns=[date_column])\n",
        "\n",
        "    print(\"convert_date_columns END\")\n",
        "\n",
        "    return df\n",
        "\n",
        "#    model = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
        "#    xs=(X_train, X_test, y_train, y_test)\n",
        "def perm_importance_model(model, xs):\n",
        "    '''\n",
        "    Function to compute Permutation Importance of a given Random Forest regression model.\n",
        "    '''\n",
        "    print(\"perm_importance_model START\")\n",
        "\n",
        "    X_train, X_test, y_train, y_test = xs\n",
        "\n",
        "    # Compute Permutation Importance\n",
        "    perm_importance = permutation_importance(\n",
        "        model, X_test, y_test, n_repeats=5, random_state=42, n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # Create the results DataFrame\n",
        "    importance_model = pd.DataFrame({\n",
        "        \"Feature\": model.feature_names_in_,\n",
        "        \"Permutation Importance\": perm_importance.importances_mean,\n",
        "        \"Permutation Std Deviation\": perm_importance.importances_std,\n",
        "        \"Model Importance\": model.feature_importances_,\n",
        "    })\n",
        "\n",
        "    # Add ranking for permutation and model importances\n",
        "    importance_model[\"Permutation Rank\"] = importance_model[\"Permutation Importance\"].rank(ascending=False)\n",
        "    importance_model[\"Model Rank\"] = importance_model[\"Model Importance\"].rank(ascending=False)\n",
        "\n",
        "    # Sort by Permutation Importance for display\n",
        "    importance_model = importance_model.sort_values(by=\"Permutation Importance\", ascending=False)\n",
        "\n",
        "    # Display the DataFrame\n",
        "    #display(importance_model)\n",
        "\n",
        "    print(\"perm_importance_model END\")\n",
        "\n",
        "    return importance_model\n",
        "\n",
        "\n",
        "\n",
        "def summarize_null_columns(dataframe):\n",
        "    \"\"\"\n",
        "    Function to find columns with null values and display detailed information\n",
        "    including column name, total null count, percentage of nulls, and data type.\n",
        "    \"\"\"\n",
        "    # Find total nulls and percentage of nulls for each column\n",
        "    columns_with_nulls = dataframe.isnull().sum()  # Total nulls per column\n",
        "    null_percentage = (columns_with_nulls / len(dataframe)) * 100  # Percentage of nulls\n",
        "\n",
        "    # Iterate through columns and display only those with null values\n",
        "    for col in dataframe.columns:\n",
        "        if columns_with_nulls[col] > 0:  # Check if the column has nulls\n",
        "            col_dtype = dataframe[col].dtype  # Get column data type\n",
        "            print(f\"Column: {col}\")\n",
        "            print(f\"  - Data Type: {col_dtype}\")\n",
        "            print(f\"  - Total Null Rows: {columns_with_nulls[col]}\")\n",
        "            print(f\"  - Percentage of Nulls: {null_percentage[col]:.2f}%\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "def drop_columns_with_high_nulls(dataframe, threshold=0.05):\n",
        "    \"\"\"\n",
        "    Function to drop columns from the DataFrame that have more than 'threshold' null values.\n",
        "\n",
        "    Parameters:\n",
        "    dataframe (pd.DataFrame): The input DataFrame.\n",
        "    threshold (int): The maximum allowable null rows for a column. Columns with nulls > threshold will be dropped.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: A new DataFrame with columns dropped based on the condition.\n",
        "    \"\"\"\n",
        "    print(\"drop_columns_with_high_nulls START\")\n",
        "\n",
        "    # Identify columns with null counts greater than the threshold\n",
        "    columns_to_drop = dataframe.columns[dataframe.isnull().sum()/dataframe.shape[0] > threshold]\n",
        "\n",
        "    print(\"drop_columns_with_high_nulls END\")\n",
        "\n",
        "    return columns_to_drop\n",
        "\n",
        "\n",
        "def update_MachineHoursCurrentMeter(df):\n",
        "  print(\"update_MachineHoursCurrentMeter START\")\n",
        "\n",
        "  # Update the 'MachineHoursCurrentMeter' column: replace NaN or 0 with 0\n",
        "  column_name = 'MachineHoursCurrentMeter'\n",
        "\n",
        "  # Check if the column exists\n",
        "  if column_name in df.columns:\n",
        "      df[column_name] = df[column_name].fillna(0)  # Replace NaN with 0\n",
        "      df[column_name] = df[column_name].replace(0, 0)  # Ensure 0 stays 0\n",
        "      #print(f\"Updated '{column_name}' column successfully!\")\n",
        "  #else:\n",
        "      #print(f\"Column '{column_name}' does not exist in the DataFrame.\")\n",
        "\n",
        "  print(\"update_MachineHoursCurrentMeter END\")\n",
        "\n",
        "  return df\n",
        "\n",
        "def update_auctioneerID(df):\n",
        "  print(\"update_auctioneerID START\")\n",
        "\n",
        "  # Assuming 'df' is your DataFrame\n",
        "  # Fill null values in the 'auctioneerID' column with 100.0\n",
        "  if 'auctioneerID' in df.columns:\n",
        "      df['auctioneerID'] = df['auctioneerID'].fillna(100.0)\n",
        "      #print(\"'auctioneerID' null values filled with 100.0 successfully!\")\n",
        "  #else:\n",
        "  #    print(\"Column 'auctioneerID' does not exist in the DataFrame.\")\n",
        "  print(\"update_auctioneerID END\")\n",
        "\n",
        "  return df\n",
        "\n",
        "def update_Enclosure(df):\n",
        "  print(\"update_Enclosure START\")\n",
        "\n",
        "  # Replace null values in the 'Enclosure' column with 'N/A'\n",
        "  if 'Enclosure' in df.columns:\n",
        "      df['Enclosure'] = df['Enclosure'].fillna('None or Unspecified')\n",
        "      #print(\"Replaced null values in the 'Enclosure' column with 'N/A'.\")\n",
        "  #else:\n",
        "      #print(\"Column 'Enclosure' does not exist in the DataFrame.\")\n",
        "\n",
        "  print(\"update_Enclosure END\")\n",
        "\n",
        "  return df\n",
        "'''\n",
        "# Function to calculate YearMade based on ModelID\n",
        "def calc_YearMade(df,model_id):\n",
        "    valid_years = df.loc[(df['ModelID'] == model_id) & (df['YearMade'] > 1000), 'YearMade']\n",
        "    return int(valid_years.mean()) if not valid_years.empty else 1000  # Default if no valid years exist\n",
        "'''\n",
        "# Function to calculate YearMade based on ModelID\n",
        "def calc_YearMade(df, model_id):\n",
        "    valid_years = df.loc[(df['ModelID'] == model_id) & (df['YearMade'] > 1000), 'YearMade']\n",
        "    return int(valid_years.mean()) if not valid_years.empty else 1000  # Default if no valid years exist\n",
        "\n",
        "\n",
        "def count_total_rows_per_column(df,column_name):\n",
        "  return df[df[column_name] == 1000].shape[0]\n",
        "\n",
        "def update_YearMade(df):\n",
        "  # Update YearMade where it is 1000\n",
        "  df.loc[df['YearMade'] == 1000, 'YearMade'] = df.loc[df['YearMade'] == 1000, 'ModelID'].apply(lambda model_id: calc_YearMade(df, model_id))\n",
        "  # Compute the mean excluding rows where YearMade == 1000\n",
        "  mean_yearmade = df.loc[df['YearMade'] != 1000, 'YearMade'].mean()\n",
        "\n",
        "  # Update all rows where YearMade == 1000 with the calculated mean\n",
        "  df.loc[df['YearMade'] == 1000, 'YearMade'] = int(round(mean_yearmade))\n",
        "\n",
        "  return df\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def update_NaN_in_numeric_cols(df, column_name):\n",
        "    \"\"\"\n",
        "    Fill NaN values in a specified column with its median while keeping the original data type.\n",
        "\n",
        "    Parameters:\n",
        "    - df: The input DataFrame.\n",
        "    - column_name: The name of the column to update.\n",
        "\n",
        "    Returns:\n",
        "    - Updated DataFrame.\n",
        "    \"\"\"\n",
        "    print(\"update_NaN_in_numeric_cols START - \", column_name)\n",
        "\n",
        "    if column_name in df.columns:\n",
        "        fill_value = pd.to_numeric(df[column_name], errors='coerce').median()  # Get median after coercing\n",
        "        df[column_name] = df[column_name].fillna(fill_value)  # Fill NaN with median while preserving dtype\n",
        "\n",
        "    print(\"update_NaN_in_numeric_cols END - \", column_name)\n",
        "    return df\n",
        "\n",
        "def check_convertible_to_numeric(df):\n",
        "    \"\"\"\n",
        "    Identify columns that can be converted to numeric and contain NaN values.\n",
        "\n",
        "    Parameters:\n",
        "    - df: The input DataFrame.\n",
        "\n",
        "    Returns:\n",
        "    - A list of column names that can be converted to numeric and contain NaNs.\n",
        "    \"\"\"\n",
        "    convertible_cols = []\n",
        "\n",
        "    for col in df.columns:\n",
        "        try:\n",
        "            # Convert column to numeric (coerce errors to NaN)\n",
        "            converted_col = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "            # If NaNs appear after conversion, store column name\n",
        "            if converted_col.isnull().sum() > 0:\n",
        "                convertible_cols.append(col)\n",
        "\n",
        "        except Exception:\n",
        "            pass  # Skip non-convertible columns\n",
        "\n",
        "    return convertible_cols\n",
        "\n",
        "\n",
        "def encode_and_impute(df):\n",
        "    '''\n",
        "    Function to convert categorical columns into category codes\n",
        "    and impute missing values in numerical columns.\n",
        "    '''\n",
        "    print(\"Data Preprocessing START\")\n",
        "\n",
        "    # Handling categorical variables\n",
        "    for col in df.select_dtypes(['object']):\n",
        "        df[col] = df[col].fillna(\"None or Unspecified\").astype('category')\n",
        "\n",
        "    for col in df.select_dtypes(['category']):\n",
        "        df[col] = df[col].cat.codes  # Convert categories to numerical codes\n",
        "\n",
        "    # Handling numerical missing values\n",
        "    for col in df.select_dtypes(['number']):\n",
        "        df[col] = df[col].fillna(df[col].median())  # Fill NaNs with median\n",
        "\n",
        "    print(\"Data Preprocessing END\")\n",
        "    return df\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def analyze_column(df, col_name):\n",
        "    \"\"\"\n",
        "    Analyzes a specified column in a DataFrame.\n",
        "    - If categorical and unique values < 30, print unique values as a list.\n",
        "    - If numeric, show the 25th percentile, 75th percentile, and list values below the 25th and above the 75th percentile.\n",
        "    - Show value counts for all columns.\n",
        "    - Count NaN values per column.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): The DataFrame containing the column.\n",
        "        col_name (str): The column to analyze.\n",
        "    \"\"\"\n",
        "    if col_name not in df.columns:\n",
        "        print(f\"Column '{col_name}' not found in DataFrame.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Analysis for column: {col_name}\\n\" + \"-\" * 40)\n",
        "\n",
        "    # Count NaN values\n",
        "    nan_count = df[col_name].isna().sum()\n",
        "    print(f\"NaN count: {nan_count}\\n\")\n",
        "\n",
        "    if df[col_name].dtype == 'object' or df[col_name].nunique() < 30:\n",
        "        # Categorical or few unique values\n",
        "        unique_values = df[col_name].unique().tolist()\n",
        "        print(f\"Unique values ({len(unique_values)}): {unique_values}\\n\")\n",
        "    else:\n",
        "        # Numeric column\n",
        "        q25 = df[col_name].quantile(0.15)\n",
        "        q75 = df[col_name].quantile(0.85)\n",
        "\n",
        "        print(f\"25th percentile: {q25}\")\n",
        "        print(f\"75th percentile: {q75}\\n\")\n",
        "\n",
        "        low_values = df[df[col_name] < q25][col_name].tolist()\n",
        "        high_values = df[df[col_name] > q75][col_name].tolist()\n",
        "\n",
        "        print(f\"Values below 25th percentile ({len(low_values)}): {low_values}\")\n",
        "        print(f\"Values above 75th percentile ({len(high_values)}): {high_values}\\n\")\n",
        "\n",
        "    # Value counts\n",
        "    print(f\"Value counts:\\n{df[col_name].value_counts()}\\n\")\n",
        "\n",
        "def update_col_NaN(df,column_name,update_to):\n",
        "   df[column_name]=df[column_name].fillna(update_to)\n",
        "   return df\n",
        "\n",
        "def clean_column(df, column_name):\n",
        "    \"\"\"\n",
        "    Cleans a column in the DataFrame.\n",
        "    - Replaces 'None or Unspecified' with 1.0.\n",
        "    - Removes double quotes (\") from values.\n",
        "    - Removes the word \"inch\" from values.\n",
        "    - Removes all spaces from values.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): The DataFrame containing the column.\n",
        "        column_name (str): The column to clean.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Updated DataFrame with cleaned column.\n",
        "    \"\"\"\n",
        "    if column_name not in df.columns:\n",
        "        raise KeyError(f\"Column '{column_name}' not found in DataFrame.\")\n",
        "\n",
        "    df[column_name] = df[column_name].fillna(0.0)\n",
        "\n",
        "    df[column_name] = df[column_name].replace('None or Unspecified', '0.0')\n",
        "\n",
        "    # Function to clean values\n",
        "    def clean_value(value):\n",
        "        str_value = str(value)\n",
        "        str_value = str_value.replace('\"', '')  # Remove double quotes\n",
        "        str_value = str_value.replace('inch', '')  # Remove 'inch'\n",
        "        str_value = str_value.replace(' ', '')  # Remove spaces\n",
        "        return str_value\n",
        "\n",
        "    df[column_name] = df[column_name].apply(clean_value)\n",
        "    df[column_name] = pd.to_numeric(df[column_name], errors='coerce')\n",
        "\n",
        "    # Replace 0.0 values with the column median\n",
        "    median_value = df[df[column_name] != 0][column_name].median()\n",
        "    df[column_name] = df[column_name].replace(0.0, median_value)\n",
        "\n",
        "    return df\n",
        "\n",
        "def convert_feet_inches(df, column_name):\n",
        "    \"\"\"\n",
        "    Converts a column containing feet and inches strings to numeric inches.\n",
        "    - Replaces 'None or Unspecified' with \"0' 0\".\n",
        "    - Converts values like \"9' 6\"\" (feet and inches) into total inches.\n",
        "    - Converts the column data type to numeric.\n",
        "    - Replaces 0 values with the column median.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): The DataFrame containing the column.\n",
        "        column_name (str): The column to process.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Updated DataFrame with cleaned numeric column.\n",
        "    \"\"\"\n",
        "    if column_name not in df.columns:\n",
        "        raise KeyError(f\"Column '{column_name}' not found in DataFrame.\")\n",
        "\n",
        "    # Replace 'None or Unspecified' with \"0' 0\" before processing\n",
        "    df[column_name] = df[column_name].replace('None or Unspecified', \"0' 0\")\n",
        "\n",
        "    # Function to convert feet and inches format to numeric inches\n",
        "    def convert_to_inches(value):\n",
        "        str_value = str(value).strip()\n",
        "\n",
        "        # Match feet and inches pattern\n",
        "        match = re.match(r\"(\\d+)'\\s*(\\d+)?\\\"?\", str_value)\n",
        "        if match:\n",
        "            feet = int(match.group(1))\n",
        "            inches = int(match.group(2)) if match.group(2) else 0\n",
        "            return feet * 12 + inches\n",
        "\n",
        "        return 0  # If format is invalid, default to 0\n",
        "\n",
        "    df[column_name] = df[column_name].apply(convert_to_inches)\n",
        "\n",
        "    # Convert column to numeric\n",
        "    df[column_name] = pd.to_numeric(df[column_name], errors='coerce')\n",
        "\n",
        "    # Replace 0 values with the column median\n",
        "    median_value = df[df[column_name] != 0][column_name].median()\n",
        "    df[column_name] = df[column_name].replace(0, median_value)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "def process_fidesc_secondary_series(df):\n",
        "    \"\"\"\n",
        "    Iterates over rows and updates fiSecondaryDesc and fiModelSeries based on fiModelDesc and fiBaseModel.\n",
        "\n",
        "    Parameters:\n",
        "    df (DataFrame): The input DataFrame.\n",
        "\n",
        "    Returns:\n",
        "    DataFrame: The modified DataFrame with updated fiSecondaryDesc and fiModelSeries.\n",
        "    \"\"\"\n",
        "    # Ensure all values are treated as strings to prevent TypeError\n",
        "    df['fiSecondaryDesc'] = df['fiSecondaryDesc'].astype(str)\n",
        "    df['fiModelSeries'] = df['fiModelSeries'].astype(str)\n",
        "\n",
        "    # Get unique values in fiSecondaryDesc (excluding 'nan')\n",
        "    fiSecondaryDesc_unique_values = sorted(\n",
        "        [val for val in df['fiSecondaryDesc'].unique() if val.lower() != 'nan'],\n",
        "        key=len, reverse=True\n",
        "    )\n",
        "\n",
        "    for index in df.index:\n",
        "        fi_model_desc = str(df.at[index, 'fiModelDesc'])  # Convert to string\n",
        "        fi_base_model = str(df.at[index, 'fiBaseModel'])  # Convert to string\n",
        "        fi_secondary_desc = df.at[index, 'fiSecondaryDesc']\n",
        "        fi_model_series = df.at[index, 'fiModelSeries']\n",
        "\n",
        "        # Create fidesc by removing fiBaseModel from fiModelDesc\n",
        "        fidesc = fi_model_desc.replace(fi_base_model, '', 1).strip()\n",
        "\n",
        "        # Skip empty or NaN fidesc\n",
        "        if fidesc.lower() == 'nan' or fidesc == '':\n",
        "            continue\n",
        "\n",
        "        # Update fiSecondaryDesc if fidesc starts with a known unique value\n",
        "        if fi_secondary_desc.lower() == 'nan':  # Only update missing values\n",
        "            for value in fiSecondaryDesc_unique_values:\n",
        "                if fidesc.startswith(value):  # Ensure the match is at the start\n",
        "                    df.at[index, 'fiSecondaryDesc'] = value  # Assign matching unique value\n",
        "                    #fidesc = fidesc[len(value):].strip()  # **Remove the exact match**\n",
        "                    fidesc = fidesc.replace(value,'')\n",
        "                    break  # Stop after the first match\n",
        "\n",
        "        # Ensure fidesc is fully cleaned before updating fiModelSeries\n",
        "        fidesc = fidesc.strip()\n",
        "\n",
        "        # Prevent fiModelSeries from receiving fiSecondaryDesc value again\n",
        "        if fi_model_series.lower() == 'nan' and fidesc and fidesc != df.at[index, 'fiSecondaryDesc']:\n",
        "            # If fiModelSeries still contains fiSecondaryDesc, remove it\n",
        "            if fidesc.startswith(df.at[index, 'fiSecondaryDesc']):\n",
        "                fidesc = fidesc[len(df.at[index, 'fiSecondaryDesc']):].strip()\n",
        "\n",
        "            df.at[index, 'fiModelSeries'] = fidesc  # Assign cleaned fidesc\n",
        "\n",
        "        #print(f\"{fi_model_desc},{fi_base_model},fiSecondaryDesc={df.at[index, 'fiSecondaryDesc']},fiModelSeries={df.at[index, 'fiModelSeries']}\")\n",
        "\n",
        "    df = df.drop(['fiModelDesc'], axis=1)\n",
        "    return df\n",
        "\n",
        "def group_low_frequency_categories(df, threshold_percent=1.0):\n",
        "    \"\"\"\n",
        "    Groups low-frequency categories in all categorical columns based on a dynamic threshold.\n",
        "\n",
        "    Parameters:\n",
        "    df (DataFrame): The input DataFrame.\n",
        "    threshold_percent (float): The percentage (default 1%) of total non-null values below which categories are grouped as \"Other\".\n",
        "\n",
        "    Returns:\n",
        "    DataFrame: The updated DataFrame with grouped low-frequency categories.\n",
        "    \"\"\"\n",
        "    df = df.copy()  # Create a copy to avoid modifying the original DataFrame\n",
        "\n",
        "    for col in df.select_dtypes(include=['object']).columns:  # Only process categorical columns\n",
        "        value_counts = df[col].value_counts()  # Get frequency of each category\n",
        "\n",
        "        threshold = df[col].count() * (threshold_percent / 100)  # Dynamic threshold\n",
        "        low_freq_categories = value_counts[value_counts < threshold].index  # Find categories below threshold\n",
        "\n",
        "        if len(low_freq_categories) > 0:  # Only modify if there are low-frequency categories\n",
        "            df[col] = df[col].apply(lambda x: 'Other' if x in low_freq_categories else x)\n",
        "            print(f\"Updated column: {col}, grouped {len(low_freq_categories)} categories into 'Other'\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def prepare_dataframe(df_pre:pd.DataFrame):\n",
        "  '''04/02/2025 Start'''\n",
        "  df_pre=update_YearMade(df_pre)\n",
        "  print()\n",
        "  df_pre=update_NaN_To_None_or_Unspecified(df_pre)\n",
        "  print()\n",
        "\n",
        "  #'''As of 05022025 use update_NaN_in_numeric_cols'''\n",
        "  df_pre=update_MachineHoursCurrentMeter(df_pre)\n",
        "  print()\n",
        "\n",
        "  df_pre=update_auctioneerID(df_pre)\n",
        "  print()\n",
        "\n",
        "  df_pre=convert_date_columns(df_pre,'saledate')\n",
        "  print()\n",
        "\n",
        "  '''Start changes 06-02-2025'''\n",
        "\n",
        "  df_pre = process_fidesc_secondary_series(df_pre)\n",
        "\n",
        "  #df_pre=update_col_NaN(df_pre,'UsageBand','None or Unspecified')\n",
        "  #df_pre=convert_feet_inches(df_pre,'Stick_Length')\n",
        "  #df_pre=clean_column(df_pre,'Tire_Size')\n",
        "  df_pre=clean_column(df_pre,'Undercarriage_Pad_Width')\n",
        "  '''End changes 06-02-2025'''\n",
        "\n",
        "  '''Start changes 05-02-2025'''\n",
        "  #remove !!!!df_pre=update_Enclosure(df_pre)\n",
        "  print()\n",
        "\n",
        "  df_pre=update_col_NaN(df_pre,'ProductSize','None or Unspecified')\n",
        "  df_pre=update_col_NaN(df_pre,'Drive_System','No')\n",
        "  df_pre=update_col_NaN(df_pre,'Stick','None or Unspecified')\n",
        "  df_pre=update_col_NaN(df_pre,'Engine_Horsepower','No')\n",
        "  df_pre=update_col_NaN(df_pre,'Track_Type','None or Unspecified')\n",
        "  df_pre=update_col_NaN(df_pre,'Grouser_Type','None or Unspecified')\n",
        "  df_pre=update_col_NaN(df_pre,'Differential_Type','None or Unspecified')\n",
        "  df_pre=update_col_NaN(df_pre,'Steering_Controls','No')\n",
        "\n",
        "\n",
        "  df_pre = group_low_frequency_categories(df_pre)\n",
        "\n",
        "  df_pre=encode_and_impute(df_pre)\n",
        "  '''End changes 05-02-2025'''\n",
        "\n",
        "  #df_pre=encode_all_categories(df_pre)\n",
        "  '''04/02/2025 End'''\n",
        "\n",
        "  return df_pre\n",
        "\n",
        "def run_test(model):\n",
        "  file_path = \"/content/drive/My Drive/Valid.csv\"  # Replace with the saved file path\n",
        "  df_valid = pd.read_csv(file_path)\n",
        "  df_valid['SalePrice']=None\n",
        "  df_valid=prepare_dataframe(df_valid)\n",
        "  X = df_valid.drop(columns=['SalePrice'])\n",
        "  y = df_valid['SalePrice']\n",
        "  y = model.predict(X)\n",
        "\n",
        "  df_valid.loc[X.index, 'SalePrice'] = y\n",
        "\n",
        "  file_path = \"/content/drive/My Drive/output.csv\"  # Save to Google Drive root\n",
        "  df_valid[['SalesID', 'SalePrice']].to_csv(file_path, index=False)\n",
        "\n",
        "  print(f\"File saved successfully at: {file_path}\")\n",
        "\n",
        "def run_train():\n",
        "  pd.set_option('display.expand_frame_repr', False)\n",
        "\n",
        "  df=load_dataframe_from_drive()\n",
        "  df=prepare_dataframe(df)\n",
        "\n",
        "  model,xs=train_model(df=df,column_to_predict='SalePrice')\n",
        "\n",
        "\n",
        "  '''\n",
        "  df=load_dataframe_from_drive('04022025_df')\n",
        "  model=load_model('04022025_model')\n",
        "  xs=generate_X_Y_params(df,'SalePrice')\n",
        "  '''\n",
        "\n",
        "  return model,xs\n"
      ],
      "metadata": {
        "id": "GetwQ5yXBMFX",
        "collapsed": true
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Starting.....')\n",
        "model,xs=run_train()\n",
        "print('Testing...')\n",
        "run_test(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YWjBrHg34pvE",
        "outputId": "dde106d2-10e6-4587-dc4c-79241487de70"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting.....\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-d54c4408393b>:45: DtypeWarning: Columns (13,39,40,41) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "update_NaN_To_None_or_Unspecified START\n",
            "update_NaN_To_None_or_Unspecified END\n",
            "\n",
            "update_MachineHoursCurrentMeter START\n",
            "update_MachineHoursCurrentMeter END\n",
            "\n",
            "update_auctioneerID START\n",
            "update_auctioneerID END\n",
            "\n",
            "convert_date_columns START\n",
            "convert_date_columns END\n",
            "\n",
            "\n",
            "Updated column: fiBaseModel, grouped 1932 categories into 'Other'\n",
            "Updated column: fiSecondaryDesc, grouped 161 categories into 'Other'\n",
            "Updated column: fiModelSeries, grouped 191 categories into 'Other'\n",
            "Updated column: fiModelDescriptor, grouped 128 categories into 'Other'\n",
            "Updated column: fiProductClassDesc, grouped 35 categories into 'Other'\n",
            "Updated column: state, grouped 27 categories into 'Other'\n",
            "Updated column: Drive_System, grouped 1 categories into 'Other'\n",
            "Updated column: Enclosure, grouped 3 categories into 'Other'\n",
            "Updated column: Pad_Type, grouped 2 categories into 'Other'\n",
            "Updated column: Transmission, grouped 4 categories into 'Other'\n",
            "Updated column: Turbocharged, grouped 1 categories into 'Other'\n",
            "Updated column: Blade_Extension, grouped 1 categories into 'Other'\n",
            "Updated column: Blade_Width, grouped 3 categories into 'Other'\n",
            "Updated column: Enclosure_Type, grouped 2 categories into 'Other'\n",
            "Updated column: Engine_Horsepower, grouped 1 categories into 'Other'\n",
            "Updated column: Hydraulics, grouped 6 categories into 'Other'\n",
            "Updated column: Tip_Control, grouped 1 categories into 'Other'\n",
            "Updated column: Tire_Size, grouped 12 categories into 'Other'\n",
            "Updated column: Coupler_System, grouped 1 categories into 'Other'\n",
            "Updated column: Grouser_Tracks, grouped 1 categories into 'Other'\n",
            "Updated column: Hydraulics_Flow, grouped 1 categories into 'Other'\n",
            "Updated column: Stick_Length, grouped 27 categories into 'Other'\n",
            "Updated column: Pattern_Changer, grouped 1 categories into 'Other'\n",
            "Updated column: Grouser_Type, grouped 1 categories into 'Other'\n",
            "Updated column: Backhoe_Mounting, grouped 1 categories into 'Other'\n",
            "Updated column: Blade_Type, grouped 6 categories into 'Other'\n",
            "Updated column: Travel_Controls, grouped 5 categories into 'Other'\n",
            "Updated column: Differential_Type, grouped 3 categories into 'Other'\n",
            "Updated column: Steering_Controls, grouped 3 categories into 'Other'\n",
            "train_model START\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "could not convert string to float: 'Medium'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-cfcff9e906b3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Starting.....'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Testing...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrun_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-d54c4408393b>\u001b[0m in \u001b[0;36mrun_train\u001b[0;34m()\u001b[0m\n\u001b[1;32m    858\u001b[0m   \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepare_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumn_to_predict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'SalePrice'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-d54c4408393b>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(df, column_to_predict, test_size_value, random_state_value)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;31m#model = LinearRegression()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m         X, y = validate_data(\n\u001b[0m\u001b[1;32m    361\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2959\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2960\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2961\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2962\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0mensure_all_finite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deprecate_force_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_all_finite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m   1371\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    971\u001b[0m         \u001b[0;31m# Use the original dtype for conversion if dtype is None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m         \u001b[0mnew_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype_orig\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m         \u001b[0;31m# Since we converted here, we do not need to convert again later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   6641\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6642\u001b[0m             \u001b[0;31m# else, only a single dtype is given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6643\u001b[0;31m             \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6644\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor_from_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6645\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"astype\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m         return self.apply(\n\u001b[0m\u001b[1;32m    431\u001b[0m             \u001b[0;34m\"astype\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors, using_cow, squeeze)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# type: ignore[call-overload]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastype_array_safe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_coerce_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/dtypes/astype.py\u001b[0m in \u001b[0;36mastype_array_safe\u001b[0;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastype_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;31m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/dtypes/astype.py\u001b[0m in \u001b[0;36mastype_array\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_astype_nansafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;31m# in pandas we don't store numpy str dtypes, so convert to object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/dtypes/astype.py\u001b[0m in \u001b[0;36m_astype_nansafe\u001b[0;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;31m# Explicit copy, or required since NumPy can't view from / to object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'Medium'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}